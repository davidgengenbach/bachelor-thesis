\todo{Explain text classification task and its real world importance}
\todo{Clearly state problem}
\todo{Give a motivating example for the task}
\todo{Explain difficulties of the task}
\todo{Explain the research question and how it relates to the field}
\todo{Hint how the presented approach is an improvement over the old approach}
\todo{Hint the hypothesis}
\todo{Introduce earlier approaches/results and their weaknesses}
\todo{Outlook}

\labelsubsection{Motivation}{subsec:motivation}
\todo{Applications also}

\paragraph{Text Classification}
\todo{Text classification is important. Why?}
\todo{Applications?}

\paragraph{Graph Classification}
\todo{Structured data}
\todo{Sequences like strings/text or trees can all be expressed intuitive as graphs.}
\todo{Relationships between entities}
\todo{Applications}

Following are some applications of graph classification:

\begin{figure}[htb!]
\centering
\begin{tabular}{llll}
field & nodes & edges & classes \\
\midrule
chemistry & atoms & bonds & toxicity (binary) \\
biology & amino acids & spatial link & protein types \\ 
social networks & users & 'are friends' or 'like the same content' & bot detection (binary) or user classification
\end{tabular}
\end{figure}

As we will see later on, the graph classification task is also similar to the task of finding similar graphs for a given graph.

\labelsubsection{Hypothesis and Goals}{subsec:hypothesis_and_goals}
In this work, we evaluate the usefulness of graph representations of text in the context of text classification. In particular, we work out how or whether the structure of so-called concept maps actually improve the classification performance.
We will introduce concept maps in the next section.

The main hypothesis of this work is:
\begin{quote}
\hypothesis
\end{quote}

There are a great number of approaches for text classification, a lot of them based on counting the words of a text and learning a classifier with these frequencies and labels.
Yet, these word-count based approaches often do not take the semantic or syntax of sentences into account, ie. text-based approaches often do not leverage the structure and meaning of sentences.
A partial solution for this issue is solved by augmenting single-word counts with n-grams counts.
N-grams are sequences of length $n$ of consecutive words in the text, therefore they capture word dependency and word order.
Text-based approaches are widely used and achieve high performance both in compute time and classification scores.

In this work, we explore how concept maps, and other graph representations like co-occurrence graphs, can improve classification scores.
We choose concept maps as the preferred graph representation since they are specially created to capture important concepts and their relation to each other, therefore maybe also capturing the semantic of the underlying text.
In the next sections, we will further explain graph representations and related concept. 
We will also gain a better insight into the potential usefulness of concept maps in the context of text classification.

\todo{Maybe some day concept maps will prevail as information representations and classification of graphs will become more important}
\todo{``text analysis, text categorization, information extraction,  and summarization"}

\labelsubsection{Thesis Structure}{subsec:thesis_structure}
In the next section, \fullref{sec:background}, we will introduce the concepts used in the rest of this work.
We also offer an overview over related work and the history of the field of text- and graph based classification.

In section \fullref{sec:evaluation}, we further describe our hypothesis and outline questions regarding it. This section also covers the methodology and experiments we use to answer these questions.

Next, in section \fullref{sec:results}, we then provide the results to the questions we pose in the preceding chapter, interpreting them in the context of our hypothesis.
Here, we also provide other observations regarding our approach.

Finally, in section \fullref{sec:conclusions}, we gather the results of the previous section into a more high-level picture.
We close with finishing remarks regarding possible further work and interpret our results in the context of previous work.