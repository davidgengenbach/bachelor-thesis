In order to test our hypothesis, we first derive a number of questions.
These questions will help us getting an insight into the usefulness of concept maps for the text classification task.
We will also often compare the results of these questions to co-occurrence graphs to find differences. While the comparison to co-occurrence graphs is interesting by itself, it is not entirely fair since co-occurrence graphs have far more content than concept maps.

In the next chapter, Chapter \ref{sec:results}, we will report the results to the experiments we devised for our questions.
We will also provide a discussion of the results, bundling the results to a judgment for the hypothesis.

\labelsection{Experiments}{subsec:experiments}
In this section, we introduce our methodology and explain the experiments we conduct to test the hypothesis.

Following are the questions we devised in order to understand and test our hypothesis which is:
\begin{quote}\hypothesis\end{quote}

We will provide a summary for each of the questions in the next chapter, revisiting the significance of the individual questions for our hypothesis.

\labelsubsection{Questions}{subsec:questions}

\subquestion{How diverse is the structure of concept maps?}{question:structure_diversity}
When the structure of the graphs is homogeneous or the structural differences are not distinct between graphs of different classes, the structure by itself will most likely not contribute to the classification performance.
When comparing the graph similarity under some graph kernel, the graphs of a given class should be distinguishable from the graphs of the other classes.

To understand the diversity in the structure of concept maps, look at different metrics, eg. a histogram of the connected components per graph or the average node/edge ratio for each graph.
All these metrics aim to find out the diversity of the connectedness of concept maps. There a lot of other metrics, but as we will see later on these metrics suffice to get a good picture about the structure of the concept maps.

\subquestion{How important is the structure of concept maps compared to the content?}{question:importance_structure}
Another important question is the relative importance of the structure compared to the content, ie. the labels of the graphs.
For this, we compare the results of different graph kernels that use \textbf{(a)} only content and \textbf{(b)} only structure and \textbf{(c)} both content and structure.

For the \textbf{content-only kernel}, we will simply count the number of occurrences of labels in the graphs and essentially create a bag-of-words representation of the graph. The kernel then creates the similarity score by calculating the inner product on these vector representations, effectively counting the number of common labels.

For the \textbf{structure-only kernel}, we will use a modified version of the Weisfeiler-Lehman kernel: before executing the WL kernel on the graphs, we first discard all labels on the graph. All nodes of all graphs get the same label. This way, only the structure gets used for the similarity score.

For the \textbf{structure-and-content kernel}, we use the plain Weisfeiler-Lehman graph kernel. 

The difference in the results will give an additional insight into the structure of concept maps and their usefulness for classification.


\subquestion{How useful are multi-word labels in concept maps?}{question:multi_labels}
Concept map node labels often consist of more than one word which constitutes one of the differences to co-occurrence graphs and other graph text-representations.
Yet, our default approach, ie. the Weisfeiler-Lehman graph kernel, does not leverage these multi-word labels.
So, for this questions we evaluate other approaches which make use of the multi-word labels and evaluate the usefulness of them.
This approach will later on also be used for the combined classification of text and graph features.

\subquestion{How diverse and useful are edge labels in concept maps?}{question:edge_labels}
Another difference of concept maps to co-occurrence graphs is that the former have edge labels.
For this question we evaluate the importance and role of these edge labels in graph classification.
To make use of the edge label, we will also further introduce the linearized, or un-rolled, graph, ie. a graph converted into text.

\subquestion{Does removing infrequent node labels from the concept map improve classification performance?}{question:infrequent_nodelabels}
As with text, there are often a great number of words which only occur once in the whole dataset.
The same applies to concept maps.
In text-based classification approaches, infrequent words are often removed from the text for two reasons: (1) the resulting classifier has less parameters to be trained, thus lower complexity, (2) removing infrequent words greatly reduces the size of feature vectors which, in some cases, can prevent overfitting to too specific features.
Another WL specific problem of infrequent words is that when a concept only occurs once and it is in the neighborhood of another node, matches in higher iterations of WL are impossible since the concept only occurs once and can not be in any other neighborhood apart from the only occurrence.
For this question, we will evaluate whether removing infrequent node labels will result in better scores.

\subquestion{Does merging infrequent concepts increase the classification score?}{question:relabeling_infrequent}
To overcome the issue of infrequent concepts in concept maps, we merge infrequent concepts with more frequent concepts.

\subquestion{How does the performance of using the directed edges in concept maps compare to undirected edges?}{question:directed_vs_undirected}
Concept maps, in contrast to co-occurrence graphs, also have directed edges.
To test the effect of using directed versus un-directed edges, we evaluate the classification performance for both cases.

\subquestion{How does the size of concept maps relate to classification performance?}{question:concept_map_size}
The size of a concept map, ie. the number of nodes and edges, is correlated to the size of its underlying text.
Also, the number of occurrences of a single concept in a given text is important for the degree of the corresponding node, ie. when a concept occurs only once in a given text, the corresponding node in the concept map will have a low degree.
On the other hand, when a concept occurs multiple times in a text, the degree of this concept will most likely increase.
These observations give rise to the question whether the size of the concept maps is correlated with the classification performance.
In this experiment we look at the classification scores of concept maps created from texts with varying length.

\subquestion{How does the classification results of co-occurrence graphs compare to concept maps?}{question:comparison_coo}
For this question, we compare the classification results for concept maps and co-occurrence graphs. As noted before, this comparison is slightly unfair since co-occurrence graphs have far more content than concept maps.
Nevertheless, it is an interesting baseline for graph-based text-classification.

\subquestion{How does the classification performance with concept maps compare to non-structural, text-based approaches?}{question:comparison_text}
Here we test the classification performance of graph-based classification directly with the performance of text-only approaches.
This experiment will serve as a baseline in later experiments.

\subquestion{How useful are features obtained from concept maps combined with text features for text classification?}{question:comparison_combined}
This question aims to directly test our hypothesis. 
When concept maps indeed have a structure that is useful for classification and when this structure is not captured by text-only approaches, the classification performance could increase when combining graph- and text based approaches.
We test the performance of the combined text- and graph features by concatenating the vectors of both approaches and then training a classifier with these vectors.
We also compare the results with both text-only and graph-only approaches.

\subquestion{How does the compute time of graph-based- compare to text-based  approaches?}{question:comparison_runtime}
Another important question concerns the complexity or run-time of our approach.
While graph kernels like the Weisfeiler-Lehman or the Random-Walk kernels haves seen a rise in efficiency and improvement in runtime, there is still also an overhead associated when generating concept maps.

\hspace{2cm}
\paragraph{Summary}
These questions and the associated experiments all aim to achieve a better insight into the usefulness of concept maps for the classification task.
After conducting the experiments and gathering the results, we will further explain the importance of each individual question for our hypothesis.

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}\includegraphics[width=\linewidth]{assets/figures/graph_classification_phi.pdf}
		\caption{Feature map $\phi$ based}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\linewidth}\includegraphics[width=\linewidth]{assets/figures/graph_classification.pdf}
		\caption{Gram matrix $A$ based}
	\end{subfigure}
	\caption[Diagram: Graph kernel based classification]{Graph kernel based classification pipeline.}
\end{figure}

\begin{figure}[htb!]
	\centering
	\includegraphics[width=0.6\linewidth]{assets/figures/graph_classification_phi_combined_text.pdf}
	\caption[Diagram: Graph kernel and text combined text classification]{Combined graph and text classification pipeline.}
\end{figure}

\subsection{Baselines}
\paragraph{Preprocessing}
Before creating the vector representations of the text documents or the creation of the co-occurrence graphs and concept maps, we first pre-process the plain text by

\begin{itemize}
\item{lower-casing the text,}
\item{removing non-printable characters,}
\item{replacing numbers with \textit{NUMBER} placeholders,}
\item{replacing tabs and newlines with a space,}
\item{and normalizing the whitespace (eg. replacing multiple spaces with a single space)}
\end{itemize}
These pre-processing steps are similar to the pre-processing done in \cite{Cachopo2007}.
An example of pre-processing can be seen in Figure \ref{fig:preprocessing_example}.

\begin{figure}[htb!]
	\begin{subfigure}[b]{0.47\linewidth}
\begin{mdframed}[nobreak=true]
\textsf{I've heard *unconfirmed* rumours that there is a new Integra being released
	for '94.
	\\
	\\
	Does anybody have any info on this?
	\\
	\\
	The local sales people know as much as I can throw them.
	\\
	\\
	--Parms.}
\end{mdframed}
    \caption{Before}
    \end{subfigure}
\hspace{0.2in}
	\begin{subfigure}[b]{0.47\linewidth}
\begin{mdframed}[nobreak=true]
\textsf{i've heard unconfirmed rumours that there is a new integra being released for ' NUMBER. does anybody have any info on this? the local sales people know as much as i can throw them. parms.}
\end{mdframed}
\vspace{0.4in}
    \caption{After}
    \end{subfigure}
	\caption[Example: Pre-Processing]{Pre-processing example. Text taken from \textit{ng20} dataset.}\label{fig:preprocessing_example}
\end{figure}

For the co-occurrence graphs, we also optionally filtered out the non-nouns to increase the compression and thus achieve more comparability to concept maps.

\paragraph{Text-based representations}
For the text classification pipeline, we used two \textit{Bag-Of-Words}, or \textit{BoW}, based text vectorization algorithms, namely
\begin{itemize}
\item{\textit{Word Frequency} (Wf): this algorithm simply gathers all words in the corpus and creates a mapping between words and consecutive indices. Then it creates a vector representation for each text so that the i-th vector component is the count of the corresponding word in the text. Ie. $i$ is the index of the word in the mapping.}
\item{\textit{Term-Frequency-Inverse-Document-Frequency} (TfIdf): this approach is an extension to the \textit{Word Frequency BoW} approach. Instead of using only the counts of a word in the text, this approach also incorporates the term frequency and the inverse document frequency of the words into the vector representation.}
\end{itemize}
Both approaches can also be extended by not only utilizing single words, or unigrams, but n-grams. A word n-gram consists of $n$ words that appear consecutively in the text.
For example, the sentence ``This is a sentence." has the following 2-grams, or bigrams: $\{ (This, is), (is, a), (a, sentence) \}$.
Note that word n-grams do not take word inversion into account, ie. the bi-gram (a, b) is not the same as (b, a).
Also note that adding n-grams to BoW feature vectors increases their dimensionality which in turn could lead to bigger classifier models and possibly over-fitting to these more specific n-gram features.

For our purposes, we test only uni-gram and bi-gram frequencies since our classification performance seldom profited from adding higher n-gram ranges.
Also, we filter out words which occur only once in the training set \cite{Heap2017}.
This has multiple advantages: (1) the dimension of resulting feature vectors is significantly lower, especially when also applying the same principle to bi-grams where there are far more combinations which occur infrequently; (2) this in turn reduces both the fit and transform time; (3) the size of models, eg. number of weights of the classifier, is far smaller; (4) overfitting to too specific features \textit{might} be lessened.
We also performed experiments to quantify the difference in performance when omitting infrequent words from the BoW approach and actually saw only insignificant differences.
We report the results in Table \ref{table:results_min_df}.

\begin{table}
\centering
\begin{tabular}{lrr}
	{} & \multicolumn{2}{c}{F1 macro} \\
	{} &                         $min_{wf} = 1$ &  $min_{wf} = 2$  \\
	\midrule
	ling-spam       & 0.983 & 0.986 \\
	ng20            & 0.788 & 0.784 \\
	nyt\_200         & 0.890 & 0.878 \\
	r8              & 0.923 & 0.919 \\
	review\_polarity & 0.867 & 0.875 \\
	rotten\_imdb     & 0.888 & 0.886 \\
	ted\_talks       & 0.453 & 0.466 \\
	\bottomrule
\end{tabular}
\caption[Results: BoW minimum word frequency]{Classification results for BoW with (1) all labels and (2) only labels that occur more than once.}
\label{table:results_min_df}
\end{table}

\paragraph{Graph-based representations}
To compare the performance of concept maps with other graphs, we generated co-occurrence graphs with window sizes $w \in \{1, 4\}$.
We also evaluated the performance of co-occurrence graphs where only nouns are retained to mimic the compression factor of concept maps.

For the extraction of the concept maps from the text, we used the implementation by Falke, introduced in \cite{Falke2017}.
An explanation of the steps to create the concept maps is given in Section \ref{sec:implementation}.

\labelsection{Datasets}{subsec:datasets}
We evaluate our approaches and experiments on a number of datasets, ranging from informal texts written by internet users, eg.  the \textit{ng20} internet forum corpus, to more structured texts like the \textit{nyt} corpus.
The texts of the corpora are also of varying length, enabling us to evaluate the effect of varying concept map sizes.

We provide download links for all the datasets except for the commercial \textit{nyt} dataset.
A script to download these datasets is also provided alongside our other code.

\paragraph{ling-spam}
The Ling-Spam dataset was created and introduced by Androutsopoulos et al. in  \cite{Androutsopoulos2000}.
The corpus contains email texts which are categorized as ``spam" and ``no spam".
One thing to note is that the classification scores with standard methods are quite high by default, so most likely no substantial increase in performance is to be expected.

We obtained a copy of the corpus from here \footnote{\url{http://csmining.org/index.php/ling-spam-datasets.html}}.


\paragraph{ng20}
The 20 Newsgroup corpus consists of posts from an internet forum and was introduced in \cite{Lang}. Each post is labelled with one of 20 different classes, corresponding to the topic it have been posted on. The texts are mostly informal and consist of discussions between users of the forum.
For this dataset, as an additional pre-processing step, we remove the headers and footers from the documents.

While the classes are nearly evenly distributed, some classes are highly correlated\footnote{\url{http://qwone.com/~jason/20Newsgroups/}}, ie. instances of one class A are very similar to the texts from another class B. This adds an additional difficulty to the task.

We obtained the ng20 corpus from here\footnote{\url{http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch\_20newsgroups.html\#sklearn.datasets.fetch_20newsgroups}}.

\paragraph{nyt\_200}
The documents in this dataset are articles published in the \textit{New York Times} newspaper in the time between 1987 and 2007.
In total it contains about 1,8 million articles, covering a great number of topics.
Each article has different attributes, examples ranging from the publish date, the author or the section where the article was posted on the \textit{New York Times} website.
As labels we used the online sections an article has been posted to.

We found this dataset when searching for a corpus consisting of long documents. 
For our purposes we only used articles with more than 3000 words and from the six most frequent labels.
Since the extraction of concept maps for documents of this document size takes a long time, we randomly selected 200 documents for each of the six labels, resulting in a dataset of 1200 articles.
Besides the long texts, the other reason we chose this dataset is that it contains high-quality texts.
The texts of most of other datasets we considered are gathered from posts by internet users and often lack basic punctuation or contain misspellings.
This missing structure makes the extraction of concept maps harder since the concept map extraction relies on reliable part-of-speech tags which in turn profit from correct spelling and syntax. 

We obtained our copy of the dataset from here\footnote{\url{https://catalog.ldc.upenn.edu/LDC2008T19} (This is a commercial dataset and has to be bought.)}.

\paragraph{reuters-21578}
This dataset consists of news articles collected and published by Carnegie Group and Reuters.
 The class distribution of the \textit{reuters-21578} dataset is highly skewed, ie. the number of instances per class is not the same for all classes.
 Some of the articles have multiple classes assigned. For our purposes, that is single-label classification, we only used articles with one class.
 We also only use documents which consist of more than 20 words, so that meaningful concept maps can be created.
Since the \textit{reuters-21578} dataset is quite big and also contains multi-label documents, ie. documents which have more than one class assigned, we do not actually use this dataset but use a subset which is described below.

We obtained the \textit{reuters-21578} dataset from here\footnote{\url{http://www.nltk.org/book/ch02.html\#reuters-corpus}}.

\paragraph{r8}
This dataset is a subset of the \textit{reuters-21578} dataset.
It consists of the 8 most frequent classes of the \textit{reuters-21578} dataset, ie. the 8 classes with the most documents.

\paragraph{review\_polarity}
The \textit{review\_polarity v2} dataset consists of positive and negative reviews for movies by users.
One thing to note is that these reviews are often quite short and informal.

The dataset was introduced in \cite{Pang2004}. We obtained our copy from the author's website \footnote{\url{http://www.cs.cornell.edu/people/pabo/movie-review-data/review\_polarity.tar.gz}}.

\paragraph{rotten\_imdb}
This dataset consists of sentences, each labeled with one of two classes: \textit{subjective} or \textit{objective}.
Note that this dataset consists of short texts and can therefore be used to evaluate the performance of small concept maps.

The dataset w as introduced in \cite{Pang2004}. We obtained our copy from the author's website\footnote{\url{http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten\_imdb.tar.gz}}.

\iffalse
\paragraph{tagmynews}
This corpus consists of summaries obtained from the RSS feeds of three news sites, namely \textit{nyt.com}, \textit{usatoday.com} and \textit{reuters.com}.
The dataset was introduced in \cite{Vitale2012a}.
We obtained the copy from the author's website\footnote{\url{http://acube.di.unipi.it/repo/news.gz}}.	


\paragraph{webkb}
This dataset consists of websites which have been downloaded from 4 american universities. It was collected 1977 during the \underline{W}orld \underline{W}ide \underline{K}nowledge \underline{B}ase project by the CMU Text Learning Group.
The webpages are grouped in seven classes. The distribution of this very skewed, for example with one class having over 3000 instances and another only a little over 100.
We obtained a copy of this dataset from here\footnote{\url{http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/}}.
\fi

\paragraph{ted\_talks}
This corpus consists of video transcripts from TED talks\footnote{\url{https://www.ted.com/}} released under the Creative Commons License\footnote{\url{https://www.ted.com/about/our-organization/our-policies-terms/ted-talks-usage-policy\#h2--ted-talks-under-creative-commons-license}}.
We obtained the transcripts from Kaggle\footnote{\url{https://www.kaggle.com/rounakbanik/ted-talks}}.
The transcripts had no labels attached, only an URL to the corresponding video.
Each TED talk video has one or more tags attached, which we automatically crawled from the video URL.
Since there are possible multiple tags per video, we had to filter out specific tags since our task entails multi-class-, not multi-label classification.
To filter out the tags, we first looked at the most-frequent tags and create a correlation, or co-occurrence, matrix of these tags to find out how often these tags co-occur.
We then selected a subset of four tags, namely $Y = (\mathrm{economics, environment, brain, entertainment})$  which were loosely correlated.
We discarded all transcripts with tags $y$ which contain more than one of the tags $Y$, ie. $|Y \cap y| > 1$.
All tags besides $Y$ were ignored.
The resulting datasets contains 682 documents, containing a large number of words per document.

\begin{table}[htb!]
\centering
\begin{tabular}{lrrrrr}
{} &  \# classes &  \# docs & \# words & median \#words/doc &  \#uniq. words/\#words \\
\midrule
ling-spam       & 2 &  2.893 &  1.303k & 277 & 0.10 \\
ng20            & 20 &  18.846 &  3.570k & 79 & 0.06 \\
nyt\_200         & 6 &  1.200 &  4.735k & 3397 & 0.07 \\
r8              & 8 &  7.288 &  855k & 82 & 0.06 \\
review\_polarity & 2 &  2.000 &  1.248k & 584 & 0.07 \\
rotten\_imdb     & 2 &  10.000 &  204k & 19 & 0.12 \\
ted\_talks       & 4 &  682 &  1.353k & 2027 & 0.09 \\
\bottomrule
\end{tabular}
\caption[Statistics: Datasets]{Dataset statistics. \textit{\# words} in thousands.}\label{table:dataset_statistics}
\end{table}

\labelsection{Methods}{subsec:methods}

\labelsubsection{Cross-Validation and Model Selection}{subsec:cross_validation_and_model_selection}

\paragraph{Cross-Validation}
For all the classification tasks we use train-/validation- and test set splits.
The split is done as follows: 80\% for train- and validation set together and 20\% for the test set.
We then use stratified 4-fold cross-validation to further split the train- and validation set.
The stratification ensures that the class distribution in the different sets is \textit{almost} the same as in the class distribution of the complete dataset.

\paragraph{Model Selection}
Hyperparameter tuning, or model selection, is done using cross-validation on the train-/ validation set, ie. we first train classifiers for all hyperparameter choices on all $k$ train-/validation splits, and after finding the best hyper-parameters on these sets, we retrain the best classifier on the whole train-/validation set and then evaluate the performance on the test set \textbf{once}.
So, the test set was \textbf{not} used for immediate evaluation or parameter tuning.
The classification performance is only evaluated once on the test set to retrieve the final results we report here.

For our experiments, we use the \textit{SVM}\cite{Cortes1995} as classifier unless stated otherwise.
The parameters for the \textit{SVM} are gathered in Table \ref{table:svm_parameters}.

\begin{table}[htb!]
	\centering
	\begin{tabular}{ll}
		\textit{SVM} parameter & Value  \\
		\toprule
		C & $\{0.01, 0.1, 1\}$ \\
		Regularization & L2 \\
		Kernel & linear \\
		Optimization problem & dual \\
		Stopping criteria tolerance & 0.0005 \\
		Class weight & balanced \\
		\bottomrule
	\end{tabular}
	\caption[Table: SVM parameters]{\textit{SVM} parameters. The \textit{C} parameter is tuned using cross-validation. A more detailed explanation is available online\footnote{\url{http://scikit-learn.org/stable/modules/svm.html\#svm-classification}} and in \cite{Cortes1995}} %
	\label{table:svm_parameters}
\end{table}

We choose the \textit{SVM} as our classifier algorithm since it can be used for both classical, vector-based classification as well as for gram-based classification, making it an ideal candidate for our experiments.
That said, the \textit{SVM} also has shown state-of-the-art performance in (text) classification and other domains.
In contrast to other learning algorithms, eg. a neural net, SVMs are also more robust in their convergence \cite{Joachims1998}.

To increase the reproducibility of our results, we utilized the same random generator seed, 42, for all experiments, eg. for ``randomly" splitting the dataset in the train/validation/test splits or ``randomly" initializing the coefficients for a \textit{SVM}.

We also evaluated results obtained with nested cross-validation to further de-bias our results \cite{Varma2006}. Yet, when inspecting the results from tests with nested cross-validation we saw that the standard deviations of the results are relatively low.
This observation and the fact that nested cross-validation also increases the compute-time significantly, led us to abandon nested cross-validation for our experiments.
However, to be able comparing the results of two different models, we consistently used the permutation significance test. 
Interpreting the resulting confidence value dampens the effect of not doing nested cross-validation.

\todo{More elegant sentence, please?}

\subsection{Metrics}
As a classification metric, we mostly focus on the F1 macro score since it captures the overall performance of classification algorithms by merging two other metrics, precision and recall.
For an overview and definition of the used metrics, please see Section \ref{subsec:classification_task}.
We will provide all our results online\footnote{TODO} with additional metrics, like the accuracy, precision and recall.
We do not report standard deviations for our experiments since we, as mentioned before, do not use nested cross-validation, so we only have results on a single static test set.
See the previous Section \ref{subsec:cross_validation_and_model_selection} for a more throughout explanation.

To analyze the significance of a difference between two models, we use significance tests.
An introduction to and an example for significance tests are given in Section \ref{subsec:significance_test}.

\subsection{Feature Scaling and Centering}
Classifiers, especially \textit{SVMs}, profit from scaled and centered features \cite{Graf2001}, ie.s features which have a mean of 0 and are in the range of $[-1, 1]$.

\paragraph{Scaling}
For our purposes, we scaled the feature vectors by using feature-wise scaling\footnote{Implementation: \url{http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html\#sklearn.preprocessing.MaxAbsScaler}} \cite[p.~567]{Bishop2006}.
Given the feature vectors $\phi(d_i) = (f_1, f_2, \ldots, f_n)$ for each of the $N$ dataset instances $d_i$, we first find the maximum for each vector entry $f_x$ across the whole dataset. For instance, the maximum of first feature vector entry, $f_1$, is
\begin{equation*}
f_1^{\textrm{max}} = \max(\{\phi(d_i)_{1} | i = \{1, 2, \ldots, N\} \})
\end{equation*}
where $\phi(d_i)_{1}$ stands for the first entry of $\phi(d_i)$.
After finding the maximum for each feature $f_x$, we then scale this feature by the absolute value of the maximum, eg. $|f_x^{\textrm{max}}|$.
The new, scaled feature vector is thus
\begin{equation*}
\phi_{\textrm{scaled}}(d_i) = (f_1 / |f_1^{\textrm{max}}|, f_2 / |f_2^{\textrm{max}}|, \ldots, f_n / |f_n^{\textrm{max}}|)
\end{equation*}

For WL, we also tried scaling the feature vectors to unit length. Yet we saw the same or even worse results than without unit-length scaling, so we did not apply it.
Unit-length scaling is applied to single feature vectors, $\phi$, so that its length $||\phi||_{norm} = 1$ under some norm, eg. L1 or L2.

\paragraph{Centering}
Centering a vector means subtracting the mean of each feature entry  \cite[p.~567]{Bishop2006}, therefor effectively centering the mean of the feature vector to 0, ie. the average of each feature vector index is 0.
Since we use only highly sparse and high-dimensional vectors, centering the feature vectors is not feasible since it would destroy the sparsity which in turn would make the subsequent computation too memory consuming if not impossible.
This is due to the fact that centering a vector often means adding or subtracting another vector from it which creates non-zero entries for all entries.
Only when the vector is already centered, the centering would not affect the sparsity.

\todo{There are possible solutions to circumvent this problem and nevertheless center sparse vectors, namely by augmenting the sparse vector data-structure by also saving the means for each feature entry.
Yet, to our knowledge there is no implementation actually having this feature.}

Fortunately, since we have very sparse features, the mean for each feature is very close to zero.
This is due to the fact that most features occur very infrequently. Together with the high number of elements in the dataset, the mean of each feature will be close to zero since the average, $\hat{f}_x$, for each feature $f_x$ is calculated by taking the sum of the feature for all elements and dividing it by the number of elements in the dataset
\begin{equation*}
\hat{f}_x = \frac{\sum_{d_n} (\phi(d_n)_x)}{N}
\end{equation*}
In our cases, $N > \sum_{d_n} (\phi(d_n)_x)$ is true for most of the features since most features occur quite infrequently and therefor the sum is also low.

\labelsubsection{Significance tests}{subsec:significance_test}
When comparing two models we use the permutation test \cite{Fisher1925}, or exact test, to test the significance of the difference in performance of these two models.
The permutation test tests whether the observed difference of the performances of the two approaches is a result of chance or really signifies a more fundamental difference without assuming an underlying distribution of the differences that can be observed.
The test only returns a probability for observing a given performance difference. The test does not give a definite answer whether the difference was due to chance.
When the probability of observing the difference by chance is below a given threshold, the confidence $p$, we say that the test indicates that the approaches significantly differ in their performance.
For our purposes, ie. to test whether one classification approach is a significant improvement over another approach, we use the two-tailed version \cite{Kock2015} of the permutation test.
We also provide the $p$ value for our tests.
We say that a model is significantly better when the observed performance difference has a frequency of $p < 0.05$ using the permutation test with a sample size of 10000.

\paragraph{Example}
In this example we will use the two-tailed permutation test to test whether the 
  performances of two given classifiers, Model A and B, differ in a non-random way.
In Figure \ref{fig:example_permutation_test} we depicted the steps in the permutation test. 
In our example, the hypothesis is that Model A has a lower score than Model B.
For this example, we choose accuracy as the score.
To calculate the accuracy, we need the true labels of the dataset, seen in Figure \ref{fig:example_permutation_test} \textbf{\subref{fig:permutation_test_true}}.
In Figure \ref{fig:example_permutation_test} \textbf{\subref{fig:permutation_test_model_predictions}} we see the predicted labels of the two models and the accuracy as a score beneath them. 
We also see the difference in the scores.
In Figure \ref{fig:example_permutation_test} \textbf{\subref{fig:permutation_test_model_predictions}} we also see that Model A has a lower score than Model B.
This lower score could be due to chance and not because Model B is fundamentally better than Model A.
To test our hypothesis more throughly we now execute the permutation test.
In the first step, Figure \ref{fig:example_permutation_test} \textbf{\subref{fig:permutation_test_samples}}, we generate $n$ samples by interchanging the predictions of the two models randomly.
Here, we depicted three randomly selected samples from the $n$ generated samples.
To generate a sample, we switch the predictions of Model A and B for every document with a probability of 0.5.
The switch of two predictions is marked with a red arrow in our depiction.
In the optimal case, one could generate all possible permutations of the predictions, ie. generate all possible samples.
Unfortunately this often is not an option due to the sheer number of possibilities. That said, generating a large number of samples also gives a good basis for judgment.
In the next step, we calculate the accuracy scores for both models for each of the $n$ samples and also calculate the difference between these scores.
In Figure \ref{fig:example_permutation_test} \textbf{\subref{fig:permutation_test_distribution}} we see the histogram of score differences in the samples.
The red lines mark the initially observed difference between the models A and B (also it's negative).
In the last step, we gather these differences and count the number of samples, $n_{higher}$, where the absolute difference between the models is smaller than the score difference of the original predictions of Model A and B.
$n_{higher}$ divided by the total number of generated samples, $n$, is then the frequency of samples, $f_{higher} = \frac{n_{higher}}{n}$, where the score difference was higher when randomly exchanging the predictions compared to the original observed score difference.
$f_{higher}$ gives an intuition about the likelihood of observing the difference between the scores of Model A and B.
In the histogram of observed differences Figure \ref{fig:example_permutation_test} \textbf{\subref{fig:permutation_test_distribution}} this $f_{higher}$ corresponds to the ratio between the area of elements in the blue surface to total area of the histogram.
In our example, $f_{higher}$ is 0.135, meaning that the probability to observe the score difference we see between the two models is 13.5\% when randomly exchanging the predictions of the two models.
This is far too high to accept the hypothesis on ground of these predictions.

\begin{figure}[htb!]
  \begin{subfigure}[t]{0.29\linewidth}
  \centering
  {\includegraphics[height=2.2in]{assets/figures/permutation_test/true_labels.pdf}\label{fig:permutation_test_true}}
  \caption{True labels}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{.29\linewidth}
  \centering
  {\includegraphics[height=2.2in]{assets/figures/permutation_test/initial_predictions.pdf}\label{fig:permutation_test_model_predictions}}
  \caption{Predictions}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.40\linewidth}
  \centering
  {\includegraphics[height=2.2in]{assets/figures/permutation_test/samples.pdf}\label{fig:permutation_test_samples}}
  \caption{Samples}
  \end{subfigure}
  \begin{subfigure}[t]{\linewidth}
  {\includegraphics[width=1\textwidth]{assets/figures/permutation_test/distribution.pdf}\label{fig:permutation_test_distribution}}
  \caption{Observed differences in sample scores}
  \end{subfigure}
  \caption[Example: Permutation Test]{Permutation test example. A significance test is most often used to test whether an observed difference is a product of chance or indeed is significant. Significance tests only provide a probability, not a definite answer.}
  \label{fig:example_permutation_test}
\end{figure}


\labelsection{Implementation}{sec:implementation}
We mostly used Python to implement the code required to run the experiments.
The code and instructions on how set-up the experiments can be found on GitHub \footnote{\url{https://github.com/davidgengenbach/bachelor-thesis}}.
Our work heavily relies on open source software and in this form would not be possible without it.
In particular, we implemented most of the experiments using \textit{scikit-learn} \cite{Pedregosa2012}, \textit{networkx} \cite{Hagberg2008} and \textit{pandas} \cite{McKinney2010}, \textit{numpy/scipy} \cite{VanderWalt2011}, \textit{SymPy} \cite{Meurer2017} and several other open-source projects.

In the core of our implementation are \textit{sci-kit learn} pipelines, classifiers, eg. \textit{SVMs}, and transformers, eg. \textit{TfidfVectorizer} or a \textit{StandardScaler}.
Pipelines contain a number of transformers and can also consist of pipelines themselves.
Each step in the pipeline retrieves the output of the previous step as input.
The first transformer in a pipeline gets the user-provided input, eg. texts or graphs, as input.
At the end of a pipeline, in the last step, is a classifier which also retrieves the output from the step before, that is vector representations of the user-provided input.
We both use out-of-the-box transformers provided by the \textit{scikit-learn}, eg. the \textit{TfidfVectorizer} and the \textit{SVM}, and implement our own transformers, eg. our implementation of the Weisfeiler Lehman graph kernel.

Each step in a pipeline can be parametrized, eg. the $C$ parameter of the SVM classifier can be provided.
This in turn enables easy-to-implement parameter grid search which in turn can be easily parallelized.
We augmented this existing framework provided by \textit{scikit-learn} with several additions.
One of them providing the ability to define experiments, or test runs, with experiment definitions.
Here, we can provide all pipeline parameters, eg. the Weisfeiler-Lehman iterations $h$ or the $C$ for the SVM, in a single file and also define the used transformers in a pipeline.
Also, meta-parameters, for instance the number of cross-validation folds or the wanted classifier metrics should be evaluated on the test set, can be specified in these experiment definitions.
We can also do model-selection with these experiment definitions by specifying multiple parameter combinations that should be evaluated with cross-validation.
After finding the best parameter combination, these parameters are then evaluated once on a test set and the predictions results also saved to disk.
This enables us to quickly explore new parameter combinations and clearly define our experiments in a single file.

All experiment definitions with all parameters are provided online\footnote{\url{https://github.com/davidgengenbach/bachelor-thesis/tree/master/code/configs/experiments}} alongside the code.

\paragraph{Concept Map Extraction}
We used the code provided by Falke\footnote{\url{https://github.com/UKPLab/ijcnlp2017-cmaps}}  to create concept maps for our datasets.
In this section we will briefly describe the steps the code performs. For a more detailed explanation, see \cite{Falke2017}.

The input to the concept map extraction algorithm is a single text document. The output is a single concept map for this text document.

\textbf{Extraction}:
First, the algorithm extracts concepts and their relations to another.
This is done by extracting binary relations from the text. A binary relation consists of three parts: two concepts and the relation between them. An example for a binary relation is ``David likes something", here ``David" and ``something`` are the concepts and ``likes" the relation them. Note that the binary relation is directed, eg. ``Something likes David" is not the same binary relation as ``David likes something."
Also, a concept can consist of multiple words, eg. for ``David likes something else.", the concepts would be ``David" and ``something else".
The extracted concepts are later used as node labels, and the relation between them as the edges between concepts.

\textbf{Filtering}:
Next, the extracted relations get filtered so that only concepts are kept which contain at least one noun and which consist of fewer words than some a given threshold, in our case 10 words. This filtering is done to ensure the brevity and practical usefulness of the concepts when visualizing the graphs.
After these steps, we can create and export the concept maps.

\textbf{Summarization}:
The original implementation also summarizes the resulting graphs by finding the most relevant sub-graph of the concept map.
This is done to further summarize the graph and remove un-connected components from the graph, eg. connected components consisting of only two nodes.
We skipped this step in our concept map creation since this additional filtering would result in much smaller graphs which would in turn harden our classification task further.