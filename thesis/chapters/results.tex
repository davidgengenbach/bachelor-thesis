\subquestionrefnew{comparison_combined}

\subquestionrefnew{structure_diversity}

\subquestionrefnew{importance_structure}

\subquestionrefnew{multi_labels}

\subquestionrefnew{edge_labels}

\subquestionrefnew{infrequent_nodelabels}

\subquestionrefnew{relabeling_infrequent}

\subquestionrefnew{directed_vs_undirected}

\subquestionrefnew{concept_map_size}

\subquestionrefnew{comparison_coo}

\subquestionrefnew{comparison_text}

\subquestionrefnew{comparison_runtime}

\labelsubsection{Summary}{subsec:results_summary}
As we have observed in several of the answers, transforming the text classification task into graph classification and using concept maps is possible.
While we see near state-to-the-art performance with the graph-based approaches, the graph-only performance still lags behind text-only performance as we have seen in the results of Question \ref{question:comparison_text}.
We provided possible explanations in Question \ref{question:structure_diversity}, for example the low connectivity and the high compression factor of concept maps to name just a few.

Questions \ref{question:importance_structure}, \ref{question:multi_labels}, \ref{question:edge_labels}, \ref{question:infrequent_nodelabels}, \ref{question:relabeling_infrequent} and  \ref{question:directed_vs_undirected} all contain approaches to improve the graph-only performance and create more meaningful graph features.
While we, as said before, came close to the classification performance of the text-only approach, there most likely is only so much one can do to improve the classificatio score of concept maps.
In some of these questions, we explored the particularities of concept maps, namely (1) the multi-word node labels, (2) the directed edges and (3) the edge labels.

For (1), we tried splitting the multi-word concepts of the concept maps into single-word nodes.
This resulted in a high increase in classification score.
This was done in Question \ref{question:multi_labels}.

In Question \ref{question:importance_structure}, we also implicitly split the multi-word concepts. Since we linearized the graph into text again, ie. un-rolling the edges into sentences. Afterwards we used conventional, text-based approaches like uni-gram BoW extracted from the resulting text.
This approach resulted in the best classification performance we achieved using the concept maps, except when combining the graph features with text features.
This observation alludes to the explanation that the structure of concept maps is either not that important for classification or the structure could not be captured usefully using the graph-based approaches we tried.

For (2), the directed edges, we compared the classification results of both directed and un-directed version of the concept maps.
Using the directed edges outperformed the un-directed edges consistently and by a high margin.
We also provided a possible explanation, namely that the neighborhood of a node are far smaller when using directed edges.
Smaller neighborhoods in turn increase the likelihood for matched neighborhoods.

For (3), the edge labels, in Question \ref{question:edge_labels} we first analyzed the distribution of occurrences of edge labels. Here, we observed that most edge labels occur either only once or very often.
The top edge labels, ie. the edge labels which had the most occurrences in the concept maps of a dataset, were almost exclusively non-topic related words, like ``are" or ``is".
Next, we linearized the concept map into text with (a) all words and (b) no edge labels.
Finally, we used a Tfidf-BoW to create features and classify them.
Using or omitting the edge labels resulted in comparable performances, hinting to the observation that edge labels are not as important for the graph-based approach.

Finally, for the last Question \ref{question:comparison_runtime}, we report the classification times and classifier model size of our graph- and text based approaches.
Here, we observe that the runtime for our graph-based approaches is higher than that for the conventional text-based classification.
We also saw the additional overhead produced by the need to generate the concept maps from the text.

One thing to note, and this is common to all the questions and observations in this work, is that we evaluated our questions on multiple datasets. Often times, the observations varied quite heavily from dataset to dataset.
For instance, the performance of one WL extensions might show great improvement on one dataset, while showing a low performance on another dataset. 
Especially when trying to find correlations and explanations for the performance of our approaches, the differing datasets proved to harden the task.
This further highlights the importance of using model selection.
Arguably more than in other domains,  graph-kernel based classification performance relies heavily on the used graphs, structure, graph kernels and particularities of the domain.
We evaluated several other graph kernels, for instance shortest-path- or random-walk based graph kernels, and observed poorer performance than with our default graph kernel, Weisfeiler-Lehman.
Yet, even with the Weisfeiler-Lehman kernel with our extensions and several graph pre-processing approaches, we did not outperform the linearized graph approach, ie. where we un-roll the concept map into text then use a conventional BoW-based approach.
A possible explanation for this observation is, that the concept maps are generated from the text by extracting multi-word concepts and their relation to each other from the text.
In order for this approach to create structurally interesting concept maps, concepts have to occur multiple times.
If that is not the case, that is when the concepts occur only once, the resulting concept maps consist of a number of connected components which contain only two nodes and one edge.
These two-node connected components are like tri-grams, with multi-concepts and edge label instead of single-words.
A similar observation was done in Question \ref{question:importance_structure}.

In the next Section \ref{sec:conclusions} we will further summarize our results and provide some approaches we have not tried yet which could further improve the classification scores.

\labelsection{Related And Intermediate Observations}{subsec:related_and_intermediate_observation}
In this section we report observations which are not directly related to our hypothesis, but have provided us valuable insights into our analysis.

\subsection{Weisfeiler-Lehman $\phi$ Feature Map Visualization}
When debugging our Weisfeiler-Lehman implementation and extensions, we often wanted to see the effect on the resulting feature vectors.
For this, we plotted the feature vector $\phi(G)$ for each graph $G$ for several WL iterations.
In Figure \ref{fig:phi_distribution_example} we see an example of such a $\phi$ distribution plot.

\begin{figure}[htb!]
	\centering
	{\includegraphics{assets/figures/wl_phi_distributions/dataset_graph_concept_map_ng20-v3_phi_npy.png}
		\caption[Example: $\phi$ distribution plot]{%
			Example of a WL $\phi$ distribution plot.
			For each WL iteration $h$, the horizontal $x$-axis corresponds to the concept maps in the datasets, while the vertical $y$-axis corresponds to the indices of $\phi(G)$ which are non-zero, ie. when there is a point at $x=0$ and $y=10$, the graph $G_0$ has the label $10$.
			There are as many points in this plot as there are vertices in the dataset.
			The colors of a point corresponds to the class of the graph, in this case the 20 classes of \textit{ng20}.
			The red line marks the number of vertices in the datasets which is also the dimension of the feature map $\phi$.
			Dataset: \textit{ng20}.
			\textit{Note: embedding this plot in a vector format  was not possible due to the sheer number of points to be drawn. The rasterized image seen here interpolates the individual points.}
		}%
		\label{fig:phi_distribution_example}}
\end{figure}

It has to be noted that we implemented WL where the labels are assigned when they are first encountered. That means that when we create the feature map $\phi(G_i)$ for graph $G_i$, we relabel the graph by assigning it a new multi-label by taking the neighborhood into account (\textit{recoloring}). In the next step, we compress the new multi-label by assigning it a new number if it has not been encountered before and save it in a multi-label-lookup. If the multi-label has been encountered before, ie. the multi-label is in the multi-label-lookup, we assign it the number it has been assigned before.
This is the \textit{label compression} step.
So, we iterate over the graphs one by one and create feature maps $\phi$.
When the first processed graph, $G_1$, has the new compressed labels $\{1, 2, \ldots, n\}$, the next processed graph, $G_2$, will have labels that are either \textbf{(1)} new, in which case they get a new, compressed label that is higher than $n$ or \textbf{(2)} they were already encountered in $G_1$, so they get the label from the lookup.
This way we get incrementing labels for each graph.
When we also sort the graphs by their assigned class, ie. all graphs of class $y_i$ are processed after another, then one can look at the labels that are assigned to each class and also see the nice pattern which arises in Figure \ref{fig:phi_distribution_example}.

This plot also gives an insight about the convergence of WL.
The highest encountered $\phi$ index in iteration $h=1$ must be higher or equal than the highest $\phi$ index encountered in $h=0$.
This is directly due to the fact that more colors are needed to color the graph nodes in higher iterations - if WL has not converged.
The number of labels, or colors, $|L_i|$ assigned in iteration $h=i$ must be smaller or equal than the labels for iteration $h=i+1$, ie. $|L_i| \leq |L_{i + 1}|$.
When $|L_i| = |L_{i + 1}|$, WL is said to have converged, ie. the number of colors will not change anymore.

In this figure, there is also a highest horizontal, green line.
This line signifies the highest $\phi$ index where there was a non-zero entry.
It also serves as a marker of the last compressed label number which was assigned in this dataset.

While this kind of plot is useful for analyzing the whole datasets at once, it becomes especially interesting when splitting the dataset into sets, eg. by splitting it into a stratified train- and test set.
When creating the feature maps $\phi$ for the graphs in the train set, for each WL iteration $h$ we save the multi-label-lookup which we is then used to create the feature maps for the test set.
The multi-label lookup therefor acts as a kind of continuation, saving the state of the WL iteration with all encountered labels.
Then, when creating the feature maps for the test set, we use the multi-label lookups to generate the feature maps in the same way, now for the graphs in the test set.
When plotting the train- and test set feature maps separated we can get interesting insights into the dataset and the concept maps.
For an example, see Figure \ref{fig:phi_distribution_train_test}.

\begin{figure}[htb!]
	\begin{subfigure}[t]{.49\linewidth}%
		{\includegraphics{assets/figures/wl_phi_distributions/dataset_graph_concept_map_ng20-v3_splitted_phi_npy_train.png}}\caption{Train}%
	\end{subfigure}%
	\begin{subfigure}[t]{.49\linewidth}%
		{\includegraphics{assets/figures/wl_phi_distributions/dataset_graph_concept_map_ng20-v3_splitted_phi_npy_test.png}}\caption{Test}%
	\end{subfigure}%
	\caption[Diagram: $\phi$ distribution plot for \textit{ng20}.]{WL $\phi$ distribution with a stratified train/test split.}%
	\label{fig:phi_distribution_train_test}
\end{figure}

While the train set looks nearly the same as if plotting the simple dataset without the split, the test set differs quite a bit.
Notice how, in the test set, there are ``clusters" around some regions for each of the classes. 
Keep in mind that the $y$-position of the point signifies a non-zero entry at index $i$ in $\phi(G)$ for that graph.
Because of the aforementioned implementation, we now are able to see a pattern for each of the classes, namely the clusters.
For the test set, we can also see how many new labels have been found in a given iteration.
As we said before, the horizontal green line, in both plots, signifies the last label that has been encountered.
In the case of the split sets, it also marks the last label that has been encountered in the train set.
All points, that is labels, above this line have are new to the test set and do not occur in the train set.

Plotting the $\phi$ distribution like so, we gain a better insight into the graph connectedness and the similarity or uniqueness of neighborhoods.
When WL converges at iteration $h$, so when the height of the green line does not change from iteration $h-1$ to $h$, this means that all neighborhoods that are different, have been marked as so and got a unique label.
If the number of labels is equal to the number of vertices, all neighborhoods are different.

%\todo{Add co-occurrence graph feature map visualization?}

\subsection{Weisfeiler-Lehman Node Weighting Extension}
With increasing iterations $h$ of the Weisfeiler-Lehman algorithm, exact matches of subtrees of height $h$ become more difficult.
In iteration 0, WL only counts the number of node labels in the graphs, for iteration 1 it takes the immediate neighborhood of the nodes into account, for iteration 2 is takes the neighborhood of the neighborhood into account and so on.
As a result, \textbf{(1)} with higher iterations, the probability of having an exact match decreases. Another difficulty arises for nodes with a high number of neighbors. \textbf{(2)} The probability of an exact match also decreases with a higher degree.

These two difficulties, \textbf{(1)} the increased difficulty of a match with higher iterations and \textbf{(2)} the increased difficulty of a match for nodes with higher degrees, are not addressed when using Weisfeiler-Lehman in its plain version. Both issues are encoded neither in the features maps nor are they considered when creating the gram matrix.

As a possible solution for these issues, we propose an extension to the Weisfeiler-Lehman algorithm.
Our extension augments the WL algorithm by adding node weights.
For each of the nodes $n$ in all graphs, we first calculate node weight $n_w$ which encodes the importance of the nodes or the difficulty of getting an exact match in WL.
An example of such node weights are the node degrees or node weights extracted by the \textit{PageRank} algorithm \cite{Page1998}.
The node degree is an interesting metric for node weights since it actually encodes the size of the immediate neighborhood of a node and therefore could possibly address both issues, \textbf{(1)} and \textbf{(2)}.

One could see our extension as similar to binary \textit{BoW}, ie. only using whether a word has occurred or not, and the extension that also uses the term frequency where the number of occurrences is taken into account, too.

When using this extension, one has to be careful with feature scaling. As we use the node weights to scale individual features, using a separate feature-wise scaler would effectively revert our weighting.

\paragraph{Experiment}
To test our extension, we compare results of using WL feature maps with and without our extension.
We classify our concept maps datasets as well as a number of other graph benchmark datasets, obtained from \cite{Kersting2016}.
For statistics about the additional datasets, consult the author's website\footnote{\url{https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets}}.
We only used datasets from the website consisting of more than 1000 graphs and which contain node labels.

\begin{table}[htb!]
	\centering
	\begin{tabular}{lrrr}
		\toprule
		{} & \multicolumn{2}{c}{F1 macro} & {$p$-value} \\
		{} &  Plain &  Node-Weights  &  \\
		\midrule
		ling-spam       & \textbf{0.8160} & 0.7760 & 0.0542 \\
		ng20            & \textbf{0.4188} & 0.4127 & 0.2340 \\
		nyt\_200         &\textbf{ 0.7436} & 0.7150 & 0.3421 \\
		r8              & \textbf{0.6772} & 0.6467 & 0.1270 \\
		review\_polarity & \textbf{0.6094} & 0.6045 & 0.8406 \\
		rotten\_imdb     & \textbf{0.6346} & 0.6338 & 0.8728 \\
		ted\_talks       & 0.2436 & \textbf{0.2984} & 0.2605 \\
		\midrule
		AIDS             & 0.9560 & \textbf{*0.9841} & 0.0146 \\
		DD               & 0.6602 & \textbf{*0.7733} & 0.0002 \\
		Mutagenicity     & \textbf{0.7702} & 0.7625 & 0.3007 \\
		NCI1             & \textbf{0.8425} & 0.8334 & 0.2821 \\
		NCI109           & \textbf{0.7929} & 0.7877 & 0.4971 \\
		PROTEINS         & 0.7266 & \textbf{0.7317} & 0.8554 \\
		Tox21\_AHR        & 0.7009 & \textbf{0.7056} & 0.6705 \\
		Tox21\_AR         & \textbf{0.8147} & 0.7978 & 0.1172 \\
		Tox21\_AR-LBD     & \textbf{*0.8477} & 0.8284 & 0.2318 \\
		Tox21\_ARE        & 0.6722 & \textbf{0.6917} & 0.1378 \\
		Tox21\_ATAD5      & \textbf{0.7456} & 0.7426 & 0.7449 \\
		Tox21\_ER         & \textbf{0.6860} & 0.6747 & 0.0868 \\
		Tox21\_ER\_LBD     & \textbf{0.7335} & 0.7180 & 0.0648 \\
		Tox21\_HSE        & 0.6156 & \textbf{*0.6499} & 0.0384 \\
		Tox21\_MMP        & 0.7663 & \textbf{0.7682} & 0.7874 \\
		Tox21\_PPAR-gamma & \textbf{0.7006} & 0.6910 & 0.7970 \\
		Tox21\_aromatase  & 0.6571 & \textbf{0.6876} & 0.1642 \\
		Tox21\_p53        & 0.7214 & \textbf{0.7375} & 0.3403 \\
		\bottomrule
	\end{tabular}
	\caption[Results: Classification using node weight WL extension]{F1 macro classification results for  the node weight extension.}\label{table:wl_node_weight_extension}
\end{table}

As we can see, the classification performance of our extension varies greatly per dataset.
Especially on the concept maps, our WL extension does not perform as good as plain WL.
On some of the graph benchmark datasets, on the other hand, we can observe significant improvements.
While these results are interesting, a more throughout analysis of the performance is needed to assert the usefulness of this extension.
Especially adding further normalization might prove to be effective.

\if
We also tried another extension to the Weisfeiler-Lehman graph kernel, namely augmenting the default node label counts with a iteration factor.
The feature maps of WL consist of the concatenated feature maps for each iteration.
For this extension, we propose weighting the feature maps of each iteration $h$ by a factor given by a function $f(h)$.
So, for a feature map, $\phi(G)$, of a graph $G$ for $h=2$ iterations, the feature map can be decomposed into the feature maps of each of the two $h$ iterations:
\begin{equation*}
\phi(G)=(\phi_{h=1}(G), \phi_{h=2}(G))
\end{equation*}
We now propose to weight the individual feature maps by the factor determined by function $f(h)$, so:
\begin{equation*}
\phi_{f}(G)=(f(1) \cdot \phi_{h=1}(G), f(2) \cdot \phi_{h=2}(G))
\end{equation*}
So, basically the function $f(h)$ works as a decaying factor to decreasing iterations $h$.
When the function $f(h)$ increasing, the increasing importance with higher iterations is encoded into the resulting feature map $\phi_f(G)$.
Of course, also this extension 
\fi