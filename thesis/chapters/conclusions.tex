In our work, we conducted several experiments to test the usefulness of graph-representations of text, especially concept maps, for text classification.
Our experiments ranged from gathering information about the structure of the graphs to actually performing the classification task with different approaches.
For our graph-based classification, we mostly capitalized on graph kernels.
Through all these experiments, we aimed to leverage the particular properties of concept maps, especially the structure.

When directly comparing the performance of graph-based and conventional text-based classification, we saw that the text-only features outperformed the graph-based approach by a high margin, for both co-occurrence graphs and concept maps.
So, our next approach was to combine text- and graph features and classify them together.
This combined approach, while having a far higher runtime due to the increased dimension of the feature vectors, presents no significant improvement of classification scores upon the text-only approach.
On some datasets, the classification score even was a little lower than the text-only approach.
This is also most likely due to the high dimensionality of the combined features and the subsequent risk of overfitting to too specific features.

However, after re-evaluating and extending our graph-based approach, we saw significant improvement in our graph-only classification performance.
Especially splitting the multi-word node labels of the concept maps into single-word nodes resulted in significant improvement on all datasets.
As another extension, we pre-processed the graphs by removing infrequent nodes to further reduce the dimensionality of the resulting feature vectors.
Since we capitalized on the Weisfeiler-Lehman graph kernel to extract our graph features, our guess was that the removal of infrequent words could improve the quality of the features since WL, roughly speaking, counts the matching neighborhoods of nodes.
So, a node label which only occurs infrequently could ``taint" its neighborhood, making an exact match of that neighborhood less likely.
However, when classifying the graphs with the infrequent labels removed, we actually saw mixed results, ranging from great improvements on some datasets to lower scores on others.
Another approach we evaluated was merging infrequent nodes by creating embeddings for each (multi-word) node label using both pre-trained embeddings and creating our own word2vec embeddings from the text.
In the next step, we merged node labels with a similarity above some given threshold.
We then assigned identifiers to the resulting label clusters.
Finally, we relabeled each node in the concept with the identifier of its cluster.
On most datasets, this approach improved the graph-only classification scores.

Besides all these Weisfeiler-Lehman specific extensions and improvements, we also evaluated ``linearizing" the concept maps into text.
By un-rolling the graph into text, we were able to perform text-based classification on them, which - surprisingly - provided the highest classification score we achieved using graphs, for both co-occurrence graphs and concept maps.
For this ``graph kernel" we discarded all structural information since we only used uni-grams, ie. single word frequencies.
In the case of co-occurrence graphs, the performance was nearly as good as the text-only approach, which is not entirely surprising since co-occurrence graphs capture \textit{nearly} all information beside the word order which was not used by our uni-gram \textit{BoW} approach anyway.
For concept maps, the classification scores with this simple linearized graph kernel were also the best, yet still approximately 5-10\% lower in the F1 macro score than both co-occurrence graphs and the text-only approach.

We subsequently repeat the experiments of text- and graph combined features with our extensions, eg. the multi-word label splitting or the relabeling.
Here, interestingly, the classification performance was lower than without these extensions, so with the plain WL graph kernel.
This observation is somewhat surprising since these extensions all improved the classification score when only classifying the graphs.
One possible explanation is, that all these WL extensions aim to transform the concept maps into graphs where the neighborhoods are equalized, ie. more similar to each other.
For instance, the relabeling extension aims to merge infrequent with frequent labels, in order to increase the likelihood of same neighborhoods.
On the other hand, doing this also removes (structural) information from the concept maps, which can not be used for text classification.
For the multi-word label splitting extension, one possible explanation for the lower classification performance when combining the resulting features with text features, is, that it significantly increases the dimensionality of the feature vectors created by WL.
This, in turn, might have lead to overfitting.

Apart the graph kernel customizations we devised for the concept maps, eg. splitting the labels or using directed instead of un-directed edges, we also proposed our own extension to WL, namely node weighting.
Here, our intuition was taken from the fact that WL matches of big neighborhoods are far more difficult than with low-size neighborhoods.
Since this issue is not explicitly encoded in the feature maps of WL, we proposed to scale each feature map entry, corresponding to the label of a node, with a node weight, eg. the degree of that node.
That way, the importance, or frequency, of a node is also encoded in the feature map.
Although this resulted in lower classification scores for our concept map datasets, we saw significant improvement on datasets obtained from \cite{Kersting2016}.
However, a more throughout analysis would be appropriate to confirm the usefulness of our WL extension.

Beyond all the different approaches and experiments, the importance of evaluating the classification scores on different datasets became clear immediately.
One approach leading to a great improvement on some datasets, might actually lead to far lower scores on other datasets.
This once more highlights the importance of appropriate model-selection per dataset, especially when using a graph-based approach.
Understanding the trade-offs of different graph kernels is crucial to achieve higher performance.
The information graph kernels capture varies widely, ranging from simple counts of node labels to more sophisticated structural information.
Therefore, knowing the structure and particularities of the processed graphs is essential to achieve higher classification performance.
In our case, the graph kernel that performed best actually translated the graphs back into text.

Even though we were not able to augment the text-based classification toolbox by another approach, that is graph-based features, we nonetheless explored a number of extensions to existing graph kernels which could be useful for other graph-based tasks.
In our work, we started from text, created graph representations and then classified them to evaluate the usefulness of graph representations in text classification.
However and importantly, concept maps and other graphs can also be created from scratch or extracted automatically from non-text sources, eg. knowledge databases.
While text is currently arguably the most important information medium, maybe - with the rise of more interactive media - concept maps become of greater importance in the future.
There are several advantages of concept maps over text.
For instance, to understand a paragraph at the end of a text, one often must have read the preceding text.
Relations between concepts in a text are often implicit and have to be inferred from the context.
For a reader, a mental picture of the content of the text therefor must carefully be created by the author of the text.
Several levels of detail have to be assessed and introduced by the author, thoughtfully connecting concepts.
In concept maps, on the other hand, concepts have explicit relations to each other.
One can start at every node of a concept map and explore the relationships between concepts by following edges.
This enables the non-linear and visual exploration of the topic of a concept map.
Apart from that, one might also imagine creating concept maps with different levels of details, therefor adding the possibility to add/remove parts of the graph based on their level of detail.
One can think of guided tours through concept maps where parts of the graphs get revealed after each other.
The interactive possibilities of concept maps are numerous, the key being their easy modifiability.
While adding information to texts can be quite laborious since one must find the appropriate section in the text where to add the new parts, with concept maps, on the other hand, extending the graph is as easy as adding a new node or edge to the graph.
Merging multiple concept maps with common concepts is also far easier since one must \textit{only} merge common nodes/concepts.
All these properties and possibilities lead to our opinion that concept maps will become an important information medium alongside text.
Not only side by side, but also a merging of the two mediums is conceivable.
For instance, concept maps could not only have nodes with single labels but assigned texts, images and so on.
Several fields could greatly benefit from concept maps, not only in the learning context \cite{Novak1984}.

%So, while we were not able to achieve a classification score improvement by augmenting text classification with concept map based features, our observations and approaches nevertheless can be useful for other, graph-related tasks.

\labelsection{Future Work}{subsec:future_work}

\epigraph{If I had to reduce all of educational psychology to just one principle, I would say this: The most important single factor influencing learning is what the learner already knows. Ascertain this and teach him accordingly.}{--- \textup{David Paul Ausubel}, Educational Psychology: A Cognitive View \cite{Ausubel1968}}

There are several aspects which have not been covered in this work.
New graph kernels and approaches to graph processing appear constantly.
These new approaches could also be explored in their usefulness for text-classification.
In our work, we capitalized on the Weisfeiler-Lehman graph kernel, not only because its ability to extract explicit feature maps which in turn could be combined with text features, eg. from BoW.
We proposed an extension to the WL kernel where we augment the plain WL with node weights. Here, it would be interesting to further evaluate the usefulness of this extension on other datasets and with additional normalization.
That said, there are numerous other graph kernels which might prove to be more useful for our task than WL.
Applying them with their different extensions to better suit the particularities of concept maps could result in the performance improvement which we were not able to achieve with our approach.
As we have seen, selecting an appropriate graph kernel is of high importance when doing kernel-based graph processing.

Exploring the differences of concept maps of different datasets could also provide useful insights and improvements for the creation of concept maps.
As we saw, the performance and different graph kernels, and their extensions, resulted in often widely varying classification performance on different datasets, despite fact that the concept maps were all generated by the exact same algorithm.
Finding correlations between the classification performance and properties of concept maps might give new ideas for more suited graph kernels for the classification task.

In our work, we mainly explored concept maps as a text representation.
However, there are a number of other structured representations, not limited to co-occurrence graphs or (semantic) parsing trees.
These other graph representations might also capture structural information about the underlying text and could be interesting candidates for further exploration.
Here, the idea is also to specially leverage the particularities and structures of these graph types.

As a more far-fetching idea, one could also research how the modifiability and composability of concept maps can be harnessed more effectively in the learning context to enable more personalized learning experiences.
For example, when a person wants to learn about some topic using a concept map, he/she most likely has some previous knowledge about that topic.
Providing him/her the same information as everyone else might be the traditional way to go. 
However, providing the person with a more personalized concept map could prove to be useful in removing a lot of information overhead and enhance the learning experience.
Such a personalized concept map could, for example, omit information the user has explored before and has previous knowledge about.
Or, add connections to other topics the person already knows as an entry point.
Also, learning the learning preferences of a user, or giving him options to easily modify/extend a given concept map (semi-) automatically could benefit concept maps becoming a more important information medium.
Observations about the behavior of users of such a learning system in turn could give better insight into possible improvements to automatic concept map creation.
For all this, the key difference to text as an information medium is that concept maps are easily modifiable and composable.
In this idea we envision here, the concept maps could all be interconnected, effectively creating a big knowledge network in which the user can explore given topics in a non-linear way.
Here, a new task arises, namely to classify subgraphs of the big concept map which can provide an entry point for a given topic, maybe also under user-provided constraints, eg. the previous knowledge of the user.
Another task would entail further summarizing concept maps, effectively changing the level of detail.
For example, given three concepts $c_1$, $c_2$, and $c_3$, where $c_1$ is connected to $c_2$ and $c_2$ to $c_3$, how can we remove node $c_2$ and add an edge from $c_1$ to $c_3$ in a meaningful way.
In this example, if the two concept $c_1$ and $c_3$ are important concepts and $c_2$ is less important by some metric, eg. its node degree or some metric of abstractness/detail, this procedure could lower the level of detail of the concept map.
This in turn could enable the interactive exploration of concept maps with varying levels of details.
So, instead of approaching the topic of concept maps as yet another way to represent text, this proposed idea would really leverage the particularities and advantages of concept maps and graphs in general.
Utilizing these aspects in learning contexts might then improve the usefulness of concept maps in other domains.