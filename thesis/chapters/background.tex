\todo{Graphs, eg. concept maps, co-occurrence graphs}
\todo{Classical text representations, eg.tfidf
Categories of algorithms, eg. algorithms on feature vectors, kernelized - algorithms}

\todo{Lay groundwork for the later description of the problem}
\todo{Introductions to used classifiers, concepts}
\todo{Concretize the problem and its notation}
\todo{Comparison of the used data sources}

\labelsubsection{Classification Task}{subsec:classification_task}
\todo{Supervised vs. unsupervised}

\labelsubsection{Graphs}{subsec:graphs}
A (labeled) graph is a triple $G = (V, E, \pi)$, where $V$ is the set of vertices or nodes, $E \subseteq V \times V$ are the edges and $\pi: V \to X$ is a labeling function which assigns a label to a given vertex.

A graph is called undirected when $\forall (v_1, v_2) \in E: (v_2, v_1) \in E$, otherwise it is called directed.

A graph $G'=(V', E', \pi)$ is called a subgraph of the graph $G = (V, E, \pi)$ when $V' \subseteq V$, $E' \subseteq E$ and $\forall (v, v') \in V': v \in V' \land v' in V'$.

The neighbours $neighbours(v)$ of a node $v$ are defined as the set $neighbours(v) = \{v' | (v, v') \in E \}$.

\paragraph{Walks and Paths}
A walk on a graph is a (finite) sequence of edges, $w = (e_1, \cdots, e_n)$ with the constraint that $\forall 0 < i < n: \forall e_i = (v_1, v_2) \in w: \exists v_3: e_{i + 1} = (v_2, v_3)$. The length of the walk is $n$.
The set of all possible walks is denoted $W_G$.
A random walk starting from vertex $v$ is a walk where the first element of walk $w$ is $e = (v, v')$ for some $v'$.

A path is the sequence of the vertices visited on a walk, ie. for the walk $w = ((v_1, v_2), (v_2, v_3), \dots, (v_{n-1}, v_n)$, the path is $(v_1, v_2, v_3, \dots, v_n)$.
The set of all possible paths is denoted $P_G$.

The distance $d(v, v')$ between two vertices is defined as the length of the shortest path between them.

\paragraph{Connected components}
A connected component $c$ of a graph is a set of vertices with the constraint $\forall v, v' \in c: \exists p \in P_G: v \in p \land v' \in p$, ie. there exists a path between every vertex in the connected component.
The connected component $c_v$ of a vertex $v$ is the connected component $c$ of a graph where $v \in c$.
The set of all connected components of a graph is $conn\_comp(G) = \{ c_v | v \in V \}$. A graph is called connected when there is only one connected component. Only the empty graph has zero connected components.

\paragraph{Degree}
The in-degree $degree_{in}$ of a vertex $v \in V$ is $degree_{in}(v) = |\{v | (v_1, v_2) \in E \land v_2 = v\}|$.
The out-degree $degree_{out}$ is analogously defined as $degree_{out}(v) = |\{v | (v_1, v_2) \in E \land v_1 = v\}|$.
The degree of a vertex $v$ is $degree(v) = degree_{in}(v) + degree_{out}(v)$ for directed graphs, and $degree(v) = degree_{in}$ for undirected graphs.

\begin{figure}[ht]
\centering
\begin{tabular}{ll}
symbol &  meaning \\
\midrule
G & Graph \\
$E$ & Set of edges \\
$V$ & Set of vertices \\
$\pi(v)$ & Vertex labeling function \\
$neighbours(v)$ & Neighbours of vertex $v$ \\
$W$ & All possible graph walks on graph $G$ \\
$w$ & Graph walk \\
$P$ & All possible paths on graph $G$ \\
$p$ & Path \\
$d(v, v')$ & Shortest path length between $v$ and $v'$ \\
$conn\_comp(G)$ & Set of all connected components of $G$ \\
$c(v)$ & Connected component of $v$ \\
\end{tabular}
\end{figure}

\subsubsection{Concept Map}
A concept map is a (directed) graph where the nodes are concepts.
Concepts are tokens in a text.
Concepts consist of one or more words.
A directed edge between two concepts show that these concepts are related to each other. The edges can have labels, too.
For an example of a concept map, see Figure \ref{fig:concept_map}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\linewidth]{assets/figures/concept_map.pdf}
\caption{Example of a concept map. The nodes are the concepts. The directed edges connect the concepts to each other and have labels. In this example of a concept map, one could recover sentences from this graph, for example ``Caffeine \textit{is} a mild CNS stimulant". Taken and adapted from: \cite{Falke2017}}
\label{fig:concept_map}
\end{figure}

Concept maps are useful for visualizing concepts and their relation to each other.
Concept maps can thus be used to quickly explore a given topic and immediately see connections between concepts.
Through the degree of a concept, one can also infer the relative importance of that concept in the underlying text.

By combining different texts of the same topic, one can also create a concept map for that topic.
The concepts are not confined to a single text in this case.
The visualization with concept maps of one or more individual texts therefor enables the non-linear exploration of multiple texts of a topic.

Because the concepts in a concept map often contain only a small subset of the text, concept maps also summarize the underlying text.
Only relevant concepts and their relation to each other are captured, making concept maps interesting for (multi-document) text summarization, too.

\todo{In Chapter \ref{sec:implementation} we introduce the concept map extraction implementation we used and further explain the steps in creating the concept maps.}


\todo{"Capture important concepts and the structure of texts (spanning over whole text)"}
\todo{Advantages}
\todo{Text summarization}

\subsubsection{Co-Occurrence Graph}
A co-occurrence graph, or graph-of-words, is generated from a text by creating a (undirected) graph with the words of the text as nodes.
There is an edge between two nodes if their labels co-occur in the text.
Two words co-occur when the distance between the words in the text is below a given threshold, the window size.

Figure \ref{fig:cooccurrence_graphs} shows examples of co-occurrence graphs with different windows sizes.

\begin{figure}[ht]%
    \centering
    \subfloat[Window size: 1]{{\includegraphics[width=0.3\textwidth]{assets/figures/cooccurrence_graph_1.pdf} }}%
    \subfloat[Window size: 2]{{\includegraphics[width=0.3\textwidth]{assets/figures/cooccurrence_graph_2.pdf} }}%
    \subfloat[Window size: 3]{{\includegraphics[width=0.3\textwidth]{assets/figures/cooccurrence_graph_3.pdf} }}%
    \caption{Example for co-occurrence graphs. Generated from the sentence: ``Quotation is a serviceable substitute for wit". With increasing window size the number of edges and therefor the connectedness of the co-occurrence graph also increases.}%
    \label{fig:cooccurrence_graphs}%
\end{figure}

Co-occurrence graphs not only contain the words of the underlying text but also their co-occurrence. Thus they capture not only the content of the text but also the relation between words of the text.

\labelsubsection{Graph Kernel}{subsec:graph_kernel}
In most cases, texts and graphs are not of a fixed size.
However, most classification algorithms can only operate on fixed size vectors.
Fortunately, there are several approaches to overcome this problem.
One of these approaches are so-called kernels.

A kernel is a function $k$ which returns a measure of similarity between two objects:
\begin{equation*}
k: X \times X \rightarrow \mathbb{Q}
\end{equation*}

A kernel function has to be:
\begin{itemize}
    \item{\textit{symmetric}: $k(x, x') = k(x', x)$}
    \item{\textit{non-negative definite}: \dots}
\end{itemize}

For example, a simple kernel on two numbers (or vectors) is $k(x, x') = | x - x' |$, where $| \cdot |$ is the absolute value (or a norm).
Another example of a kernel between strings is $k(s, s') = \delta(s = s')$, where $\delta(\cdot)$ is the Kronecker delta. This kernel returns 1 if the two given strings are the same, and 0 if they are different.
Note that indeed both these kernels are valid.

Kernel functions can be used directly in kernel methods for classification and other tasks.
Prominent examples of kernel methods are the SVM, kernelized PCA and Gaussian Processes. For a subset of kernels, one can also create explicit feature vectors for a given object. In this case, the kernel can be written as an inner product on two fixed-size vectors:
\begin{equation*}
    k(X, X') = \langle \phi(X), \phi(X') \rangle
\end{equation*}
Here, $\phi(X)$ returns a fixed-size vector representation of the object $X$ and is the main part of the kernel.
This $\phi(X)$ can then be used in all vector-based classification algorithms, not only kernel methods.
An example for such a kernel is the string kernel which counts the number of occurrences of words in a given text, then creates a vector representation by giving each word an index in the vector and setting this index to the number of occurrences of that word. The inner product of two of these vector representations then create a kernel.

A graph kernel is simply a kernel that operates on graphs. A graph kernel returns a measure of similarity between two graphs.

Graph kernels that measure the similarity of two graphs are related to the isomorphism test between two graphs.
The graph isomorphism test returns whether two graphs are structurally the same.
In this test, the labels of the vertices and edges are ignored. Otherwise the task would be far easier, since one could only compare the labels and edges of the graphs - when they are the same, the graphs are the same.
Instead, in the graph isomorphism test, one has to find a mapping between the vertices of one graph to another, the same for the edges.
\inlinetodo{Explain constraints for the mapping}
If such a mapping exists, the two graphs are called isomorphic.
While the complexity of the graph isomorphism test is not known yet, it is suspected to lie in P or maybe is also NP-complete. This potentially super-polynomial complexity has grave consequences for a graph kernel.
\todo{Namely?}

\todo{Difference of graph isomorphism and graph kernels}

\todo{Creating a graph kernel is always a trade-off}
\todo{Kernels can be combined!}
\todo{Mention similarity of graph kernel task to graph isomorphism test}
\todo{Explain when $\phi$ can be constructed (Mercer's theorem: an implicitly defined function $\varphi$  exists whenever the space $\mathcal{X}$ can be equipped with a suitable measure ensuring the function $k$ satisfies Mercer's condition.)}

\paragraph{Gram matrix}
A gram or kernel matrix for a given kernel is a matrix $A$ where the entries are the pairwise similarities of two objects in a dataset:
\begin{equation*}
    A_{i,j} = k(x_i, x_j)
\end{equation*}
The gram matrix is symmetric by definition since the underlying kernel is also symmetric, so $A_{i,j} = A_{j, i}$.

\paragraph{Graph Kernel Categories}
Several graph kernels have been proposed in the literature.
Categories of graph kernels include graph kernels based on shortest-paths, random-walks and subtree patterns, to name just a few.
Most approaches to graph kernels have in common that they first create a decomposition of the graph into sub-structures, for example into subtrees, and then count the occurrences of these sub-structures in the graphs.

\todo{Mention R-Convolution?}

\subsubsection{Examples}
\paragraph{Simple Non-Structural Graph Kernel}
As the first example for a graph kernel we will investigate a simple kernel which actually discards all structural information about the graph. This graph kernel only operates on the labels of the vertices:
\begin{equation*}
k_{common\_label}(G_1, G_2) = | l(G_1) \cap l(G_2) |
\end{equation*}
where $l(G)$ returns the set of labels for graph $G$.
Therefor, this graph kernel counts the number of common labels of the two graphs. Under this kernel, two graphs are the same when they have the same node labels, completely ignoring the edges.

An extension to this graph kernel is using the Jaccard coefficient instead of simply counting the common node labels:
\begin{equation*}
k_{common\_label\_jaccard}(G_1, G_2) = \frac{| l(G_1) \cap l(G_2) |}{| l(G_1) \cup l(G_2) |}
\end{equation*}
This extension normalizes the function and confines the codomain of $k$ to $[0, 1]$.

\paragraph{Simple Structure-Only Graph Kernel}
Another example for a graph kernel which only operates on the structure of the graph, is the following:
\begin{equation*}
k_{simple\_structure}(G_1, G_2) = \langle \phi(G_1), \phi(G_1) \rangle
\end{equation*}
with $\phi(G) = (n_{triangle}(G), n_{rectangle}(G))^T$. Here, $n_{triangle}$ is the number of the triangle subgraphs in $G$ and $n_{rectangle}$ the number of rectangle subgraphs in $G$.
This graph kernel is an example of a kernel where the explicit feature map $\phi(G)$ can be computed directly and completely independent of other graphs.

\todo{This kernel actually is quite similar to a real-world kernel named graphlet kernel. In the graphlet kernel, the sub-structures are not triangles and rectangles, but graphlets.}

\paragraph{Weisfeiler-Lehman Graph Kernel}
The Weisfeiler-Lehman graph kernel is also a kernel that works by counting common sub-structures on two graphs.
Here the sub-structures are the subtrees starting from each vertex.
In each iteration $i$ with $0 < i < h$ for a given number of iterations $h$ (or till convergence), the WL graph kernel relabels the vertices of the two given graphs. This relabeling is also called recoloring.

The WL graph kernel relabels a vertex $v$ by concatenating two components:
\begin{enumerate}
    \item{the current label of the vertex: $\pi_{i-1}(v)$}
    \item{the sorted labels of the neighbourhood: $\{ \pi_{i-1}(v') | v' \in neighbours(v) \}$}
\end{enumerate}
where $\pi_{i}$ is the labeling function for the iteration $i$ and $\pi_0 = \pi$.
The resulting new label of $v$ is the sequence $(\pi_{i-1}(v), \pi_{i-1}(v_{neighbour,1}), \dots , \pi_{i-1}(v_{neighbour,n}))$.

The new labeling function is updated accordingly: $\pi_i(v)$ now returns to the new label of $v$.


\todo{Mention complexity}
\todo{Explain extensions/modifications we actually used: fast\_wl, label compression, ...}
\todo{Mention possibility to create $\phi(G)$ feature map directly (not possible with label-compression)}

\subsubsection{Kernel Methods and Kernel trick}

\labelsubsection{Graph Kernel Based Text Classification}{subsec:graph_kernel_based_text_classification}
\todo{How did the field evolve?}
\todo{Hint related tasks}
\todo{What is the most related work, how do they differ from our approach
Relate the other work to the new approach and work out their differences/results}
\todo{Other approaches to graph classification}
\todo{Mention other graphs}

\paragraph{``Shortest-Path Graph Kernels for Document Similarity" \cite{Nikolentzos2017a}}
The most similar and recent approach to our approach that we found was \cite{Nikolentzos2017a}. In this work, the authors first create co-occurrence graphs out of the text. Next, they create the gram matrix for the graphs with their own graph kernel and use a SVM to classify them.

Their graph kernel consists of these sub-kernels:
\begin{enumerate}
\item{\textit{Simple label matching}: the number of matching node labels are compared between the two graphs}
\item{\textit{Shortest path}: generate a shortest-path graph}
\item{\textit{Normalization}: the gram matrix rows are normalized by \inlinetodo{?}}
\end{enumerate}

\paragraph{``Text Categorization as a Graph Classification Problem" \cite{Rousseau2015a}}
In this paper, the authors generate co-occurrence graphs out of different text classification corpora.
To classify the graphs they introduce their own graph kernel which is also an instance of the aforementioned kernels that count the number occurrences of sub-structures.
Here they first the k-core (or main core), then do most-frequent subgraph mining on the graphs.

This work is the first real evaluation of graph-based classification with a high number of graphs/unique nodes/...

\todo{Only using the most-important subgraph of a graph (main core) does not greatly damage prediction accuracy, but substantially reduces compute time}

\paragraph{``Concept Graph Preserving Semantic Relationship for Biomedical Text Categorization" \cite{Gulrandhe2015}}
Graphs: extracted keywords/concepts (POS tagger, only nouns), mapping of concepts to UMLS database, extraction of relationship between them.

Nodes are get 4 different weight components: tf, idf, connectivity, #connected components.

Kernel: number of shared edges

No results

\paragraph{``Deep Graph Kernels" \cite{Yanardag2015}}
\todo{R-Convolution?}
\todo{Embeddings for graphs}