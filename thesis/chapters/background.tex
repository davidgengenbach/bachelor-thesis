\todo[inline]{Lay groundwork for the later description of the problem}
\todo[inline]{Introductions to used classifiers, concepts}
\todo[inline]{Concretize the problem and its notation}

\labelsubsection{Classification Task}{subsec:classification_task}
\todo[inline]{Supervised vs. unsupervised}

\labelsubsection{Texts}{subsec:texts}
\todo[inline]{Mention approaches to represent text}

\labelsubsection{Graphs}{subsec:graphs}
A (labeled) graph is a triple $G = (V, E, \pi)$, where $V$ is the set of vertices or nodes, $E \subseteq V \times V$ are the edges and $\pi: V \to X$ is a labeling function which assigns a label to a given vertex.

A graph is called undirected when $\forall (v_1, v_2) \in E: (v_2, v_1) \in E$, otherwise it is called directed.

A graph $G'=(V', E', \pi)$ is called a subgraph of the graph $G = (V, E, \pi)$ when $V' \subseteq V$, $E' \subseteq E$ and $\forall (v, v') \in V': v \in V' \land v' \in V'$.

The neighbours $n(v)$ of a node $v$ are defined as the set $n(v) = \{v' | (v, v') \in E \}$.

\paragraph{Walks and Paths}
A walk on a graph is a (finite) sequence of edges, $w = (e_1, \cdots, e_n)$ with the constraint that $\forall 0 < i < n: \forall e_i = (v_1, v_2) \in w: \exists v_3: e_{i + 1} = (v_2, v_3)$. The length of the walk is $n$.
The set of all possible walks is denoted $W_G$.
A random walk starting from vertex $v$ is a walk where the first element of walk $w$ is $e = (v, v')$ for some $v'$.

A path is the sequence of the vertices visited on a walk, ie. for the walk $w = ((v_1, v_2), (v_2, v_3), \dots, (v_{n-1}, v_n)$, the path is $(v_1, v_2, v_3, \dots, v_n)$.
The set of all possible paths is denoted $P_G$.

The distance $d_{istance}(v, v')$ between two vertices is defined as the length of the shortest path between them.

\paragraph{Connected components}
A connected component $c$ of a graph is a set of vertices with the constraint $\forall v, v' \in c: \exists p \in P_G: v \in p \land v' \in p$, ie. there exists a path between every vertex in the connected component.
The connected component $c_v$ of a vertex $v$ is the connected component $c$ of a graph where $v \in c$.
The set of all connected components of a graph is $c_{all}(G) = \{ c_v | v \in V \}$. A graph is called connected when there is only one connected component. Only the empty graph has zero connected components.

\paragraph{Degree}
The in-degree $d_{in}$ of a vertex $v \in V$ is $d_{in}(v) = |\{v | (v_1, v_2) \in E \land v_2 = v\}|$.
The out-degree $d_{out}$ is defined analogously as $d_{out}(v) = |\{v | (v_1, v_2) \in E \land v_1 = v\}|$.
The degree of a vertex $v$ is $d(v) = d_{in}(v) + d_{out}(v)$ for directed graphs, and $d(v) = degree_{in}$ for undirected graphs.

\begin{figure}[ht]
\centering
\begin{tabular}{ll}
symbol &  meaning \\
\midrule
G & Graph \\
$E$ & Set of edges \\
$V$ & Set of vertices \\
$\pi(v)$ & Vertex labeling function \\
$n(v)$ & Neighbours of vertex $v$ \\
$W$ & All possible graph walks on graph $G$ \\
$w$ & Graph walk \\
$P$ & All possible paths on graph $G$ \\
$p$ & Path \\
$d(v)$ & Degree of vertex $v$ \\
$d_{in}(v)$ & In-degree of vertex $v$ \\
$d_{out}(v)$ & Out-degree of vertex $v$ \\
$d_{istance}(v, v')$ & Shortest path length between $v$ and $v'$ \\
$c_{all}(G)$ & Set of all connected components of $G$ \\
$c(v)$ & Connected component of $v$ \\
\end{tabular}
\end{figure}

\subsubsection{Concept Map}
A concept map is a (directed) graph where the nodes are concepts.
Concepts are tokens in a text.
Concepts consist of one or more words.
A directed edge between two concepts show that these concepts are related to each other. The edges can have labels, too.
For an example of a concept map, see Figure \ref{fig:concept_map}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\linewidth]{assets/figures/concept_map.pdf}
\caption{Example of a concept map. The nodes are the concepts. The directed edges connect the concepts to each other and have labels. In this example of a concept map, one could recover sentences from this graph, for example ``Caffeine \textit{is} a mild CNS stimulant". Taken and adapted from: \cite{Falke2017}}
\label{fig:concept_map}
\end{figure}

Concept maps are useful for visualizing concepts and their relation to each other.
Concept maps can be used to quickly explore a given topic and immediately see connections between concepts.
Through the degree of a concept, one can also infer the relative importance of that concept in the underlying text.

By combining different texts of the same topic, one can also create a concept map for that topic.
The concepts are not confined to a single text in this case.

The visualization with concept maps of one or more individual texts therefor enables the non-linear exploration of multiple texts of a topic.

Because the concepts in a concept map often contain only a small subset of the text, concept maps also summarize the underlying text.
Only relevant concepts and their relation to each other are captured, making concept maps interesting for (multi-document) text summarization.

In Chapter \ref{sec:implementation} we introduce the concept map extraction implementation we used and further explain the steps in creating the concept maps.

\todo[inline]{"Capture important concepts and the structure of texts (spanning over whole text)"}

\subsubsection{Co-Occurrence Graph}
A co-occurrence graph, or graph-of-words, is generated from a text by creating a (undirected) graph with the words of the underlying text as nodes.
There is an edge between two nodes if their words co-occur in the text.
Two words co-occur when the distance between the words in the text is below a given threshold, the window size.

Figure \ref{fig:cooccurrence_graphs} shows examples of co-occurrence graphs with different windows sizes.

\begin{figure}[ht]%
    \centering
    \subfloat[Window size: 1]{{\includegraphics[width=0.3\textwidth]{assets/figures/cooccurrence_graph_1.pdf} }}%
    \subfloat[Window size: 2]{{\includegraphics[width=0.3\textwidth]{assets/figures/cooccurrence_graph_2.pdf} }}%
    \subfloat[Window size: 3]{{\includegraphics[width=0.3\textwidth]{assets/figures/cooccurrence_graph_3.pdf} }}%
    \caption{Example for co-occurrence graphs. Generated from the sentence: ``Quotation is a serviceable substitute for wit". With increasing window size the number of edges and therefor the connectedness of the co-occurrence graph also increases.}%
    \label{fig:cooccurrence_graphs}%
\end{figure}

Since co-occurrence graphs not only contain the words of the underlying text but also their co-occurrence, they capture not only the content of the text but also the relation between words of the text.
Therefor they contain structural information about the text as well.

\todo[inline]{Co-occurrence graphs can have edge weights corresponding to the number of occurrences of a co-occurrence of words. One can derive the number of occurrences of the label by summing up the edge weights of edges of a node}

\labelsubsection{Graph Kernel}{subsec:graph_kernel}
In most cases, texts and graphs are not of fixed size.
However, most classification algorithms can only operate on fixed size vectors.
Fortunately, there are several approaches to overcome this problem.
One of these approaches are so-called kernels.

A kernel is a function $k$ which returns a measure of similarity between two objects:
\begin{equation*}
k: X \times X \rightarrow \mathbb{Q}
\end{equation*}

A kernel function has to be:
\begin{itemize}
    \item{\textit{symmetric}: $k(x, x') = k(x', x)$}
    \item{\textit{non-negative definite}: \dots}
\end{itemize}

An interesting property of kernels is that they can be combined easily with symmetric operations, eg. by adding or multiplicating the results of two kernel functions, $k_1(x, x')$ and $k_2(x, x')$:
\begin{equation*}
k_{combined}(x, x') = k_1(x, x') * k_2(x, x')
\end{equation*}
One can easily confirm that when $k_1$ and $k_2$ are valid kernels, $k_{combined}$ is also a valid kernel.
This property enables composite kernels to capture multiple means of similarity in one metric.

For example, a simple kernel on two numbers (or vectors) is $k(x, x') = | x * x' |$, where $| \cdot |$ is the absolute value (or a norm).
Another example of a kernel between strings is $k(s, s') = \delta(s = s')$, where $\delta(\cdot)$ is the Kronecker delta. This kernel returns 1 if the two given strings are the same, and 0 if they are different.
Note that both these kernels are indeed valid.

Kernel functions can be used in kernel methods for classification and other tasks.
Prominent examples of kernel methods are the SVM, kernelized PCA and Gaussian Processes.

For a subset of kernels, one can also create explicit feature vectors for a given object. In this case, the kernel can be written as an inner product on two fixed-size vectors:
\begin{equation*}
    k(X, X') = \langle \phi(X), \phi(X') \rangle
\end{equation*}
Here, $\phi(X)$ returns a fixed-size vector representation of the object $X$ and is the main part of the kernel.
This $\phi(X)$ can then be used in all vector-based classification algorithms, not only kernel methods.
An example for such a kernel is the string kernel which counts the number of occurrences of words in a given text, then creates a vector representation by giving each word an index in the vector and setting this index to the number of occurrences of that word. The inner product of two of these vector representations then create a kernel.

A graph kernel is simply a kernel that operates on graphs.

Graph kernels that measure the similarity of two graphs are related to the isomorphism test between two graphs.
The graph isomorphism test returns whether two graphs are structurally the same.
In this test, the labels of the vertices and edges are ignored. Otherwise the task would be far easier since one could simply compare the labels and edges of the graphs - when they are both exactly the same, the graphs are the same.
Instead, in the graph isomorphism test one has to find two mappings: one bijective mapping $\psi_{V}: V_1 \mapsto V_2$ from the vertices of the first graph to the vertices of the second graph and another bijective mapping $\psi_{E}: E_1 \mapsto E_2$ with the constraint:
\begin{equation*}
    \forall e = (v_s, v_t) \in E_1:
    \exists e' = (v'_s, v'_t) \in E_2:
    \psi_{E}(e) = e'
    \Rightarrow
    \psi_{V}(v_s) = v'_s
    \land
    \psi_{V}(v_t) = v'_t
\end{equation*}


\todo[inline]{Explain constraints for the mapping}
If such a mapping exists, the two graphs are called isomorphic.
While the complexity of the graph isomorphism test is not known yet, it is suspected to lie in P or maybe is also NP-complete.

\todo[inline]{This potentially super-polynomial complexity has grave consequences for a graph kernel.}
\todo[inline]{Namely?}

\todo[inline]{Creating a graph kernel is always a trade-off}
\todo[inline]{Kernels can be combined!}
\todo[inline]{Explain when $\phi$ can be constructed (Mercer's theorem: ``an implicitly defined function $\varphi$  exists whenever the space $\mathcal{X}$ can be equipped with a suitable measure ensuring the function $k$ satisfies Mercer's condition.")}
\todo[inline]{Mention that functions that do not satisfy Mercer's condition and are not non-negative definite can nevertheless perform quite good in real world applications}

\paragraph{Gram matrix}
A gram, or kernel matrix, for a given kernel is a matrix $A$ for a set of objects, $D = (x_1, x_2, \ldots, x_n)$
The entries of the gram matrix are the pairwise similarities between all objects in the dataset:
\begin{equation*}
    A_{i,j} = k(x_i, x_j)
\end{equation*}
When the $\phi$ for each $x$ is given, one can also construct the gram matrix by stacking these vectors into a matrix $M_{\phi}$ and calculating the product on the transpose
\begin{equation*}
    A = M_{\phi} M_{\phi}^T
\end{equation*}
The gram matrix is symmetric by definition since the underlying kernel is also symmetric, ie. $A_{i,j} = A_{j, i}$.

\paragraph{Graph Kernel Categories}
Several graph kernels have been proposed in the literature.
Categories of graph kernels include kernels based on shortest-paths, random-walks and subtree patterns, to name just a few.
A great number of graph kernels have in common that they first create a decomposition of the graph into sub-structures, for example into subtrees, and then count the occurrences of these sub-structures in the graphs.
The kernels in this category are sometimes called R-Convolution graph kernels.
In the next section, we introduce some examples of graph kernels to get a feeling for the possibilities and trade-offs that different approaches have.
As mentioned before, any set of valid kernels can be combined easily to form a composite kernel.
So, any of the algorithms we will introduce in the next section can be combined and extended using other kernels.
As we will also see in this next section, graph kernels can be quite diverse and capture different information of the objects they are operating on.
The scope of a graph kernel can therefor also vary widely from only capturing narrow aspects of the graph to actually capturing the complete graph structure and content.

\todo[inline]{Mention linearization of graphs?}

\subsubsection{Examples}
\paragraph{Simple Equality Graph Kernel}
This graph kernel simply compares the nodes and edges of the two graphs and returns whether they are exactly the same:
\begin{equation*}
    k(G_1, G_2) = \delta(V_{G_1} = V_{G_2}) * \delta(E_{G_1} = E_{G_2})
\end{equation*}
This kernel takes both structure and content of the graphs into account. Note that there is no partial similarity. The co-domain of $k$ is $\{0, 1\}$, returning 1 when the graphs are exactly the same and 0 otherwise.
While this is an exact, simple and fast algorithm, in most cases it is not of great use since it depends on exact matches.

\paragraph{Simple Non-Structural Graph Kernel}
Following is an example of a simple kernel which actually discards all structural information about the graph. This graph kernel only operates on the labels of the vertices:
\begin{equation*}
k_{common\_label}(G_1, G_2) = | l(G_1) \cap l(G_2) |
\end{equation*}
where $l(G)$ returns the set of labels for graph $G$.
Therefor, this graph kernel counts the number of common labels of the two graphs. Under this kernel, two graphs are the same when they have the same node labels, completely ignoring the edges.

An extension to this graph kernel is using the Jaccard coefficient instead of simply counting the common node labels:
\begin{equation*}
k_{common\_label\_jaccard}(G_1, G_2) = \frac{| l(G_1) \cap l(G_2) |}{| l(G_1) \cup l(G_2) |}
\end{equation*}
This extension normalizes the function and confines the codomain of $k$ to $[0, 1]$.

\paragraph{Simple Structure-Only Graph Kernel}
An example for a graph kernel which only operates on the structure of the graph, is the following:
\begin{equation*}
k_{simple\_structure}(G_1, G_2) = \langle \phi(G_1), \phi(G_1) \rangle
\end{equation*}
with $\phi(G) = (n_{triangle}(G), n_{rectangle}(G))^T$. Here, $n_{triangle}$ is the number of the triangle subgraphs in $G$ and $n_{rectangle}$ the number of rectangle subgraphs in $G$.
This graph kernel is an example of a kernel where the explicit feature map $\phi(G)$ can be computed directly and completely independent of other graphs.

\todo[inline]{This kernel actually is quite similar to a real-world kernel named graphlet kernel. In the graphlet kernel, the sub-structures are not triangles and rectangles, but graphlets.}

\paragraph{Random Walk Graph Kernel}
This kernel does random walks on both graphs and returns the number of matching random walks between the two graphs:
\begin{equation*}
    k(G_1, G_2) = |r(G_1, n, l) \cap r(G_2, n, l)|
\end{equation*}
where $r(G, n, l)$ is return a set of $n$ random walks of length $l$ on graph $G$.
This graph kernel takes both structure and content into account.

\paragraph{Weisfeiler-Lehman Graph Kernel}
The Weisfeiler-Lehman graph kernel is also a kernel that works by counting common sub-structures on two graphs.
Here, the sub-structures are subtrees starting from each vertex.
In each iteration $i$ with $0 < i < h$ for a given number of iterations $h$ , or until convergence, the WL graph kernel relabels the vertices of the two given graphs.
This relabeling is also called recoloring or color refinement.

The WL graph kernel relabels a vertex $v$ by concatenating two components:
\begin{enumerate}
    \item{the current label of the vertex: $\pi_{i-1}(v)$}
    \item{the sorted labels of the neighbourhood: $\{ \pi_{i-1}(v') | v' \in n(v) \}$}
\end{enumerate}
where $\pi_{i}$ is the labeling function for iteration $i$ with $\pi_0 = \pi$.
The resulting new label of $v$ is the sequence $(\pi_{i-1}(v), \pi_{i-1}(v_{neighbour,1}), \dots , \pi_{i-1}(v_{neighbour,n}))$.
After the iteration, a new labeling function is created accordingly: $\pi_i(v)$ now returns to the new label of $v$.
So, for each iteration, a node gets the same label as another node if they have the same label and neighbourhood.
For the first iteration, the labels of a node consist of the immediate neighbourhood.
In the next iteration, the label of a node than encodes the neighbourhood of the neighbourhood, ie. 

The Weisfeiler-Lehman graph kernel has been to shown to achieve state-of-the-art performance on several datasets and applications.

\todo[inline]{Complexity? $\mathcal{O}(hm)$, where $h$ is the number of iterations and $m$ is the number of edges in the dataset.}

It has to be noted that the Weisfeiler-Lehman does not use edge labels by default, but can be extended to do so.
There are also several other extensions to the Weisfeiler-Lehman kernel in the literature.
One example of such an extension reduces the space requirement of the algorithm by ``compressing" the labels after each iterations, ie. by giving each composite label a unique and shorter label.

In Figure \ref{fig:wl_example} we show an example of one WL iteration with label compression.

\todo[inline]{Explain and mention ``fast_wl"}
\todo[inline]{How?}
\todo[inline]{Mention complexity}
\todo[inline]{Explain extensions/modifications we actually used: fast\_wl, label compression, ...}
\todo[inline]{Mention possibility to create $\phi(G)$ feature map directly (not possible with label-compression)}
\todo[inline]{Mention that WL can be used as a graph isomorphism test - but it fails on certain types of graphs, ie. it reports a isomorphism between graphs that are not the same}

\begin{figure}[ht]
  \subfloat[Initial graphs]{\includegraphics[width=0.32\textwidth]{assets/figures/wl_example/wl_iteration_0.pdf}\label{fig:wl_example_0}}
  \hfill
  \subfloat[After relabeling]{\includegraphics[width=0.32\textwidth]{assets/figures/wl_example/wl_iteration_1_stage_0_recolored}\label{fig:wl_example_1}}
  \hfill
  \subfloat[After label compression]{\includegraphics[width=0.32\textwidth]{assets/figures/wl_example/wl_iteration_1_stage_1_compressed.pdf}\label{fig:wl_example_2}}
  \caption{Weisfeiler Lehman algorithm example. This examples shows one WL iteration with two graphs. The initial graphs are shown in \textbf{(a)}. The first step of the iteration is relabeling the graph vertices by concatenating each node label with its sorted neighbourhood labels. The finished relabeled graph is shown in \textbf{(b)}. As an extension to the plain Weisfeiler-Lehman algorithm, we also show the label compression step where the long labels, consisting of the node label and its neighbourhood labels, get compressed into a smaller label by assigning each unique label a new number. Same labels in different graphs get the same new label. The result of this step is shown in \textbf{(c)}. Under \textbf{(a)} and \textbf{(c)} we also added the feature maps $\phi{}$ for the graphs. Each entry in the vector corresponds to a label. If a vector entry is 1, it means that the corresponding label is present in the given graph. The inner product on the vectors of the graphs is the value of the Weisfeiler-Lehman graph kernel for these two graphs.
  In this case, the similarity before the iteration would be $\langle \phi(G_1), \phi(G_2) \rangle = \langle [1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0] \rangle = 4$. After the relabeling, the value of the inner product is $2$.
  Before doing the WL iteration, the inner product on the feature maps simply returns the number of common labels. After the first iteration, the neighbourhood of each node and therefor the structure of the graph is also reflected in the feature map - resulting in a lower similarity in out example.
  Please note that these feature vectors are specific to these two graphs and their labels. The dimension of the vector corresponds to the sum of the number of vertices in the two graphs.}\label{fig:wl_example}
\end{figure*}

\subsubsection{Kernel Methods and Kernel trick}
We have seen that a kernel can be used to calculate a measure of similarity between two objects, eg. graphs.
Some kernels can be expressed as an inner product $k(X, X') = \langle \phi(X), \phi(X') \rangle$ on two (implicit) vector representations of the objects $X$ and $X'$.
In this case, the vector representations $\phi(X)$ can be used in conventional classification algorithms.
Yet, calculating these representations is not always possible or feasible since the dimension of such a representation is quite high and most often very sparse.
There is another approach for classification (and regression), the aforementioned kernel methods.
Kernel methods operate directly with the kernel values and do not use the implicit feature representations $\phi(X)$.
Instead, kernel methods use the similarities $k(X, X')$ directly.
Therefor kernel methods can be used with kernels where no implicit feature representation $\phi(X)$ can be calculated or where calculating $\phi(X)$ is not feasible.

One popular example of a kernel method is the Support Vector Machine, or SVM.
The SVM is actually able to use either vector representations of the elements in the dataset or the pairwise similarities, the gram matrix, between them.
At training time, the gram matrix $A$ of all objects in the training set, $d_{train} = \{X_1, X_2, \ldots, X_n\}$, is calculated.
This gram matrix $A$ and the corresponding labels then get fed into the SVM which optimizes its parameters.
\todo[inline]{Prediction time?}


\labelsubsection{Graph Kernel Based Text Classification}{subsec:graph_kernel_based_text_classification}
In this section, we will introduce related work and briefly recap the history of the field.

\todo[inline]{How did the field evolve?}
\todo[inline]{Hint related tasks}
\todo[inline]{What is the most related work, how do they differ from our approach}
\todo[inline]{Relate the other work to the new approach and work out their differences/results}
\todo[inline]{Other approaches to graph classification}
\todo[inline]{Mention other graphs}

\paragraph{``Shortest-Path Graph Kernels for Document Similarity" \cite{Nikolentzos2017a}}
The most similar and recent work to our approach that we found was \cite{Nikolentzos2017a}. In this work, the authors first create co-occurrence graphs out of the text. Next, they create the gram matrix for the graphs with their own graph kernel and use a SVM to classify them.

Their graph kernel is a combination of two sub-kernels:
\begin{enumerate}
    \item{\textit{Simple label matching}: the number of matching node labels are compared between the two graphs}
    \item{\textit{Shortest path}: generate a shortest-path graph, then compare the edges}
\end{enumerate}
Note that the simple label matching sub-kernel does not take the structure into account, while the second sub-kernel actually uses both.
Both sub-kernels return a number which is added to produce the final value of the kernel.

The shortest path sub-kernel works as follows:
\begin{enumerate}
    \item{Generate shortest-path graphs for the two given graphs: The shortest path graph of a given graph has the same nodes.
    The edges are added by calculating the shortest paths between all pairs of nodes in the original graph. An edge is added between two nodes in the shortest path graph if the shortest path length between these nodes in the original graph is below a given parameter $d$. The edge also gets a weight assigned, which is $\frac{1}{d}$}
    \item{Count the number of same edges in the both graphs: when two edges are the same, add the product of their edge weights to the similarity}
\end{enumerate}

\todo[inline]{Mention that they did NOT perform experiments to differentiate the performance of the subkernels! My guess is that most of the performance comes from the simple-set matching subkernel.}
\todo[inline]{Note that the shortest-path sub-kernel does not actually add a lot of performance?}
\todo[inline]{Why is this approach different to ours? Co-occurrence vs. concept map}

\paragraph{``Text Categorization as a Graph Classification Problem" \cite{Rousseau2015a}}
In this paper, the authors generate co-occurrence graphs out of different text classification datasets.
To classify the graphs, they introduce their own graph kernel which is also an instance of the aforementioned kernels that count the number of occurrences of sub-structures.
Here, they first calculate the k-core (or main core) of the graphs, then do most-frequent subgraph mining on the graphs.

This work is the first real evaluation of graph-based classification with a high number of graphs/unique nodes/...

\todo[inline]{Only using the most-important subgraph of a graph (main core) does not greatly damage prediction accuracy, but substantially reduces compute time}
\todo[inline]{How does this approach differ from ours? In this case, the authors use graphs with duplicate labels. The subgraph mining is not that relevant and useful in our case. This is because in our cases, the graphs are quite small.}

\paragraph{``Concept Graph Preserving Semantic Relationship for Biomedical Text Categorization" \cite{Gulrandhe2015}}
Graphs: extracted keywords/concepts (POS tagger, only nouns), mapping of concepts to UMLS database, extraction of relationship between them.

Nodes are get 4 different weight components: tf, idf, connectivity, #connected components.

Kernel: number of shared edges

No results

\paragraph{``Deep Graph Kernels" \cite{Yanardag2015}}
\todo[inline]{Embeddings for graphs}