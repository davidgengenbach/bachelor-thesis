Automatically generated by Mendeley Desktop 1.17.12
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Rousseau2015a,
abstract = {In this paper, we consider the task of text categorization as a graph classification problem. By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part - its main core - speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classification datasets show statistically significant higher accuracy and macro-Averaged F1-score compared to baseline approaches.},
author = {Rousseau, Francois and Kiagias, Emmanouil and Vazirgiannis, Michalis},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Text Categorization as a Graph Classification Problem - Rousseau, Kiagias, Vazirgiannis - 2015(2).pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
pages = {1702--1712},
title = {{Text Categorization as a Graph Classification Problem}},
url = {http://www.aclweb.org/anthology/P15-1164},
year = {2015}
}
@article{Giscard2017,
abstract = {With the recent rise in the amount of structured data available, there has been considerable interest in methods for machine learning with graphs. Many of these approaches have been kernel methods, which focus on measuring the similarity between graphs. These generally involving measuring the similarity of structural elements such as walks or paths. Borgwardt and Kriegel proposed the all-paths kernel but emphasized that it is NP-hard to compute and infeasible in practice, favouring instead the shortest-path kernel. In this paper, we introduce a new algorithm for computing the all-paths kernel which is very efficient and enrich it further by including the simple cycles as well. We demonstrate how it is feasible even on large datasets to compute all the paths and simple cycles up to a moderate length. We show how to count labelled paths/simple cycles between vertices of a graph and evaluate a labelled path and simple cycles kernel. Extensive evaluations on a variety of graph datasets demonstrate that the all-paths and cycles kernel has superior performance to the shortest-path kernel and state-of-the-art performance overall.},
archivePrefix = {arXiv},
arxivId = {1708.01410},
author = {Giscard, P. -L. and Wilson, R. C.},
eprint = {1708.01410},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/The All-Paths and Cycles Graph Kernel - Giscard, Wilson - 2017.pdf:pdf},
pages = {1--8},
title = {{The All-Paths and Cycles Graph Kernel}},
url = {http://arxiv.org/abs/1708.01410},
year = {2017}
}
@article{Neuhaus2009,
abstract = {One of the major difficulties in graph classification is the lack of mathematical structure in the space of graphs. The use of kernel machines allows us to overcome this fundamental limitation in an elegant manner by addressing the pattern recognition problem in an implicitly existing feature vector space instead of the original space of graphs. In this paper we propose three novel error-tolerant graph kernels -- a diffusion kernel, a convolution kernel, and a random walk kernel. The kernels are closely related to one of the most flexible graph matching methods, graph edit distance. Consequently, our kernels are applicable to virtually any kind of graph. They also show a high degree of robustness against various types of distortion. In an experimental evaluation involving the classification of line drawings, images, diatoms, fingerprints, and molecules, we demonstrate the superior performance of the proposed kernels in conjunction with support vector machines over a standard nearest-neighbor reference method and several other graph kernels including a standard random walk kernel.},
author = {Neuhaus, Michel and Riesen, Kaspar and Bunke, Horst},
doi = {10.1163/156856809789476119},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Novel kernels for error-tolerant graph classification. - Neuhaus, Riesen, Bunke - 2009.pdf:pdf},
isbn = {0169-1015},
issn = {0169-1015},
journal = {Spatial vision},
keywords = {error-tolerant graph matching,graph classification,graph edit distance,graph kernel},
number = {5},
pages = {425--441},
pmid = {19814905},
title = {{Novel kernels for error-tolerant graph classification.}},
volume = {22},
year = {2009}
}
@article{Lei2017,
abstract = {The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.},
archivePrefix = {arXiv},
arxivId = {1705.09037},
author = {Lei, Tao and Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
eprint = {1705.09037},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Deriving Neural Architectures from Sequence and Graph Kernels - Lei et al. - 2017.pdf:pdf},
title = {{Deriving Neural Architectures from Sequence and Graph Kernels}},
url = {http://arxiv.org/abs/1705.09037},
year = {2017}
}
@article{Kashima2002,
abstract = {In this paper, we apply kernel methods to graph classification problems. To achieve the goal, we have to design an appropriate kernel for computing inner products for pairs of graphs represented in a feature space. We define a graph kernel by a random walk on a vertex product graph of two graphs. Some experiments on predicting properties of chemical compounds show encouraging results.},
author = {Kashima, Hisashi and Inokuchi, Akihiro},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Kernels for graph classification - Kashima, Inokuchi - 2002.pdf:pdf},
journal = {ICDM Workshop on Active Mining},
pages = {25},
title = {{Kernels for graph classification}},
url = {http://www.geocities.jp/kashi{\_}pong/publication/am02.pdf},
volume = {2002},
year = {2002}
}
@article{Kang2012,
abstract = {Random walk graph kernel has been used as an important tool for various data mining tasks including classi fication and similarity computation. Despite its usefulness, however, it suffers from the expensive computational cost which is at least O(n3) or O(m2) for graphs with n nodes and m edges. In this paper, we propose Ark, a set of fast algorithms for random walk graph kernel computation. Ark is based on the observation that real graphs have much lower intrinsic ranks, compared with the orders of the graphs. Ark exploits the low rank structure to quickly compute random walk graph kernels in O(n 2) or O(m) time. Experimental results show that our method is up to 97,865× faster than the existing algorithms, while providing more than 91.3{\%} of the accuracies.},
author = {Kang, U and Tong, Hanghang and Sun, Jimeng},
doi = {10.1137/1.9781611972825.71},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Fast Random Walk Graph Kernel - Kang, Tong, Sun - 2012.pdf:pdf},
isbn = {9781611972320},
journal = {Proceedings of the 2012 SIAM International Conference on Data Mining},
pages = {828--838},
title = {{Fast Random Walk Graph Kernel}},
url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972825.71},
year = {2012}
}
@article{Kalchbrenner2014,
abstract = {The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25{\%} error reduction in the last task with respect to the strongest baseline.},
archivePrefix = {arXiv},
arxivId = {1404.2188v1},
author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
doi = {10.3115/v1/P14-1062},
eprint = {1404.2188v1},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A Convolutional Neural Network for Modelling Sentences - Kalchbrenner, Grefenstette, Blunsom - 2014.pdf:pdf},
isbn = {9781937284725},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages = {655--665},
title = {{A Convolutional Neural Network for Modelling Sentences}},
url = {http://aclweb.org/anthology/P14-1062},
year = {2014}
}
@book{Bishop2006,
author = {Bishop, Christopher M.},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Pattern recognition and machine learning - Bishop - 2006.pdf:pdf},
isbn = {0120588307},
pmid = {4394542},
publisher = {Springer},
title = {{Pattern recognition and machine learning}},
year = {2006}
}
@article{Lang,
author = {Lang, Ken},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/NewsWeeder Learning to Filter Netnews - Lang - Unknown.pdf:pdf},
keywords = {-matching profiles to be,but,consequently,designed by the user,first efforts have been,made to have the,or able to build,profiles be,sufficiently complex ones,the average netnews reader,will probably not be,willing},
title = {{NewsWeeder : Learning to Filter Netnews}}
}
@article{Sebastiani2001,
abstract = {The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.},
archivePrefix = {arXiv},
arxivId = {cs/0110053},
author = {Sebastiani, Fabrizio},
doi = {10.1145/505282.505283},
eprint = {0110053},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Machine Learning in Automated Text Categorization - Sebastiani - 2001.pdf:pdf},
isbn = {0360-0300},
issn = {03600300},
pmid = {18202440},
primaryClass = {cs},
title = {{Machine Learning in Automated Text Categorization}},
url = {http://arxiv.org/abs/cs/0110053},
year = {2001}
}
@article{Lodhi2002,
abstract = {We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets},
author = {Lodhi, Huma and Saunders, Craig and Shawe-Taylor, John and Cristianini, Nello and Watkins, Chris},
doi = {10.1162/153244302760200687},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Text Classification using String Kernels - Lodhi et al. - 2002.pdf:pdf},
isbn = {1532-4435},
issn = {0003-6951},
journal = {Journal of Machine Learning Research},
keywords = {String Kernels,text classification},
pages = {419--444},
title = {{Text Classification using String Kernels}},
url = {http://discovery.ucl.ac.uk/13443/{\%}5Cnhttp://www.crossref.org/deleted{\_}DOI.html},
volume = {2},
year = {2002}
}
@article{Eiben2017,
abstract = {Given a graph {\$}G{\$} and {\$}k\backslashin{\{}\backslashmathbb N{\}}{\$}, the Dominating Set problem asks for a subset {\$}D{\$} of {\$}k{\$} vertices such that every vertex in {\$}G{\$} is either in {\$}D{\$} or has a neighbor in {\$}D{\$}. It is well known that Dominating Set is {\$}{\{}\backslashsf W{\}}[2]{\$}-hard when parameterized by {\$}k{\$}. But it admits a linear kernel on graphs of bounded expansion and a polynomial kernel on {\$}K{\_}{\{}d,d{\}}{\$}-free graphs, for a fixed constant {\$}d{\$}. In contrast, the closely related Connected Dominating Set problem (where {\$}G[D]{\$} is required to be connected) is known not to admit such kernels unless {\$}\backslashtextsf{\{}NP{\}} \backslashsubseteq \backslashtextsf{\{}coNP/poly{\}}{\$}. We show that even though the kernelization complexity of Dominating Set and Connected Dominating Set diverges on sparse graphs this divergence is not as extreme as kernelization lower bounds suggest. To do so, we study the Connected Dominating Set problem under the recently introduced framework of lossy kernelization. In this framework, for {\$}\backslashalpha{\textgreater}1{\$}, an {\$}\backslashalpha{\$}-approximate bikernel (kernel) is a polynomial-time algorithm that takes as input an instance {\$}(I,k){\$} and outputs an instance {\$}(I',k'){\$} of a problem (the same problem) such that, for every {\$}c {\textgreater} 1{\$}, a {\$}c{\$}-approximate solution for the new instance can be turned into a {\$}c\backslashalpha{\$}-approximate solution of the original instance in polynomial time. Moreover, the size of {\$}(I',k'){\$} is bounded by a function of {\$}k{\$} and {\$}\backslashalpha{\$}. We show that Connected Dominating Set admits an {\$}\backslashalpha{\$}-approximate bikernel on graphs of bounded expansion and an {\$}\backslashalpha{\$}-approximate kernel on {\$}K{\_}{\{}d,d{\}}{\$}-free graphs, for every {\$}\backslashalpha{\textgreater}1{\$}. For {\$}K{\_}{\{}d,d{\}}{\$}-free graphs we obtain instances of size {\$}k{\^{}}{\{}\backslashmathcal{\{}O{\}}(d{\^{}}2 / (\backslashalpha-1)){\}}{\$} while for bounded expansion graphs we obtain instances of size {\$}\backslashmathcal{\{}O{\}}(f(\backslashalpha) k){\$} (i.e, linear in {\$}k{\$}), where {\$}f(\backslashalpha){\$} is a computable function depending only on {\$}\backslashalpha{\$}.},
archivePrefix = {arXiv},
arxivId = {1706.09339},
author = {Eiben, Eduard and Kumar, Mithilesh and Mouawad, Amer E. and Panolan, Fahad},
eprint = {1706.09339},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Lossy Kernels for Connected Dominating Set on Sparse Graphs - Eiben et al. - 2017.pdf:pdf},
keywords = {and phrases kernelization,connected dominating set,lossy kernels,sparse graph},
pages = {1--20},
title = {{Lossy Kernels for Connected Dominating Set on Sparse Graphs}},
url = {http://arxiv.org/abs/1706.09339},
year = {2017}
}
@article{Reimers2017,
abstract = {In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches. We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant (p {\textless} 10{\^{}}-4) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F1-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1707.09861},
author = {Reimers, Nils and Gurevych, Iryna},
eprint = {1707.09861},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Reporting Score Distributions Makes a Difference Performance Study of LSTM-networks for Sequence Tagging - Reimers, Gurevych - 2017.pdf:pdf},
title = {{Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging}},
url = {http://arxiv.org/abs/1707.09861},
year = {2017}
}
@article{Morris2017b,
abstract = {Most state-of-the-art graph kernels only take local graph properties into account, i.e., the kernel is computed with regard to properties of the neighborhood of vertices or other small substructures. On the other hand, kernels that do take global graph propertiesinto account may not scale well to large graph databases. Here we propose to start exploring the space between local and global graph kernels, striking the balance between both worlds. Specifically, we introduce a novel graph kernel based on the {\$}k{\$}-dimensional Weisfeiler-Lehman algorithm. Unfortunately, the {\$}k{\$}-dimensional Weisfeiler-Lehman algorithm scales exponentially in {\$}k{\$}. Consequently, we devise a stochastic version of the kernel with provable approximation guarantees using conditional Rademacher averages. On bounded-degree graphs, it can even be computed in constant time. We support our theoretical results with experiments on several graph classification benchmarks, showing that our kernels often outperform the state-of-the-art in terms of classification accuracies.},
archivePrefix = {arXiv},
arxivId = {1703.02379},
author = {Morris, Christopher and Kersting, Kristian and Mutzel, Petra},
eprint = {1703.02379},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Global Weisfeiler-Lehman Graph Kernels - Morris, Kersting, Mutzel - 2017.pdf:pdf},
title = {{Global Weisfeiler-Lehman Graph Kernels}},
url = {http://arxiv.org/abs/1703.02379},
year = {2017}
}
@article{Smola2003,
abstract = {We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators.},
author = {Smola, Aj Alexander J and Kondor, Risi},
doi = {10.1007/978-3-540-45167-9_12},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Kernels and Regularization on Graphs - Smola, Kondor - 2003.pdf:pdf},
isbn = {3540407200},
issn = {03029743},
journal = {Machine Learning},
pages = {1--15},
title = {{Kernels and Regularization on Graphs}},
url = {http://www.springerlink.com/index/H96KDX90DCMM6FX0.pdf{\%}5Cnhttp://link.springer.com/chapter/10.1007/978-3-540-45167-9{\_}12},
volume = {2777},
year = {2003}
}
@article{Gaspar2011,
abstract = {Use another document representation DRT.Random graph kernel and direct product graph.The process is:from a text to a DRS;from a DRS to a graph;obtain the direct product graph between all pairs of documents;calculate the value of the kernel function(equation 1);kernel matrix that can be given to the SVM.},
author = {Gaspar, Miguel and Gon{\c{c}}alves, T and Quaresma, Paulo},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Text classification using semantic information and graph kernels - Gaspar, Gon{\c{c}}alves, Quaresma - 2011.pdf:pdf},
journal = {EPIA-11, 15th Portuguese Conference on Artificial Intelligence},
title = {{Text classification using semantic information and graph kernels}},
url = {http://www.di.uevora.pt/{~}pq/papers/epia2011a.pdf},
year = {2011}
}
@article{Bekkerman2003,
abstract = {In the past decade a sufﬁcient effort has been expended on attempting to come up with a document representation which is richer than the simple Bag-Of-Words (BOW). One of the widely explored approaches to enrich the BOW representation is in using n-grams (usually bigrams) of words in addition to (or in place of) single words (unigrams). After more than ten years of unsuccessful attempts to improve the text categorization results by applying bigrams, many researchers agree that there might be a certain limitation in usability of bigrams for text categorization. We analyze the related works and discuss possible reasons for this limitation. In addition, we demonstrate our own attempt to incorporate bigrams in a document representation based on distributional clusters of unigrams, and report (statistically insignificant) improvement to our baseline results on the 20 Newsgroups (20NG) dataset. Nevertheless, the reported result is (to our knowledge) the best categorization result ever achieved on this highly popular dataset.},
author = {Bekkerman, R and Allan, J},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Using Bigrams in Text Categorization - Bekkerman, Allan - 2003.pdf:pdf},
journal = {Work},
keywords = {classification,uni vs bi},
pages = {1--10},
title = {{Using Bigrams in Text Categorization}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.1999{\&}rep=rep1{\&}type=pdf},
volume = {1003},
year = {2003}
}
@article{Kulharia2008,
abstract = {We present a unified framework to study graph kernels, special cases of which include the random walk graph kernel $\backslash$citep{\{}GaeFlaWro03,BorOngSchVisetal05{\}}, marginalized graph kernel $\backslash$citep{\{}KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04{\}}, and geometric kernel on graphs $\backslash$citep{\{}Gaertner02{\}}. Through extensions of linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a Sylvester equation, we construct an algorithm that improves the time complexity of kernel computation from {\$}O(n{\^{}}6){\$} to {\$}O(n{\^{}}3){\$}. When the graphs are sparse, conjugate gradient solvers or fixed-point iterations bring our algorithm into the sub-cubic domain. Experiments on graphs from bioinformatics and other application domains show that it is often more than a thousand times faster than previous approaches. We then explore connections between diffusion kernels $\backslash$citep{\{}KonLaf02{\}}, regularization on graphs $\backslash$citep{\{}SmoKon03{\}}, and graph kernels, and use these connections to propose new graph kernels. Finally, we show that rational kernels $\backslash$citep{\{}CorHafMoh02,CorHafMoh03,CorHafMoh04{\}} when specialized to graphs reduce to the random walk graph kernel.},
archivePrefix = {arXiv},
arxivId = {0807.0093},
author = {Kulharia, Viveka and Ghosh, Arnab and Karnick, Harish},
eprint = {0807.0093},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph Kernels - Kulharia, Ghosh, Karnick - 2008.pdf:pdf},
keywords = {Learning},
pages = {1201--1242},
title = {{Graph Kernels}},
url = {http://arxiv.org/abs/0807.0093},
year = {2008}
}
@article{Kriege2012,
author = {Kriege, Nils and Mutzel, Petra},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Subgraph Matching Kernels for Attributed Graphs - Kriege, Mutzel - 2012.pdf:pdf},
title = {{Subgraph Matching Kernels for Attributed Graphs}},
year = {2012}
}
@article{Falke2017b,
author = {Falke, Tobias and Gurevych, Iryna},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Concept-Map-Based Multi-Document Summarization using Concept Co-Reference Resolution and Global Importance Optimization - Falke, Gurevyc.pdf:pdf},
pages = {1--10},
title = {{Concept-Map-Based Multi-Document Summarization using Concept Co-Reference Resolution and Global Importance Optimization}},
year = {2017}
}
@inproceedings{Neuhaus2006,
abstract = {General graph matching methods often suffer from the lack of mathematical structure in the space of graphs. Using kernel functions to evaluate structural graph similarity allows us to formulate the graph matching problem in an implicitly existing vector space and to apply well-known methods for pattern analysis. In this paper we propose a novel convolution graph kernel. Our kernel function differs from other graph kernels mainly in that it is closely related to error-tolerant graph edit distance and can therefore be applied to attributed graphs of various kinds. The proposed kernel function is evaluated on two graph datasets. It turns out that our method is generally more accurate than a standard edit distance based nearest-neighbor classifier, an edit distance based kernel variant, and a random walk graph kernel},
author = {Neuhaus, Michel and Bunke, Horst},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2006.57},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A convolution edit kernel for error-tolerant graph matching - Neuhaus, Bunke - 2006.pdf:pdf},
isbn = {0-7695-2521-0},
issn = {10514651},
pages = {220--223},
title = {{A convolution edit kernel for error-tolerant graph matching}},
volume = {4},
year = {2006}
}
@article{Perozzi2014,
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide {\$}F{\_}1{\$} scores up to 10{\%} higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60{\%} less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
archivePrefix = {arXiv},
arxivId = {1403.6652},
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
doi = {10.1145/2623330.2623732},
eprint = {1403.6652},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/DeepWalk Online Learning of Social Representations - Perozzi, Al-Rfou, Skiena - 2014.pdf:pdf},
isbn = {978-1-4503-2956-9},
issn = {9781450329569},
title = {{DeepWalk: Online Learning of Social Representations}},
url = {http://arxiv.org/abs/1403.6652{\%}0Ahttp://dx.doi.org/10.1145/2623330.2623732},
year = {2014}
}
@article{Campbell1991,
author = {Campbell, Douglas M. and Radford, David},
doi = {10.2307/2690833},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Tree Isomorphism Algorithms Speed vs. Clarity - Campbell, Radford - 1991.pdf:pdf},
issn = {0025-570X; 1930-0980/e},
journal = {Mathematics Magazine},
number = {4},
pages = {252},
title = {{Tree Isomorphism Algorithms: Speed vs. Clarity}},
url = {http://www.jstor.org/stable/10.2307/2690833?origin=crossref{\%}0Ahttp://www.jstor.org/stable/10.2307/2690833?origin=crossref{\%}0Apapers3://publication/doi/10.2307/2690833},
volume = {64},
year = {1991}
}
@article{Rousseau2013,
abstract = {In this paper, we introduce novel document representation (grap-of-word) and retrieval model (TW-IDF) for ad hoc IR. Questioning the term independence assumption behind the traditional bag-of-word model, we propose a different representation of a document that captures the relationships between the terms using an unweighted directed graph of terms. From this graph, we extract at indexing time meangingful term weights (TW) that replace traditional term frequencies (TF) and from which we define a novel scoring function, namely TW-IDF, by analogy with TF-IDF. This approach leads to a retrieval model that consistently and significantly outperforms BM25 and in som cases its extension BM25+ on various standard TREC datasets. In particular, experiments show that counting the number of different contexts in which a term occurs inside a document is more effective and relevant to search than considering an overall convave term frequency in the context of ad hoc IR.},
author = {Rousseau, Fran{\c{c}}ois and Vazirgiannis, Michalis},
doi = {10.1145/2505515.2505671},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph-of-word and TW-IDF new approach to ad hoc IR - Rousseau, Vazirgiannis - 2013.pdf:pdf},
isbn = {9781450322638},
issn = {1450322638},
journal = {22nd ACM international conference on Conference on information {\&} knowledge management},
keywords = {IR theory,TW-IDF,grap-of-word,graph representation of documents,graph-based term weighting,scoring functions},
pages = {59--68},
title = {{Graph-of-word and TW-IDF: new approach to ad hoc IR}},
url = {http://dl.acm.org/citation.cfm?id=2505671},
year = {2013}
}
@article{Borgwardt2005,
author = {Borgwardt, K.M. and Kriegel, H.},
doi = {10.1109/ICDM.2005.132},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Shortest-Path Kernels on Graphs - Borgwardt, Kriegel - 2005.pdf:pdf},
isbn = {0-7695-2278-5},
issn = {1550-4786},
journal = {Fifth IEEE International Conference on Data Mining (ICDM'05)},
pages = {74--81},
title = {{Shortest-Path Kernels on Graphs}},
year = {2005}
}
@incollection{Rupp2012,
abstract = {It is widely believed that comparing discrepancies in the protein-protein interaction (PPI) networks of individuals will become an important tool in understanding and preventing diseases. Currently PPI networks for individuals are not available, but gene expression data is becoming easier to obtain and allows us to represent individuals by a co-integrated gene expression/protein interaction network. Two major problems hamper the application of graph kernels - state-of-the-art methods for whole-graph comparison - to compare PPI networks. First, these methods do not scale to graphs of the size of a PPI network. Second, missing edges in these interaction networks are biologically relevant for detecting discrepancies, yet, these methods do not take this into account. In this article we present graph kernels for biological network comparison that are fast to compute and take into account missing interactions. We evaluate their practical performance on two datasets of co-integrated gene expression/PPI networks.},
archivePrefix = {arXiv},
arxivId = {0807.0093},
author = {Rupp, Matthias},
booktitle = {Statistical and Machine Learning Approaches for Network Analysis},
doi = {10.1002/9781118346990.ch8},
eprint = {0807.0093},
isbn = {9780470195154},
issn = {1793-5091},
keywords = {Graph kernels defined on graphs, application in bi,Graph kernels, kernel-based algorithms for transfo,Graphlet kernels, randomly sampling small subgraph,Kernels for structured data, and idea of convoluti,Random walk kernels, label sequence, marginalized},
pages = {217--243},
pmid = {17992741},
title = {{Graph Kernels}},
year = {2012}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Scikit-learn Machine Learning in Python - Pedregosa et al. - 2012.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Siebertz2017,
abstract = {For {\$}\backslashalpha\backslashcolon\backslashmathbb{\{}N{\}}\backslashrightarrow\backslashmathbb{\{}R{\}}{\$}, an {\$}\backslashalpha{\$}-approximate bi-kernel is a polynomial-time algorithm that takes as input an instance {\$}(I, k){\$} of a problem {\$}Q{\$} and outputs an instance {\$}(I',k'){\$} of a problem {\$}Q'{\$} of size bounded by a function of {\$}k{\$} such that, for every {\$}c\backslashgeq 1{\$}, a {\$}c{\$}-approximate solution for the new instance can be turned into a {\$}c\backslashcdot\backslashalpha(k){\$}-approximate solution of the original instance in polynomial time. This framework of $\backslash$emph{\{}lossy kernelization{\}} was recently introduced by Lokshtanov et al. We prove that for every nowhere dense class of graphs, every {\$}\backslashalpha{\textgreater}1{\$} and {\$}r\backslashin\backslashmathbb{\{}N{\}}{\$} there exists a polynomial {\$}p{\$} (whose degree depends only on {\$}r{\$} while its coefficients depend on {\$}\backslashalpha{\$}) such that the connected distance-{\$}r{\$} dominating set problem with parameter {\$}k{\$} admits an {\$}\backslashalpha{\$}-approximate bi-kernel of size {\$}p(k){\$}. Furthermore, we show that this result cannot be extended to more general classes of graphs which are closed under taking subgraphs by showing that if a class {\$}C{\$} is somewhere dense and closed under taking subgraphs, then for some value of {\$}r\backslashin\backslashmathbb{\{}N{\}}{\$} there cannot exist an {\$}\backslashalpha{\$}-approximate bi-kernel for the (connected) distance-{\$}r{\$} dominating set problem on {\$}C{\$} for any function {\$}\backslashalpha\backslashcolon\backslashmathbb{\{}N{\}}\backslashrightarrow\backslashmathbb{\{}R{\}}{\$} (assuming the Gap Exponential Time Hypothesis).},
archivePrefix = {arXiv},
arxivId = {1707.09819},
author = {Siebertz, Sebastian},
eprint = {1707.09819},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Lossy kernels for connected distance-{\$}r{\$} domination on nowhere dense graph classes - Siebertz - 2017.pdf:pdf},
number = {665778},
title = {{Lossy kernels for connected distance-{\$}r{\$} domination on nowhere dense graph classes}},
url = {http://arxiv.org/abs/1707.09819},
year = {2017}
}
@article{Shuman2013,
abstract = {In applications such as social, energy, transportation, sensor, and neuronal networks, high-dimensional data naturally reside on the vertices of weighted graphs. The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs. In this tutorial overview, we outline the main challenges of the area, discuss different ways to define graph spectral domains, which are the analogues to the classical frequency domain, and highlight the importance of incorporating the irregular structures of graph data domains when processing signals on graphs. We then review methods to generalize fundamental operations such as filtering, translation, modulation, dilation, and downsampling to the graph setting, and survey the localized, multiscale transforms that have been proposed to efficiently extract information from high-dimensional data on graphs. We conclude with a brief discussion of open issues and possible extensions.},
archivePrefix = {arXiv},
arxivId = {1211.0053},
author = {Shuman, David I. and Narang, Sunil K. and Frossard, Pascal and Ortega, Antonio and Vandergheynst, Pierre},
doi = {10.1109/MSP.2012.2235192},
eprint = {1211.0053},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/The emerging field of signal processing on graphs Extending high-dimensional data analysis to networks and other irregular domains - Shu.pdf:pdf},
isbn = {1053-5888 VO - 30},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {3},
pages = {83--98},
pmid = {6494675},
title = {{The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains}},
volume = {30},
year = {2013}
}
@article{Douglas2011,
abstract = {Arxiv},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.5211v1},
author = {Douglas, Brendan L},
eprint = {arXiv:1101.5211v1},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/The Weisfeiler-Lehman Method and Graph Isomorphism Testing - Douglas - 2011.pdf:pdf},
journal = {ArXiv},
pages = {1--43},
title = {{The Weisfeiler-Lehman Method and Graph Isomorphism Testing}},
year = {2011}
}
@article{Suzuki2003,
abstract = {This paper proposes the “Hierarchical Di- rected Acyclic Graph (HDAG) Kernel” for structured natural language data. The HDAGKernel directly accepts several lev- els of both chunks and their relations, and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs. We applied the proposed method to question classifica- tion and sentence alignment tasks to eval- uate its performance as a similarity mea- sure and a kernel function. The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods.},
author = {Suzuki, Jun and Hirao, Tsutomu and Sasaki, Yutaka and Maeda, Eisaku},
doi = {10.3115/1075096.1075101},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Hierarchical Directed Acyclic Graph Kernel Methods for structured natural language data - Suzuki et al. - 2003.pdf:pdf},
journal = {Annual Meeting of the Association for computational Linguistics},
number = {July},
pages = {32--39},
title = {{Hierarchical Directed Acyclic Graph Kernel: Methods for structured natural language data}},
year = {2003}
}
@article{Wang2008,
abstract = {Document classification presents difficult challenges due to the sparsity$\backslash$nand the high dimensionality of text data, and to the complex semantics$\backslash$nof the natural language. The traditional document representation$\backslash$nis a word-based vector (Bag of Words, or BOW), where each dimension$\backslash$nis associated with a term of the dictionary containing all the words$\backslash$nthat appear in the corpus. Although simple and commonly used, this$\backslash$nrepresentation has several limitations. It is essential to embed$\backslash$nsemantic information and conceptual patterns in order to enhance$\backslash$nthe prediction capabilities of classification algorithms. In this$\backslash$npaper, we overcome the shortages of the BOW approach by embedding$\backslash$nbackground knowledge derived from Wikipedia into a semantic kernel,$\backslash$nwhich is then used to enrich the representation of documents. Our$\backslash$nempirical evaluation with real data sets demonstrates that our approach$\backslash$nsuccessfully achieves improved classification accuracy with respect$\backslash$nto the BOW technique, and to other recently developed methods.},
author = {Wang, Pu and Domeniconi, Carlotta},
doi = {10.1145/1401890.1401976},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Building semantic kernels for text classification using wikipedia - Wang, Domeniconi - 2008.pdf:pdf},
isbn = {9781605581934},
journal = {Kdd},
keywords = {kernel methods,semantic,text classification,wikipedia},
number = {August 2008},
pages = {713--721},
title = {{Building semantic kernels for text classification using wikipedia}},
year = {2008}
}
@article{Joachims1998,
abstract = {This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.},
author = {Joachims, Thorsten},
doi = {10.1007/BFb0026683},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Text Categorization with Support Vector Machines Learning with Many Relevant Features - Joachims - 1998.pdf:pdf},
isbn = {3540644172},
issn = {03436993},
journal = {Machine Learning},
number = {LS-8 Report 23},
pages = {137--142},
pmid = {9934216},
title = {{Text Categorization with Support Vector Machines: Learning with Many Relevant Features}},
url = {http://www.springerlink.com/index/drhq581108850171.pdf},
volume = {1398},
year = {1998}
}
@article{Mahe2009,
abstract = {Motivated by chemical applications, we revisit and extend a family of positive definite kernels for graphs based on the detection of common subtrees, initially proposed by Ramon and G{\"{a}}rtner (Proceedings of the first international workshop on mining graphs, trees and sequences, pp. 65–74, 2003). We propose new kernels with a parameter to control the complexity of the subtrees used as features to represent the graphs. This parameter allows to smoothly interpolate between classical graph kernels based on the count of common walks, on the one hand, and kernels that emphasize the detection of large common subtrees, on the other hand. We also propose two modular extensions to this formulation. The first extension increases the number of subtrees that define the feature space, and the second one removes noisy features from the graph representations. We validate experimentally these new kernels on problems of toxicity and anti-cancer activity prediction for small molecules with support vector machines.},
archivePrefix = {arXiv},
arxivId = {arXiv:q-bio/0609024v1},
author = {Mah{\'{e}}, Pierre and Vert, Jean Philippe},
doi = {10.1007/s10994-008-5086-2},
eprint = {0609024v1},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph kernels based on tree patterns for molecules - Mah{\'{e}}, Vert - 2009(2).pdf:pdf},
isbn = {978-3-642-04413-7},
issn = {08856125},
journal = {Machine Learning},
keywords = {Chemoinformatics,Graph kernels,Support vector machines},
number = {1},
pages = {3--35},
primaryClass = {arXiv:q-bio},
title = {{Graph kernels based on tree patterns for molecules}},
volume = {75},
year = {2009}
}
@article{Valerio2008,
author = {Valerio, Alejandro and Leake, David B and Ca{\~{n}}as, Alberto J and Ihmc, Machine Cognition},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Automatic classification of concept maps based on a topological taxonomy and its application to studying features of human-built maps -.pdf:pdf},
title = {{Automatic classification of concept maps based on a topological taxonomy and its application to studying features of human-built maps}},
year = {2008}
}
@article{McKinney2010,
abstract = {In this paper we are concerned with the practical issues of working with data sets common to finance, statistics, and other related fields. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss specific design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
author = {McKinney, Wes},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Data Structures for Statistical Computing in Python - McKinney - 2010.pdf:pdf},
isbn = {0440877763224},
issn = {0440877763224},
journal = {Proceedings of the 9th Python in Science Conference},
number = {Scipy},
pages = {51--56},
pmid = {1000224767},
title = {{Data Structures for Statistical Computing in Python}},
url = {http://conference.scipy.org/proceedings/scipy2010/mckinney.html},
volume = {1697900},
year = {2010}
}
@article{Trofimov2010,
abstract = {Earlier we introduced (M.I.Trofimov, E.A.Smolenskii, Application of the Electronegativity Indices of Organic Molecules to Tasks of Chemical Informatics, Russian Chemical Bulletin 54(2005), 2235-2246. DOI:10.1007/s11172-006-0105-6) effective recursive algorithm for graph isomorphism testing. In this paper we describe used approach and iterative modification of this algorithm, which modification has polynomial time complexity for all graphs.},
archivePrefix = {arXiv},
arxivId = {1004.1808},
author = {Trofimov, Michael I},
eprint = {1004.1808},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Polynomial Time Algorithm for Graph Isomorphism Testing - Trofimov - 2010.pdf:pdf},
isbn = {1117200601056},
journal = {Arxiv preprint arXiv10041808},
keywords = {graph invariant,graph isomorphism,np complexity},
pages = {9},
title = {{Polynomial Time Algorithm for Graph Isomorphism Testing}},
url = {http://arxiv.org/abs/1004.1808},
year = {2010}
}
@article{Gartner2003a,
abstract = {Kernel methods in general and support vector machines in particular have been successful in various learning tasks on data represented in a single table. Much 'real-world' data, however, is structured - it has no natural representation in a single table. Usually, to apply kernel methods to 'real-world' data, extensive pre-processing is performed to embed the data into areal vector space and thus in a single table. This survey describes several approaches of defining positive definite kernels on structured instances directly.},
author = {G{\"{a}}rtner, Thomas},
doi = {10.1145/959242.959248},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A survey of kernels for structured data - G{\"{a}}rtner - 2003.pdf:pdf},
isbn = {1931-0145},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
keywords = {inductive logic programming,ing,kernel methods,multi-relational data min-,structured data},
number = {1},
pages = {49},
title = {{A survey of kernels for structured data}},
url = {http://portal.acm.org/citation.cfm?doid=959242.959248},
volume = {5},
year = {2003}
}
@article{Jiang2010a,
abstract = {A graph-based approach to document classification is described in this paper. The graph representation offers the advantage that it allows for a much more expressive document encoding than the more standard bag of words/phrases approach, and consequently gives an improved classification accuracy. Document sets are represented as graph sets to which a weighted graph mining algorithm is applied to extract frequent subgraphs, which are then further processed to produce feature vectors (one per document) for classification. Weighted subgraph mining is used to ensure classification effectiveness and computational efficiency; only the most significant subgraphs are extracted. The approach is validated and evaluated using several popular classification algorithms together with a real world textual data set. The results demonstrate that the approach can outperform existing text classification algorithms on some dataset. When the size of dataset increased, further processing on extracted frequent features is essential. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Jiang, Chuntao and Coenen, Frans and Sanderson, Robert and Zito, Michele},
doi = {10.1016/j.knosys.2009.11.010},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Text classification using graph mining-based feature extraction - Jiang et al. - 2010(2).pdf:pdf},
isbn = {978-1-84882-982-4},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Feature extraction,Graph mining,Graph representation,Text classification,Weighted graph mining},
number = {4},
pages = {302--308},
publisher = {Elsevier B.V.},
title = {{Text classification using graph mining-based feature extraction}},
url = {http://dx.doi.org/10.1016/j.knosys.2009.11.010},
volume = {23},
year = {2010}
}
@article{Prieto2017,
abstract = {The pervasive presence of interconnected objects enables new communication paradigms where devices can easily reach each other while interacting within their environment. The so-called Internet of Things (IoT) represents the integration of several computing and communications systems aiming at facilitating the interaction between these devices. Arduino is one of the most popular platforms used to prototype new IoT devices due to its open, flexible and easy-to-use archi- tecture. Ardunio Yun is a dual board microcontroller that supports a Linux distribution and it is currently one of the most versatile and powerful Arduino systems. This feature positions Arduino Yun as a popular platform for developers, but it also introduces unique infection vectors from the secu- rity viewpoint. In this work, we present a security analysis of Arduino Yun. We show that Arduino Yun is vulnerable to a number of attacks and we implement a proof of concept capable of exploiting some of them.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.07016v1},
author = {Prieto, Luis P. and Rodr{\'{i}}guez-Triana, Mar{\'{i}}a Jes{\'{u}}s and Kusmin, Marge and Laanpere, Mart},
doi = {10.1145/1235},
eprint = {arXiv:1603.07016v1},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Smart school multimodal dataset and challenges - Prieto et al. - 2017(2).pdf:pdf},
isbn = {9781450321389},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Multimodal learning analytics,Multimodal teaching analytics,STEM education,Sensors,Smart classroom,Smart school},
pages = {53--59},
title = {{Smart school multimodal dataset and challenges}},
volume = {1828},
year = {2017}
}
@article{Kearnes2016,
abstract = {Molecular "fingerprints" encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular "graph convolutions", a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph---atoms, bonds, distances, etc.---which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement.},
archivePrefix = {arXiv},
arxivId = {1603.00856},
author = {Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick},
doi = {10.1007/s10822-016-9938-8},
eprint = {1603.00856},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Molecular graph convolutions moving beyond fingerprints - Kearnes et al. - 2016.pdf:pdf},
issn = {15734951},
journal = {Journal of Computer-Aided Molecular Design},
keywords = {Artificial neural networks,Deep learning,Machine learning,Molecular descriptors,Virtual screening},
number = {8},
pages = {595--608},
pmid = {27558503},
title = {{Molecular graph convolutions: moving beyond fingerprints}},
volume = {30},
year = {2016}
}
@article{Britton1978,
abstract = {Conducted 7 studies to determine lower bounds for the amount of lexical ambiguity of words used in English text. Two special types of ambiguity are described and quantified, and a refined method for quantifying the ambiguity of individual lexical units is presented. Results of the studies indicate that at least 32{\%} of the words used in English text were ambiguous; it is suggested, however, that this figure is probably conservative. Temporary word definitions established for special purposes occurred in 30{\%} of the sample of texts. (21 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Britton, Bruce K.},
doi = {10.3758/BF03205079},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Lexical ambiguity of words used in english text - Britton - 1978.pdf:pdf},
issn = {1554351X},
journal = {Behavior Research Methods {\&} Instrumentation},
number = {1},
pages = {1--7},
title = {{Lexical ambiguity of words used in english text}},
volume = {10},
year = {1978}
}
@article{Bleik2013,
author = {Bleik, Said and Mishra, Meenakshi and Huan, Jun and Song, Min},
doi = {10.1109/tcbb.2013.16},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Text Categorization of Biomedical Data Sets Using Graph Kernels and a Controlled Vocabulary - Bleik et al. - 2013.pdf:pdf},
isbn = {1545-5963},
issn = {1545-5963},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
keywords = {biomedical ontologies,classifier design and evaluation,graph kernels,graph representations,index terms,mining methods and algorithms,modeling structured,text categorization,text mining,textual and},
number = {5},
pages = {1211--1217},
title = {{Text Categorization of Biomedical Data Sets Using Graph Kernels and a Controlled Vocabulary}},
url = {http://ieeexplore.ieee.org/ielx7/8857/6731359/06475935.pdf?tp={\&}arnumber=6475935{\&}isnumber=6731359},
volume = {10},
year = {2013}
}
@article{Nikolentzos2017b,
abstract = {In this paper, we present a novel document similarity measure based on the
definition of a graph kernel between pairs of documents. The proposed measure
takes into account both the terms contained in the documents and the
relationships between them. By representing each document as a graph-of-words,
we are able to model these relationships and then determine how similar two
documents are by using a modified shortest-path graph kernel. We evaluate our
approach on two tasks and compare it against several baseline approaches using
various performance metrics such as DET curves and macro-average F1-score.
Experimental results on a range of datasets showed that our proposed approach
outperforms traditional techniques and is capable of measuring more accurately
the similarity between two documents.},
author = {Nikolentzos, Giannis and Meladianos, Polykarpos and Rousseau, Francois and Stavrakas, Yannis and Vazirgiannis, Michalis},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Shortest-Path Graph Kernels for Document Similarity - Nikolentzos et al. - 2017(2).pdf:pdf},
journal = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
pages = {1891--1901},
title = {{Shortest-Path Graph Kernels for Document Similarity}},
url = {https://www.aclweb.org/anthology/D17-1202},
year = {2017}
}
@article{Zhou2017,
abstract = {Representation learning for molecules is important for molecular properties prediction, material design, drug screening, etc. In this work a graph convolutional network architecture for learning representations for molecules is presented. An operation for convolving k-neighbourhood of a specific node in graph is defined, which is corresponding to kernel size of k in convolutional neural networks. Besides, A module of adaptive filtering is defined to find the sampling locations based on graph connections and node features.},
archivePrefix = {arXiv},
arxivId = {1706.09916},
author = {Zhou, Zhenpeng},
eprint = {1706.09916},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph Convolutional Networks for Molecules - Zhou - 2017.pdf:pdf},
title = {{Graph Convolutional Networks for Molecules}},
url = {http://arxiv.org/abs/1706.09916},
year = {2017}
}
@article{Varma2006,
abstract = {BACKGROUND: Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data. RESULTS: We used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these "null" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With "null" and "non null" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error. The CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the "null" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30{\{}{\%}{\}} on 18.5{\{}{\%}{\}} of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30{\{}{\%}{\}} on 38{\{}{\%}{\}} of "null" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance. The nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for "null" and "non-null" data distributions. CONCLUSION: We show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error.},
author = {Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Bias in error estimation when using cross-validation for model selection - Varma, Simon - 2006.pdf:pdf},
isbn = {1471-2105},
issn = {14712105},
journal = {BMC Bioinformatics},
pages = {1--8},
pmid = {16504092},
title = {{Bias in error estimation when using cross-validation for model selection}},
volume = {7},
year = {2006}
}
@article{Henaff2015,
abstract = {Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.},
archivePrefix = {arXiv},
arxivId = {1506.05163},
author = {Henaff, Mikael and Bruna, Joan and LeCun, Yann},
eprint = {1506.05163},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Deep Convolutional Networks on Graph-Structured Data - Henaff, Bruna, LeCun - 2015.pdf:pdf},
pages = {1--10},
title = {{Deep Convolutional Networks on Graph-Structured Data}},
url = {http://arxiv.org/abs/1506.05163},
year = {2015}
}
@article{Forman2003,
abstract = {Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives—accuracy, F-measure, precision, and recall—since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair—e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.},
author = {Forman, George},
doi = {10.1162/153244303322753670},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/An Extensive Empirical Study of Feature Selection Metrics for Text Classification - Forman - 2003.pdf:pdf},
isbn = {1558604863},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {ROC,document categorization,supervised learning,support vector machines},
pages = {1289--1305},
title = {{An Extensive Empirical Study of Feature Selection Metrics for Text Classification}},
volume = {3},
year = {2003}
}
@article{Vishwanathan2006,
abstract = {It is widely believed that comparing discrepancies in the protein-protein interaction (PPI) networks of individuals will become an important tool in understanding and preventing diseases. Currently PPI networks for individuals are not available, but gene expression data is becoming easier to obtain and allows us to represent individuals by a co-integrated gene expression/protein interaction network. Two major problems hamper the application of graph kernels - state-of-the-art methods for whole-graph comparison - to compare PPI networks. First, these methods do not scale to graphs of the size of a PPI network. Second, missing edges in these interaction networks are biologically relevant for detecting discrepancies, yet, these methods do not take this into account. In this article we present graph kernels for biological network comparison that are fast to compute and take into account missing interactions. We evaluate their practical performance on two datasets of co-integrated gene expression/PPI networks.},
author = {Vishwanathan, S V N and Schraudolph, Nicol N and Borgwardt, Karsten M and Kriegel, Hans-Peter},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Fast computation of graph kernels - Vishwanathan et al. - 2006.pdf:pdf},
isbn = {0-262-19568-2},
issn = {1793-5091},
journal = {Advances in Neural Information Processing Systems},
keywords = {Computational Biology,Databases,Disease Progression,Gene Expression Profiling,Gene Expression Profiling: statistics {\&} numerical,Genetic,Humans,Learning/Statistics {\&} Optimisation,Prognosis,Protein Array Analysis,Protein Array Analysis: statistics {\&} numerical dat,Protein Interaction Mapping,Protein Interaction Mapping: statistics {\&} numerica},
pages = {1449--1456},
pmid = {17992741},
title = {{Fast computation of graph kernels}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17992741{\%}5Cnhttp://eprints.pascal-network.org/archive/00003990/},
volume = {11},
year = {2006}
}
@article{Chowdhury2003,
abstract = {ABSTRACT Work in computational linguistics began very soon after the development of the first computers (Booth, Brandwood and Cleave 1958), yet in the intervening four decades there has been a pervasive feeling that progress in computer understanding of natural language has not been commensurate with progress in other computer applications. Recently, a number of prominent researchers in natural language processing met to assess the state of the discipline and discuss future directions (Bates and Weischedel 1993). The consensus of this meeting was that increased attention to large amounts of lexical and domain knowledge was essential for significant progress, and current research efforts in the field reflect this point of view.},
archivePrefix = {arXiv},
arxivId = {0812.0143v2},
author = {Chowdhury, Gobinda G.},
doi = {10.1017/S0267190500001446},
eprint = {0812.0143v2},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Natural Language Processing - Chowdhury - 2003.pdf:pdf},
isbn = {9783662459232},
issn = {0267-1905},
journal = {Annual Review of Applied Linguistics},
number = {1},
pages = {51--89},
pmid = {21846786},
title = {{Natural Language Processing}},
volume = {37},
year = {2003}
}
@article{Wang2010,
author = {Wang, Xiaohong and Smalter, Aaron and Huan, Jun and Lushington, Gerald H},
doi = {10.1145/1516360.1516416.G-Hash},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/G-Hash Towards Fast Kernel-based Similarity Search in Large Graph Databases - Wang et al. - 2010.pdf:pdf},
journal = {Database},
pages = {472--480},
title = {{G-Hash: Towards Fast Kernel-based Similarity Search in Large Graph Databases}},
year = {2010}
}
@misc{Deerwester1990,
author = {Deerwester, S and Dumais, ST and Furnas, GW},
booktitle = {Journal of the},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Indexing by latent semantic analysis - Deerwester, Dumais, Furnas - 1990.pdf:pdf},
title = {{Indexing by latent semantic analysis}},
url = {http://search.proquest.com/openview/a1907164bd88dfc38a4875b73a3f7b3d/1?pq-origsite=gscholar{\&}cbl=1818555},
year = {1990}
}
@article{Gulrandhe2015,
abstract = {Recently, graph representations of text have been showing improved performance over conventional bag-of-words representations in text categorization applications. In this paper, we present a graph-based representation for biomedical articles and use graph kernels to classify those articles into high-level categories. In our representation, common biomedical concepts and semantic relationships are identified with the help of an existing ontology and are used to build a rich graph structure that provides a consistent feature set and preserves additional semantic information that could improve a classifier's performance. We attempt to classify the graphs using both a set-based graph kernel that is capable of dealing with the disconnected nature of the graphs and a simple linear kernel. Finally, we report the results comparing the classification performance of the kernel classifiers to common text based classifiers.},
author = {Gulrandhe, Chetna and Bawankar, Chetan},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Concept Graph Preserving Semantic Relationship for Biomedical Text Categorization - Gulrandhe, Bawankar - 2015.pdf:pdf},
journal = {International Journal Of Computer Science And Applications},
keywords = {biomedical ontology,classifier design and,evaluation,graph kernels,graph representations,methods and algorithms,mining,modelling structured,query processing,s,text categorization,text mining,text retrieval,textual and multimedia data},
number = {1},
pages = {9--12},
title = {{Concept Graph Preserving Semantic Relationship for Biomedical Text Categorization}},
volume = {8},
year = {2015}
}
@article{Shervashidze2009a,
abstract = {State-of-the-art graph kernels do not scale to large graphs with hundreds of nodes and thousands of edges. In this article we propose to compare graphs by counting graphlets, i.e., subgraphs with k nodes where k ∈ {\{}3, 4, 5{\}}. Exhaustive enumeration of all graphlets be-ing prohibitively expensive, we introduce two theoretically grounded speedup schemes, one based on sampling and the second one specif-ically designed for bounded degree graphs. In our experimental evaluation, our novel kernels allow us to efficiently compare large graphs that cannot be tackled by existing graph kernels.},
author = {Shervashidze, Nino and Vishwanathan, S V N and Petri, Tobias H and Mehlhorn, Kurt and Borgwardt, Karsten M},
doi = {10.1.1.165.7842},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Efficient Graphlet Kernels for Large Graph Comparison - Shervashidze et al. - 2009.pdf:pdf},
isbn = {1938-7228},
issn = {15324435},
journal = {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},
pages = {488--495},
title = {{Efficient Graphlet Kernels for Large Graph Comparison}},
volume = {5},
year = {2009}
}
@article{Figueiredo2011,
abstract = {In this article we propose a data treatment strategy to generate new discriminative features, called compound-features (or c-features), for the sake of text classification. These c-features are composed by terms that co-occur in documents without any restrictions on order or distance between terms within a document. This strategy precedes the classification task, in order to enhance documents with discriminative c-features. The idea is that, when c-features are used in conjunction with single-features, the ambiguity and noise inherent to their bag-of-words representation are reduced. We use c-features composed of two terms in order to make their usage computationally feasible while improving the classifier effectiveness. We test this approach with several classification algorithms and single-label multi-class text collections. Experimental results demonstrated gains in almost all evaluated scenarios, from the simplest algorithms such as kNN (13{\%} gain in micro-average F1 in the 20 Newsgroups collection) to the most complex one, the state-of-the-art SVM (10{\%} gain in macro-average F1 in the collection OHSUMED). {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Figueiredo, F{\'{a}}bio and Rocha, Leonardo and Couto, Thierson and Salles, Thiago and Gon{\c{c}}alves, Marcos Andr{\'{e}} and Meira, Wagner},
doi = {10.1016/j.is.2011.02.002},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Word co-occurrence features for text classification - Figueiredo et al. - 2011.pdf:pdf},
issn = {03064379},
journal = {Information Systems},
keywords = {Classification,Feature extraction,Text mining},
number = {5},
pages = {843--858},
title = {{Word co-occurrence features for text classification}},
volume = {36},
year = {2011}
}
@inproceedings{Hido2009,
abstract = {The design of a good kernel is fundamental for knowledge discovery from graph-structured data. Existing graph kernels exploit only limited information about the graph structures but are still computationally expensive. We propose a novel graph kernel based on the structural characteristics of graphs. The key is to represent node labels as binary arrays and characterize each node using logical operations on the label set of the connected nodes. Our kernel has a linear time complexity with respect to the number of nodes times the average number of neighboring nodes in the given graphs. The experimental result shows that the proposed kernel performs comparable and much faster than a state-of-the-art graph kernel for benchmark data sets and shows high scalability for new applications with large graphs.},
author = {Hido, Shohei and Kashima, Hisashi},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2009.30},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A linear-time graph kernel - Hido, Kashima - 2009.pdf:pdf},
isbn = {9780769538952},
issn = {15504786},
keywords = {Coding theory,Graph kernel,Hash},
pages = {179--188},
title = {{A linear-time graph kernel}},
year = {2009}
}
@article{Defferrard2016,
abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
archivePrefix = {arXiv},
arxivId = {1606.09375},
author = {Defferrard, Micha{\"{e}}l and Bresson, Xavier and Vandergheynst, Pierre},
eprint = {1606.09375},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering - Defferrard, Bresson, Vandergheynst - 2016(2).pdf:pdf},
issn = {10495258},
number = {Nips},
title = {{Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering}},
url = {http://arxiv.org/abs/1606.09375},
year = {2016}
}
@article{Li2004,
abstract = {This paper is a comparative study of feature selection methods in$\backslash$nstatistical learning of text categorization . The focus is on aggres-$\backslash$nsive dimensionality reduction. Five meth- ods were evaluated, including$\backslash$nterm selection based on document frequency (DF), informa- tion gain$\backslash$n...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Li, T. and Zhang, C. and Ogihara, M.},
doi = {10.1093/bioinformatics/bth267},
eprint = {arXiv:1011.1669v3},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression - Li,.pdf:pdf},
isbn = {1558604863},
issn = {1367-4803},
journal = {Bioinformatics},
number = {15},
pages = {2429--2437},
pmid = {15087314},
title = {{A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/bth267},
volume = {20},
year = {2004}
}
@article{Peng2010,
abstract = {This paper presents a novel feature selection approach to deal with issues of high dimensionality in biomedical data classification. Extensive research has been performed in the field of pattern recognition and machine learning. Dozens of feature selection methods have been developed in the literature, which can be classified into three main categories: filter, wrapper and hybrid approaches. Filter methods apply an independent test without involving any learning algorithm, while wrapper methods require a predetermined learning algorithm for feature subset evaluation. Filter and wrapper methods have their, respectively, drawbacks and are complementary to each other in that filter approaches have low computational cost with insufficient reliability in classification while wrapper methods tend to have superior classification accuracy but require great computational power. The approach proposed in this paper integrates filter and wrapper methods into a sequential search procedure with the aim to improve the classification performance of the features selected. The proposed approach is featured by (1) adding a pre-selection step to improve the effectiveness in searching the feature subsets with improved classification performances and (2) using Receiver Operating Characteristics (ROC) curves to characterize the performance of individual features and feature subsets in the classification. Compared with the conventional Sequential Forward Floating Search (SFFS), which has been considered as one of the best feature selection methods in the literature, experimental results demonstrate that (i) the proposed approach is able to select feature subsets with better classification performance than the SFFS method and (ii) the integrated feature pre-selection mechanism, by means of a new selection criterion and filter method, helps to solve the over-fitting problems and reduces the chances of getting a local optimal solution. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Peng, Yonghong and Wu, Zhiqing and Jiang, Jianmin},
doi = {10.1016/j.jbi.2009.07.008},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A novel feature selection approach for biomedical data classification - Peng, Wu, Jiang - 2010.pdf:pdf},
isbn = {0010-4809},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Biomedical data classification,Feature selection,ROC,SFFS},
number = {1},
pages = {15--23},
pmid = {19647098},
publisher = {Elsevier Inc.},
title = {{A novel feature selection approach for biomedical data classification}},
url = {http://dx.doi.org/10.1016/j.jbi.2009.07.008},
volume = {43},
year = {2010}
}
@article{Feragen2013,
abstract = {While graphs with continuous node attributes arise in many applications, state- of-the-art graph kernels for comparing continuous-attributed graphs suffer from a high runtime complexity. For instance, the popular shortest path kernel scales as O(n4), where n is the number of nodes. In this paper, we present a class of graph kernels with computational complexity O(n2(m + log n + $\delta$2 + d)), where $\delta$ is the graph diameter, m is the number of edges, and d is the dimension of the node attributes. Due to the sparsity and small diameter of real-world graphs, these kernels typically scale comfortably to large graphs. In our experiments, the presented kernels outperform state-of-the-art kernels in terms of speed and accuracy on classification benchmark datasets.},
author = {Feragen, Aasa and Kasenburg, Niklas and Petersen, Jens and de Bruijne, Marleen and Borgwardt, Karsten M.},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Scalable kernels for graphs with continuous attributes - Feragen et al. - 2013.pdf:pdf},
issn = {10495258},
journal = {Neural Information Processing Systems (NIPS) 2013},
keywords = {Forschungsgruppe Borgwardt},
pages = {216--224},
title = {{Scalable kernels for graphs with continuous attributes}},
url = {http://papers.nips.cc/paper/5155-scalable-kernels-for-graphs-with-continuous-attributes.pdf},
volume = {2},
year = {2013}
}
@article{Scholkopf1998,
abstract = {A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map—for instance, the space of all possible five-pixel products in 16 × 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sch{\"{o}}lkopf, Bernhard and Smola, Alexander and M{\"{u}}ller, Klaus-Robert},
doi = {10.1162/089976698300017467},
eprint = {arXiv:1011.1669v3},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Nonlinear Component Analysis as a Kernel Eigenvalue Problem - Sch{\"{o}}lkopf, Smola, M{\"{u}}ller - 1998.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {5},
pages = {1299--1319},
pmid = {21939901},
title = {{Nonlinear Component Analysis as a Kernel Eigenvalue Problem}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976698300017467},
volume = {10},
year = {1998}
}
@article{Hermansson2015,
abstract = {We consider the problem of classifying graphs using graph kernels. We define a new graph kernel, called the generalized shortest path kernel, based on the number and length of shortest paths between nodes. For our example classification problem, we consider the task of classifying random graphs from two well-known families, by the number of clusters they contain. We verify empirically that the generalized shortest path kernel outperforms the original shortest path kernel on a number of datasets. We give a theoretical analysis for explaining our experimental results. In particular, we estimate distributions of the expected feature vectors for the shortest path kernel and the generalized shortest path kernel, and we show some evidence explaining why our graph kernel outperforms the shortest path kernel for our graph classification problem.},
archivePrefix = {arXiv},
arxivId = {1510.06492},
author = {Hermansson, Linus and Johansson, Fredrik D. and Watanabe, Osamu},
doi = {10.1007/978-3-319-24282-8_8},
eprint = {1510.06492},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Generalized shortest path kernel on graphs - Hermansson, Johansson, Watanabe - 2015.pdf:pdf},
isbn = {9783319242811},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Graph kernel,Machine learning,SVM,Shortest path},
pages = {78--85},
title = {{Generalized shortest path kernel on graphs}},
volume = {9356},
year = {2015}
}
@article{Cachopo2007,
abstract = {As the volume of information in digital form increases, the use of Text Categoriza- tion techniques aimed at finding relevant information becomes more necessary. To improve the quality of the classification, I propose the combination of dif- ferent classification methods. The results show that k-NN-LSI, the combination of k-NN with LSI, presents an average Accuracy on the five datasets that is higher than the average Accuracy of each original method. The results also show that SVM-LSI, the combination of SVM with LSI, outperforms both original meth- ods in some datasets. Having in mind that SVM is usually the best performing method, it is particularly interesting that SVM-LSI performs even better in some situations. To reduce the number of labeled documents needed to train the classifier, I propose the use of a semi-supervised centroid-based method that uses informa- tion from small volumes of labeled data together with information from larger volumes of unlabeled data for text categorization. Using one synthetic dataset and three real-world datasets, I provide empirical evidence that, if the initial clas- sifier for the data is sufficiently precise, using unlabeled data improves perfor- mance. On the other hand, using unlabeled data actually degrades the results if the initial classifier is not good enough. The dissertation includes a comprehensive comparison between the classifi- cation methods that are most frequently used in the Text Categorization area and the combinations of methods proposed.},
author = {Cachopo, Ana Margarida De Jesus Cardoso},
doi = {10.1002/cplx},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Improving Methods for Single-label Text Categorization - Cachopo - 2007.pdf:pdf},
keywords = {Combining Classification Methods,Datasets,Evaluation Metrics,Incremental Classification,Semi-supervised Classification,Text Categorization},
pages = {167},
title = {{Improving Methods for Single-label Text Categorization}},
url = {http://web.ist.utl.pt/acardoso/},
year = {2007}
}
@article{The2017,
author = {The, Koji Tsuda and Kwansei, Akihiro Inokuchi},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Kernels for Graphs - The, Kwansei - 2017.pdf:pdf},
number = {September},
title = {{Kernels for Graphs}},
year = {2017}
}
@article{West,
author = {West, B},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Introduction to Graph Theory - West - Unknown.pdf:pdf},
title = {{Introduction to Graph Theory}}
}
@article{Li2011,
author = {Li, Geng and Semerci, Murat and Yener, Bulent},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph Classification via Topological and Label Attributes - Li, Semerci, Yener - 2011.pdf:pdf},
isbn = {9781450308342},
journal = {Mlg},
title = {{Graph Classification via Topological and Label Attributes}},
url = {http://users.cis.fiu.edu/{~}lzhen001/activities/KDD2011Program/workshops/MLG/doc/paper{\_}1.pdf},
year = {2011}
}
@article{Manessi2017,
abstract = {Many different classification tasks need to manage structured data, which are usually modeled as graphs. Moreover, these graphs can be dynamic, meaning that the vertices/edges of each graph may change during time. Our goal is to jointly exploit structured data and temporal information through the use of a neural network model. To the best of our knowledge, this task has not been addressed using these kind of architectures. For this reason, we propose two novel approaches, which combine Long Short-Term Memory networks and Graph Convolutional Networks to learn long short-term dependencies together with graph structure. The quality of our methods is confirmed by the promising results achieved.},
archivePrefix = {arXiv},
arxivId = {1704.06199},
author = {Manessi, Franco and Rozza, Alessandro and Manzo, Mario},
eprint = {1704.06199},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Dynamic Graph Convolutional Networks - Manessi, Rozza, Manzo - 2017.pdf:pdf},
pages = {1--16},
title = {{Dynamic Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1704.06199},
year = {2017}
}
@article{Canas2008,
abstract = {This website aims to present the results of the ongoing research on a cartographic approach to the representation of knowledge in its present configurations. The aim of the research is to extend the cartographic metaphor beyond visual analogy, and to expose it as a narrative model and tool to intervene in complex, heterogeneous, dynamic realities, just like those of human geography. The map, in this context, is not only a passive representation of reality but a tool for the production of meaning. The map is thus a communication device: a mature representation artefact, aware of its own language and its own rhetoric, equipped with it its own tools, languages, techniques and supports. A model that recovers the narrative abilities of pre-scientific maps and presents itself not as a mere mimetic artefact, but as a poetic and political tool...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ca{\~{n}}as, Alberto J and Novak, Joseph D},
doi = {10.1007/978-1-84800-149-7},
eprint = {arXiv:1011.1669v3},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Concept Mapping Using CmapTools to Enhance Meaningful Learning - Ca{\~{n}}as, Novak - 2008.pdf:pdf},
isbn = {978-1-84800-148-0},
issn = {1610-3947},
pages = {23--46},
pmid = {25246403},
title = {{Concept Mapping Using CmapTools to Enhance Meaningful Learning}},
url = {http://link.springer.com/10.1007/978-1-84800-149-7},
year = {2008}
}
@article{Needham2011,
abstract = {Polyurea is a long-life pavement marking material used for assets requiring long periods of uninterrupted accessibility. Knowing the performance characteristics of such markings is critical to asset management planning focused on maximizing marking material life-cycles. This paper presents the performance characteristics of polyurea pavement markings in North Carolina using linear regression models. This research constructed performance models for polyurea based on the independent variables of time, initial retroreflectivity, and lateral line location. The models generated by this research provide pavement marking managers with tools to better allocate limited manpower and resources in order to optimize budgets while meeting newly proposed pavement marking retroreflectivity levels of service as proposed by the Federal Highway Administration. Using the models generated by this research, the pavement marking manager can predict the level of service and remaining life of a given pavement marking. A key finding of this paper is that polyurea pavement marking degradation is significantly impacted by the type of glass bead inserted into the marking.},
author = {Needham, Jonathan D},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Air force institute of technology - Needham - 2011.pdf:pdf},
title = {{Air force institute of technology}},
year = {2011}
}
@article{Graf2001,
abstract = {In this article, we discuss issues about formulations of support vector machines (SVM) from an optimization point of view. First, SVMs map training data into a higher- (maybe infinite-) dimensional space. Currently primal and dual formulations of SVM are derived in the finite dimensional space and readily extend to the infinite-dimensional space. We rigorously discuss the primal-dual relation in the infinite-dimensional spaces. Second, SVM formulations contain penalty terms, which are different from unconstrained penalty functions in optimization. Traditionally unconstrained penalty functions approximate a constrained problem as the penalty parameter increases. We are interested in similar properties for SVM formulations. For two of the most popular SVM formulations, we show that one enjoys properties of exact penalty functions, but the other is only like traditional penalty functions, which converge when the penalty parameter goes to infinity.},
author = {Graf, Arnulf B a and Borer, Silvio},
doi = {10.1007/3-540-45404-7_37},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Normalization in Support Vector Machines - Graf, Borer - 2001.pdf:pdf},
isbn = {9783540425960},
journal = {Neural Computation},
keywords = {feature space,input space,malization,nor,optimal separating hyperplane,support vector machines},
pages = {277--282},
title = {{Normalization in Support Vector Machines}},
url = {http://www.springerlink.com/index/q3068112g9332857.pdf},
volume = {13},
year = {2001}
}
@article{DeCastilho2014,
abstract = {There is a current trend to combine natural language analysis with$\backslash$nresearch questions from the humanities. This requires an integration of$\backslash$nautomatic analysis with manual analysis, e.g. to develop a theory behind$\backslash$nthe analysis, to test the theory against a corpus, to generate training$\backslash$ndata for automatic analysis based on machine learning algorithms, and to$\backslash$nevaluate the quality of the results from automatic analysis. Manual$\backslash$nanalysis is traditionally the domain of linguists, philosophers, and$\backslash$nresearchers from other humanities disciplines, who are often not expert$\backslash$nprogrammers. Automatic analysis, on the other hand, is traditionally done$\backslash$nby expert programmers, such as computer scientists and more recently$\backslash$ncomputational linguists. It is important to bring these communities, their$\backslash$ntools, and data closer together, to produce analysis of a higher quality$\backslash$nwith less effort. However, promising cooperations involving manual and$\backslash$nautomatic analysis, e.g. for the purpose of analyzing a large corpus, are$\backslash$nhindered by many problems:$\backslash$n$\backslash$n$\backslash$nNo comprehensive set of interoperable automatic analysis components is$\backslash$navailable.$\backslash$nAssembling automatic analysis components into workflows is too$\backslash$ncomplex.$\backslash$nAutomatic analysis tools, exploration tools, and annotation editors are$\backslash$nnot interoperable.$\backslash$nWorkflows are not portable between computers.$\backslash$nWorkflows are not easily deployable to a compute cluster.$\backslash$nThere are no adequate tools for the selective annotation of large$\backslash$ncorpora.$\backslash$nIn automatic analysis, annotation type systems are predefined, but$\backslash$nmanual annotation requires customizability.$\backslash$nImplementing new interoperable automatic analysis components is too$\backslash$ncomplex.$\backslash$nWorkflows and components are not sufficiently debuggable and$\backslash$nrefactorable.$\backslash$nWorkflows that change dynamically via parametrization are not readily$\backslash$nsupported.$\backslash$nThe user has no control over workflows that rely on expert skills from$\backslash$na different domain, undocumented knowledge, or third-party infrastructures,$\backslash$ne.g. web services.$\backslash$n$\backslash$n$\backslash$nIn cooperation with researchers from the humanities, we develop$\backslash$ninnovative technical solutions and designs to facilitate the use of$\backslash$nautomatic analysis and to promote the integration of manual and automatic$\backslash$nanalysis. To address these issues, we set foundations in four areas:$\backslash$n$\backslash$n$\backslash$nUsability is improved by reducing the complexity of the APIs for$\backslash$nbuilding workflows and creating custom components, improving the handling$\backslash$nof resources required by such components, and setting up auto-configuration$\backslash$nmechanisms.$\backslash$nReproducibility is improved through a concept for self-contained,$\backslash$nportable analysis components and workflows combined with a declarative$\backslash$nmodeling approach for dynamic parametrized workflows, that facilitates$\backslash$navoiding unnecessary auxiliary manual steps in automatic workflows.$\backslash$nFlexibility is achieved by providing an extensive collection of$\backslash$ninteroperable automatic analysis components. We also compare annotation$\backslash$ntype systems used by different automatic analysis components to locate$\backslash$ndesign patterns that allow for customization when used in manual analysis$\backslash$ntasks.$\backslash$nInteractivity is achieved through a novel "annotation-by-query" process$\backslash$ncombining corpus search with annotation in a multi-user scenario. The$\backslash$nprocess is supported by a web-based tool.$\backslash$n$\backslash$n$\backslash$nWe demonstrate the adequacy of our concepts through examples which$\backslash$nrepresent whole classes of research problems. Additionally, we integrated$\backslash$nall our concepts into existing open-source projects, or we implemented and$\backslash$npublished them within new open-source projects.},
author = {de Castilho, Richard Eckart},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Natural Language Processing Integration of Automatic and Manual Analysis - de Castilho - 2014.pdf:pdf},
pages = {205},
title = {{Natural Language Processing: Integration of Automatic and Manual Analysis}},
year = {2014}
}
@book{Eds2013,
author = {Eds, Sebastian Rudolph and Hutchison, David},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/The Semantic Web Semantics and Big Data - Eds, Hutchison - 2013.pdf:pdf},
isbn = {9783642382871},
number = {May},
title = {{The Semantic Web : Semantics and Big Data}},
year = {2013}
}
@article{Duvenaud2015,
abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
archivePrefix = {arXiv},
arxivId = {1509.09292},
author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and G{\'{o}}mez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'{a}}n and Adams, Ryan P.},
doi = {10.1021/acs.jcim.5b00572},
eprint = {1509.09292},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Convolutional Networks on Graphs for Learning Molecular Fingerprints - Duvenaud et al. - 2015(2).pdf:pdf},
isbn = {9783527678679},
issn = {10495258},
pages = {1--9},
pmid = {26726827},
title = {{Convolutional Networks on Graphs for Learning Molecular Fingerprints}},
url = {http://arxiv.org/abs/1509.09292},
year = {2015}
}
@article{Wang2008a,
abstract = {Traditional approaches to document classification requires labeled data in order to construct reliable and accurate classifiers. Unfortunately, labeled data are seldom available, and often too expensive to obtain. Given a learning task for which training data are not available, abundant labeled data may exist for a different but related domain. One would like to use the related labeled data as auxiliary information to accomplish the classification task in the target domain. Recently, the paradigm of transfer learning has been introduced to enable effective learning strategies when auxiliary data obey a different probability distribution. A co-clustering based classification algorithm has been previously proposed to tackle cross-domain text classification. In this work, we extend the idea underlying this approach by making the latent semantic relationship between the two domains explicit. This goal is achieved with the use of Wikipedia. As a result, the pathway that allows to propagate labels between the two domains not only captures common words, but also semantic concepts based on the content of documents. We empirically demonstrate the efficacy of our semantic-based approach to cross-domain classification using a variety of real data.},
author = {Wang, Pu and Domeniconi, Carlotta and Hu, Jian},
doi = {10.1109/ICDM.2008.136},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Using wikipedia for co-clustering based cross-domain text classification - Wang, Domeniconi, Hu - 2008.pdf:pdf},
isbn = {9780769535029},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
pages = {1085--1090},
title = {{Using wikipedia for co-clustering based cross-domain text classification}},
year = {2008}
}
@article{Hofmann2008,
abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0701907v3},
author = {Hofmann, Thomas and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J.},
doi = {10.1214/009053607000000677},
eprint = {0701907v3},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Kernel methods in machine learning - Hofmann, Sch{\"{o}}lkopf, Smola - 2008.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Graphical models,Machine learning,Reproducing kernels,Support vector machines},
number = {3},
pages = {1171--1220},
primaryClass = {arXiv:math},
title = {{Kernel methods in machine learning}},
volume = {36},
year = {2008}
}
@article{Fisher1925,
abstract = {14th ed., rev. and enlarged.},
archivePrefix = {arXiv},
arxivId = {0-05-002170-2},
author = {Fisher, Ronald Aylmer},
doi = {10.1056/NEJMc061160},
eprint = {0-05-002170-2},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Statistical Methods for Research Workers - Fisher - 1925.pdf:pdf},
isbn = {0-050-02170-2.},
issn = {15334406},
pages = {362},
pmid = {10139950},
title = {{Statistical Methods for Research Workers}},
year = {1925}
}
@article{Namomsa1965a,
author = {Haussler, David},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Convolution Kernels on Discrete Structures - Haussler - 1965.pdf:pdf},
pages = {1--25},
title = {{Convolution Kernels on Discrete Structures}},
year = {1965}
}
@article{Hernandez2011,
abstract = {This article aims to order and classify a wide number of metrics, proposed to characterize graphs, and the services using those graphs. The number of proposed metrics over the graph history is overwhelming. Over the years, scientists constantly introduce new metrics in order to measure specific features of specific graphs. Aiming for generality, this research will focus on the classification of unweighted, undirected, general graph metrics.},
author = {Hernandez, Javier Martin and {Van Mieghem}, Piet},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Classification of graph metrics - Hernandez, Van Mieghem - 2011.pdf:pdf},
keywords = {correlation,graph metrics,service,topology},
pages = {1--20},
title = {{Classification of graph metrics}},
year = {2011}
}
@article{Li2015,
abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
archivePrefix = {arXiv},
arxivId = {1511.05493},
author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
doi = {10.1103/PhysRevLett.116.082003},
eprint = {1511.05493},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Gated Graph Sequence Neural Networks - Li et al. - 2015.pdf:pdf},
issn = {0031-9007},
number = {1},
pages = {1--20},
title = {{Gated Graph Sequence Neural Networks}},
url = {http://arxiv.org/abs/1511.05493},
year = {2015}
}
@article{Hagberg2008,
abstract = {NetworkX is a Python language package for exploration and analysis of networks and network algorithms. The core package provides data structures for representing many types of networks, or graphs, including simple graphs, directed graphs, and graphs with parallel edges and self-loops. The nodes in NetworkX graphs can be any (hashable) Python object and edges can contain arbitrary data; this flexibility makes NetworkX ideal for representing networks found in many different scientific fields. In addition to the basic data structures many graph algorithms are implemented for calculating network properties and structure measures: shortest paths, betweenness centrality, clustering, and degree distribution and many more. NetworkX can read and write various graph formats for easy exchange with existing data, and provides generators for many classic graphs and popular graph models, such as the Erdos-Renyi, Small World, and Barabasi-Albert models. The ease-of-use and flexibility of the Python programming language together with connection to the SciPy tools make NetworkX a powerful tool for scientific computations. We discuss some of our recent work studying synchronization of coupled oscillators to demonstrate how NetworkX enables research in the field of computational networks.},
author = {Hagberg, Aric A. and Schult, Daniel A. and Swart, Pieter J.},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Exploring network structure, dynamics, and function using NetworkX - Hagberg, Schult, Swart - 2008.pdf:pdf},
isbn = {3333333333},
issn = {1540-9295},
journal = {Proceedings of the 7th Python in Science Conference (SciPy 2008)},
number = {SciPy},
pages = {11--15},
title = {{Exploring network structure, dynamics, and function using NetworkX}},
year = {2008}
}
@book{Koronacki2008,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
author = {Koronacki, Jacek and Ra{\'{s}}, Zbigniew W. and Wierzcho{\'{n}}, S{\l}awomir T. and Kacprzyk, Janusz},
booktitle = {Igarss 2014},
doi = {10.1007/978-3-540-79452-3},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Machine Learning - Koronacki et al. - 2008.pdf:pdf},
isbn = {978-3-540-79451-6},
issn = {1995-6819},
keywords = {high resolution images,research,risks management,sustainable reconstruction},
number = {1},
pages = {1--5},
title = {{Machine Learning}},
url = {http://link.springer.com/10.1007/978-3-642-05177-7{\%}5Cnhttp://link.springer.com/10.1007/978-3-540-79452-3},
volume = {262},
year = {2008}
}
@article{Kudo2004,
abstract = {This paper presents an application of Boosting for classifying labeled graphs, general structures for modeling a number of real-world data, such as chemical compounds, natural language texts, and bio sequences. The proposal consists of i decision stumps that use subgraph as features, and ii a Boosting algorithm in which subgraph-based decision stumps are used as weak learners. We also discuss the relation between our algorithm and SVMs with convolution kernels. Two experiments using natural language data and chemical compounds show that our method achieves comparable or even better performance than SVMs with convolution kernels as well as improves the testing efficiency.},
author = {Kudo, Taku and Maeda, E and Matsumoto, Y},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/An application of boosting to graph classification - Kudo, Maeda, Matsumoto - 2004.pdf:pdf},
issn = {0036-8075},
journal = {Proceedings of the 18th Annual Conference on Neural Information Processing Systems (NIPS 2004)},
pages = {729--736},
title = {{An application of boosting to graph classification}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}369.pdf},
year = {2004}
}
@article{Neumann2012,
author = {Neumann, Marion and Patricia, Novi and Garnett, Roman and Kersting, Kristian},
doi = {10.1007/978-3-642-33460-3_30},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Efficient graph kernels by randomization - Neumann et al. - 2012.pdf:pdf},
isbn = {9783642334597},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {378--393},
title = {{Efficient graph kernels by randomization}},
volume = {7523 LNAI},
year = {2012}
}
@article{Morris2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.00064v1},
author = {Morris, Christopher and Kriege, Nils M. and Kersting, Kristian and Mutzel, Petra},
doi = {10.1109/ICDM.2016.114},
eprint = {arXiv:1610.00064v1},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Faster kernels for graphs with continuous attributes via hashing - Morris et al. - 2017.pdf:pdf},
isbn = {9781509054725},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
pages = {1095--1100},
title = {{Faster kernels for graphs with continuous attributes via hashing}},
year = {2017}
}
@article{Weikum2002,
abstract = {"Statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. Anyone who wants to learn this field would be well advised to get this book. For that matter, the same goes for anyone who is already in the field. I know that it is going to be one of the most well-thumbed books on my bookshelf." - Eugene Charniak, Department of Computer Science, Brown University Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications. More on this book},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Weikum, Gerhard},
doi = {10.1145/601858.601867},
eprint = {arXiv:1011.1669v3},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Foundations of statistical natural language processing - Weikum - 2002.pdf:pdf},
isbn = {0262133601},
issn = {01635808},
journal = {ACM SIGMOD Record},
number = {3},
pages = {37},
pmid = {20955506},
title = {{Foundations of statistical natural language processing}},
url = {http://portal.acm.org/citation.cfm?doid=601858.601867},
volume = {31},
year = {2002}
}
@article{Rupp2007,
abstract = {Similarity measures for molecules are of basic importance in chemical, biological, and pharmaceutical applications. We introduce a molecular similarity measure defined directly on the annotated molecular graph, based on iterative graph similarity and optimal assignments. We give an iterative algorithm for the computation of the proposed molecular similarity measure, prove its convergence and the uniqueness of the solution, and provide an upper bound on the required number of iterations necessary to achieve a desired precision. Empirical evidence for the positive semidefiniteness of certain parametrizations of our function is presented. We evaluated our molecular similarity measure by using it as a kernel in support vector machine classification and regression applied to several pharmaceutical and toxicological data sets, with encouraging results.},
author = {Rupp, Matthias and Proschak, Ewgenij and Schneider, Gisbert},
doi = {10.1021/ci700274r},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Kernel approach to molecular similarity based on iterative graph similarity - Rupp, Proschak, Schneider - 2007.pdf:pdf},
issn = {15499596},
journal = {Journal of Chemical Information and Modeling},
number = {6},
pages = {2280--2286},
pmid = {17985866},
title = {{Kernel approach to molecular similarity based on iterative graph similarity}},
volume = {47},
year = {2007}
}
@article{Pang2004,
abstract = {Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as "thumbs up" or "thumbs down". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.},
archivePrefix = {arXiv},
arxivId = {cs/0409058},
author = {Pang, Bo and Lee, Lillian},
doi = {10.3115/1218955.1218990},
eprint = {0409058},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A Sentimental Education Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts - Pang, Lee - 2004.pdf:pdf},
issn = {1554-0669},
primaryClass = {cs},
title = {{A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts}},
url = {http://arxiv.org/abs/cs/0409058},
year = {2004}
}
@article{VanderWalt2011,
abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
author = {van der Walt, St{\'{e}}fan and Colbert, S Chris and Varoquaux, Ga{\"{e}}l},
doi = {10.1109/MCSE.2011.37},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/The NumPy Array A Struture for Efficient Numerical Computation - van der Walt, Colbert, Varoquaux - 2011.pdf:pdf},
issn = {1521-9615},
journal = {Computing in Science {\{}{\&}{\}} Engeneering},
pages = {22--30},
pmid = {1000224770},
title = {{The NumPy Array: A Struture for Efficient Numerical Computation}},
volume = {13},
year = {2011}
}
@article{Smola2002,
author = {Smola, Alexander J and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Learning with Kernels - Smola, Sch{\"{o}}lkopf - 2002.pdf:pdf},
isbn = {978-0-262-19475-4},
number = {February},
title = {{Learning with Kernels}},
year = {2002}
}
@article{Neuhaus2006a,
author = {Neuhaus, Michel and Bunke, Horst},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A Random Walk Kernel Derived from Graph Edit Distance - Neuhaus, Bunke - 2006.pdf:pdf},
isbn = {3-540-37236-9},
issn = {16113349},
journal = {Proc. 11th Int. Workshop on Structural and Syntactic Pattern Recognition,},
number = {Im},
pages = {91--199.},
title = {{A Random Walk Kernel Derived from Graph Edit Distance}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.90.1546},
year = {2006}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Distributed Representations of Words and Phrases and their Compositionality - Mikolov et al. - 2013.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://arxiv.org/abs/1310.4546},
year = {2013}
}
@article{Nikolentzos2017,
abstract = {Graph kernels have emerged as a powerful tool for graph comparison. Most existing graph kernels focus on local properties of graphs and ignore global structure. In this paper, we compare graphs based on their global properties as these are captured by the eigenvectors of their adjacency matrices. We present two algorithms for both labeled and unlabeled graph comparison. These algorithms represent each graph as a set of vectors corresponding to the embeddings of its vertices. The similarity between two graphs is then determined using the Earth Mover's Distance metric. These similarities do not yield a positive semidefinite matrix. To address for this, we employ an algorithm for SVM classification using indefinite kernels. We also present a graph kernel based on the Pyramid Match kernel that finds an approximate correspondence between the sets of vectors of the two graphs. We further improve the proposed kernel using the Weisfeiler-Lehman framework. We evaluate the proposed methods on several benchmark datasets for graph classification and compare their performance to state-of-the-art graph kernels. In most cases, the proposed algorithms outperform the competing methods, while their time complexity remains very attractive.},
author = {Nikolentzos, Giannis},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Matching Node Embeddings for Graph Similarity - Nikolentzos - 2017.pdf:pdf},
journal = {Proceedings of the 31th Conference on Artificial Intelligence (AAAI 2017)},
keywords = {Machine Learning Methods},
pages = {2429--2435},
title = {{Matching Node Embeddings for Graph Similarity}},
year = {2017}
}
@article{Wu,
author = {Wu, Gang and Chang, Edward Y},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Formulating Distance Functions via the Kernel Trick - Wu, Chang - Unknown.pdf:pdf},
isbn = {159593135X},
keywords = {distance function,kernel trick},
number = {1},
pages = {703--709},
title = {{Formulating Distance Functions via the Kernel Trick}}
}
@article{DeVries2013,
abstract = {In this paper we introduce an approximation of the Weisfeiler-Lehman graph kernel algorithm aimed at improving the computation time of the kernel when applied to Resource Description Framework (RDF) data. Typically, applying graph kernels to RDF is done by ex-tracting subgraphs from a large RDF graph and computing the kernel on this set of subgraphs. In contrast, our algorithm computes the Weisfeiler-Lehman kernel directly on the large RDF graph, but still retains the sub-graph information. We show that this algorithm is faster than the regular Weisfeiler-Lehman kernel for RDF data and has at least the same per-formance. Furthermore, we show that our method has similar or better performance, and is faster, than other recently introduced graph kernels for RDF.},
author = {{De Vries}, Gerben K D},
doi = {10.1007/978-3-642-40988-2_39},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A fast approximation of the Weisfeiler-Lehman graph kernel for RDF data - De Vries - 2013.pdf:pdf},
isbn = {9783642409875},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Graph Kernels,Resource Description Framework (RDF),Weisfeiler-Lehman},
number = {PART 1},
pages = {606--621},
title = {{A fast approximation of the Weisfeiler-Lehman graph kernel for RDF data}},
volume = {8188 LNAI},
year = {2013}
}
@article{Rousseau2015,
abstract = {In this paper, we consider the task of text categorization as a graph classification problem. By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part - its main core - speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classification datasets show statistically significant higher accuracy and macro-Averaged F1-score compared to baseline approaches. {\textcopyright} 2015 Association for Computational Linguistics.},
author = {Rousseau, Francois and Kiagias, Emmanouil and Vazirgiannis, Michalis},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Text Categorization as a Graph Classification Problem - Rousseau, Kiagias, Vazirgiannis - 2015.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
pages = {1702--1712},
title = {{Text Categorization as a Graph Classification Problem}},
url = {http://www.aclweb.org/anthology/P15-1164},
year = {2015}
}
@article{Vitale2012,
abstract = {We propose a novel approach to the classification of short texts based on two factors: the use of Wikipedia-based annotators that have been recently introduced to detect the main topics present in an input text, represented via Wikipedia pages, and the design of a novel classification algorithm that measures the similarity between the input text and each output category by deploying only their annotated topics and the Wikipedia link-structure. Our approach waives the common practice of expanding the feature-space with new dimensions derived either from explicit or from latent semantic analysis. As a consequence it is simple and maintains a compact intelligible representation of the output categories. Our experiments show that it is efficient in construction and query time, accurate as state-of-the-art classifiers (see e.g. Phan et al. WWW '08), and robust with respect to concept drifts and input sources.},
author = {Vitale, Daniele and Ferragina, Paolo and Scaiella, Ugo},
doi = {10.1007/978-3-642-28997-2_32},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Classification of short texts by deploying topical annotations - Vitale, Ferragina, Scaiella - 2012.pdf:pdf},
isbn = {9783642289965},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {376--387},
title = {{Classification of short texts by deploying topical annotations}},
volume = {7224 LNCS},
year = {2012}
}
@article{Goldberg2014,
abstract = {The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in "Distributed Representations of Words and Phrases and their Compositionality" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.},
archivePrefix = {arXiv},
arxivId = {1402.3722},
author = {Goldberg, Yoav and Levy, Omer},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1402.3722},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/word2vec Explained deriving Mikolov et al.'s negative-sampling word-embedding method - Goldberg, Levy - 2014.pdf:pdf},
isbn = {2150-8097},
issn = {0003-6951},
number = {2},
pages = {1--5},
pmid = {903},
title = {{word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method}},
url = {http://arxiv.org/abs/1402.3722},
year = {2014}
}
@book{Prasad2007,
author = {Prasad, Deba and Shubhra, Mandal and Ray, Sankar and Zhang, David and Pal, Sankar K and Conference, International and Hutchison, David},
doi = {10.1007/978-3-540-77046-6},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Pattern Recognition and Machine Intelligence - Prasad et al. - 2007.pdf:pdf},
isbn = {978-3-540-77045-9},
title = {{Pattern Recognition and Machine Intelligence}},
url = {http://link.springer.com/10.1007/978-3-540-77046-6},
volume = {4815},
year = {2007}
}
@article{Gartner2003,
abstract = {As most 'real-world' data is structured, research in kernel methods has begun investigating kernels for various kinds of structured data. One of the most widely used tools for modeling structured data are graphs. An interesting and important challenge is thus to investigate kernels on instances that are represented by graphs. So far, only very specific graphs such as trees and strings have been considered. This paper investigates kernels on labeled directed graphs with general structure. It is shown that computing a strictly positive definite graph kernel is at least as hard as solving the graph isomorphism problem. It is also shown that computing an inner product in a feature space indexed by all possible graphs, where each feature counts the number of subgraphs isomorphic to that graph, is {\{}NP-hard.{\}} On the other hand, inner products in an alternative feature space, based on walks in the graph, can be computed in polynomial time. Such kernels are defined in this paper.},
author = {G{\"{a}}rtner, Thomas and Flach, Peter and Wrobel, Stefan and G{\"{a}}rtner, Thomas},
doi = {10.1007/b12006},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/On Graph Kernels Hardness Results and Efficient Alternatives - G{\"{a}}rtner et al. - 2003.pdf:pdf},
isbn = {978-3-540-40720-1},
issn = {0302-9743},
journal = {Proceedings of the 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop},
keywords = {graph,kernels},
pages = {129--143},
title = {{On Graph Kernels: Hardness Results and Efficient Alternatives}},
url = {http://link.springer.com/10.1007/b12006},
year = {2003}
}
@article{Cao2016,
abstract = {In this paper, we propose a novel model for learning graph representations, which generates a low-dimensional vector representation for each vertex by capturing the graph struc- tural information. Different from other previous research ef- forts, we adopt a random surfing model to capture graph structural information directly, instead of using the sampling- based method for generating linear sequences proposed by Perozzi et al. (2014). The advantages of our approach will be illustrated from both theorical and empirical perspectives. We also give a new perspective for the matrix factorization method proposed by Levy and Goldberg (2014), in which the pointwise mutual information (PMI) matrix is considered as an analytical solution to the objective function of the skip- gram model with negative sampling proposed by Mikolov et al. (2013). Unlike their approach which involves the use of the SVD for finding the low-dimensitonal projections from the PMI matrix, however, the stacked denoising autoencoder is introduced in our model to extract complex features and model non-linearities. To demonstrate the effectiveness of our model, we conduct experiments on clustering and visualiza- tion tasks, employing the learned vertex representations as features. Empirical results on datasets of varying sizes show that our model outperforms other stat-of-the-art models in such tasks. 1},
author = {Cao, Shaosheng and Lu, Wei and Xu, Qiongkai},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Deep Neural Networks for Learning Graph Representations - Cao, Lu, Xu - 2016.pdf:pdf},
journal = {Aaai},
keywords = {Technical Papers: Machine Learning Applications},
pages = {1145--1152},
title = {{Deep Neural Networks for Learning Graph Representations}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12423/11715},
year = {2016}
}
@article{Cortes1995,
abstract = {Oil/water partition coefficient (log P) is one of the key points for lead compound to be drug. In silico log P models based solely on chemical structures have become an important part of modern drug discovery. Here, we report support vector machines, radial basis function neural networks, and multiple linear regression methods to investigate the correlation between partition coefficient and physico-chemical descriptors for a large data set of compounds. The correlation coefficient r (2) between experimental and predicted log P for training and test sets by support vector machines, radial basis function neural networks, and multiple linear regression is 0.92, 0.90, and 0.88, respectively. The results show that non-linear support vector machines derives statistical models that have better prediction ability than those of radial basis function neural networks and multiple linear regression methods. This indicates that support vector machines can be used as an alternative modeling tool for quantitative structure-property/activity relationships studies.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1023/A:1022627411411},
eprint = {arXiv:1011.1669v3},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Support-Vector Networks - Cortes, Vapnik - 1995.pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
pmid = {19549084},
title = {{Support-Vector Networks}},
volume = {20},
year = {1995}
}
@article{Da2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06589v1},
author = {Da, Giovanni and Martino, San},
doi = {10.1007/978-3-319-46687-3},
eprint = {arXiv:1509.06589v1},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph Kernels Exploiting Weisfeiler-Lehman Graph Isomorphism Test Extensions - Da, Martino - 2016.pdf:pdf},
isbn = {978-3-319-46686-6},
issn = {0302-9743},
number = {September},
title = {{Graph Kernels Exploiting Weisfeiler-Lehman Graph Isomorphism Test Extensions}},
url = {http://link.springer.com/10.1007/978-3-319-46687-3},
volume = {9947},
year = {2016}
}
@article{Androutsopoulos2000,
abstract = {It has recently been argued that a Naive Bayesian classifier can be used to filter unsolicited bulk e-mail ("spam"). We conduct a thorough evaluation of this proposal on a corpus that we make publicly available, contributing towards standard benchmarks. At the same time we investigate the effect of attribute-set size, training-corpus size, lemmatization, and stop-lists on the filter's performance, issues that had not been previously explored. After introducing appropriate cost-sensitive evaluation measures, we reach the conclusion that additional safety nets are needed for the Naive Bayesian anti-spam filter to be viable in practice.},
archivePrefix = {arXiv},
arxivId = {cs/0006013},
author = {Androutsopoulos, Ion and Koutsias, John and Chandrinos, Konstantinos V. and Paliouras, George and Spyropoulos, Constantine D.},
doi = {10.1109/IAW.2007.381951},
eprint = {0006013},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/An Evaluation of Naive Bayesian Anti-Spam Filtering - Androutsopoulos et al. - 2000.pdf:pdf},
isbn = {1424413036},
issn = {01635840},
journal = {European Conference on Machine Learning},
pages = {9--17},
primaryClass = {cs},
title = {{An Evaluation of Naive Bayesian Anti-Spam Filtering}},
url = {http://arxiv.org/abs/cs/0006013},
year = {2000}
}
@article{Wo2006,
author = {Wo, Adam and Kalousis, Alexandros and Hilario, Melanie},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Matching Based Kernels for Labeled Graphs - Wo, Kalousis, Hilario - 2006.pdf:pdf},
pages = {4--7},
title = {{Matching Based Kernels for Labeled Graphs}},
year = {2006}
}
@article{Jain2010,
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Jain, Anil K.},
doi = {10.1016/j.patrec.2009.09.011},
eprint = {0402594v3},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Data clustering 50 years beyond K-means - Jain - 2010.pdf:pdf},
isbn = {9781424417360},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
number = {8},
pages = {651--666},
primaryClass = {arXiv:cond-mat},
publisher = {Elsevier B.V.},
title = {{Data clustering: 50 years beyond K-means}},
url = {http://dx.doi.org/10.1016/j.patrec.2009.09.011},
volume = {31},
year = {2010}
}
@article{Kriege2014,
author = {Kriege, Nils and Neumann, Marion and Kersting, Kristian and Mutzel, Petra},
doi = {10.1109/ICDM.2014.129},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Explicit versus Implicit Graph Feature Maps A Computational Phase Transition for Walk Kernels - Kriege et al. - 2014.pdf:pdf},
title = {{Explicit versus Implicit Graph Feature Maps : A Computational Phase Transition for Walk Kernels}},
year = {2014}
}
@article{Valerio2007,
abstract = {. (2007). Automatically associating documents with concept map knowledge models. In XXXIII Conferencia Latinoamericana de Inform{\'{a}}tica (CLEI 2007), San Jos{\'{e}}, Costa Rica.},
author = {Valerio, A and Leake, D and Canas, A.},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Automatically associating documents with concept map knowledge models - Valerio, Leake, Canas - 2007.pdf:pdf},
journal = {In XXXIII Conferencia Latinoamericana de Inform{\'{a}}tica (CLEI 2007)},
title = {{Automatically associating documents with concept map knowledge models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.1489{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@article{Kipf2016,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, Thomas N. and Welling, Max},
doi = {10.1051/0004-6361/201527329},
eprint = {1609.02907},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Semi-Supervised Classification with Graph Convolutional Networks - Kipf, Welling - 2016.pdf:pdf},
isbn = {9781611970685},
issn = {0004-6361},
pages = {1--14},
pmid = {23459267},
title = {{Semi-Supervised Classification with Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1609.02907},
year = {2016}
}
@article{OClery2013,
abstract = {Using the intrinsic relationship between the external equitable partition (EEP) and the spectral properties of the graph Laplacian, we characterize convergence and observability properties of consensus dynamics on networks. In particular, we establish the relationship between the original consensus dynamics and the associated consensus of the quotient graph under varied initial conditions, and characterize the asymptotic convergence to the synchronization manifold under nonuniform input signals. We also show that the EEP with respect to a node can reveal nodes in the graph with an increased rate of asymptotic convergence to the consensus value, as characterized by the second smallest eigenvalue of the quotient Laplacian. Finally, we show that the quotient graph preserves the observability properties of the full graph and how the inheritance by the quotient graph of particular aspects of the eigenstructure of the full Laplacian underpins the observability and convergence properties of the system.},
author = {O'Clery, Neave and Yuan, Ye and Stan, Guy Bart and Barahona, Mauricio},
doi = {10.1103/PhysRevE.88.042805},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Observability and coarse graining of consensus dynamics through the external equitable partition - O'Clery et al. - 2013.pdf:pdf},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {4},
pages = {1--13},
title = {{Observability and coarse graining of consensus dynamics through the external equitable partition}},
volume = {88},
year = {2013}
}
@article{KashimaHisashi;TsundaKoji;Inokuchi2003,
abstract = {A new kernel function between two labeled graphs is presented. Feature vectors are de-fined as the counts of label paths produced by random walks on graphs. The kernel com-putation finally boils down to obtaining the stationary state of a discrete-time linear sys-tem, thus is efficiently performed by solv-ing simultaneous linear equations. Our ker-nel is based on an infinite dimensional fea-ture space, so it is fundamentally different from other string or tree kernels based on dy-namic programming. We will present promis-ing empirical results in classification of chem-ical compounds.},
author = {{Kashima, Hisashi; Tsunda, Koji; Inokuchi}, Akihiro},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Marginalized Kernels Between Labeled Graphs - Kashima, Hisashi Tsunda, Koji Inokuchi - 2003.pdf:pdf},
isbn = {1-57735-189-4},
journal = {Icml},
number = {2002},
pages = {321--328},
title = {{Marginalized Kernels Between Labeled Graphs}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-044.pdf},
year = {2003}
}
@book{Bondy1976,
author = {Bondy, J.A. A and Murty, U.S.R. S R},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph theory with applications - Bondy, Murty - 1976.pdf:pdf},
isbn = {0444194517},
title = {{Graph theory with applications}},
url = {http://people.math.jussieu.fr/{~}jabondy/books/gtwa/pdf/preface.pdf},
volume = {290},
year = {1976}
}
@article{Mahe2009a,
abstract = {Motivated by chemical applications, we revisit and extend a family of positive definite kernels for graphs based on the detection of common subtrees, initially proposed by Ramon and G{\"{a}}rtner (Proceedings of the first international workshop on mining graphs, trees and sequences, pp. 65–74, 2003). We propose new kernels with a parameter to control the complexity of the subtrees used as features to represent the graphs. This parameter allows to smoothly interpolate between classical graph kernels based on the count of common walks, on the one hand, and kernels that emphasize the detection of large common subtrees, on the other hand. We also propose two modular extensions to this formulation. The first extension increases the number of subtrees that define the feature space, and the second one removes noisy features from the graph representations. We validate experimentally these new kernels on problems of toxicity and anti-cancer activity prediction for small molecules with support vector machines.},
archivePrefix = {arXiv},
arxivId = {arXiv:q-bio/0609024v1},
author = {Mah{\'{e}}, Pierre and Vert, Jean Philippe},
doi = {10.1007/s10994-008-5086-2},
eprint = {0609024v1},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph kernels based on tree patterns for molecules - Mah{\'{e}}, Vert - 2009.pdf:pdf},
isbn = {978-3-642-04413-7},
issn = {08856125},
journal = {Machine Learning},
keywords = {Chemoinformatics,Graph kernels,Support vector machines},
number = {1},
pages = {3--35},
primaryClass = {arXiv:q-bio},
title = {{Graph kernels based on tree patterns for molecules}},
volume = {75},
year = {2009}
}
@article{Jiang2010,
abstract = {A graph-based approach to document classification is described in this paper. The graph representation offers the advantage that it allows for a much more expressive document encoding than the more standard bag of words/phrases approach, and consequently gives an improved classification accuracy. Document sets are represented as graph sets to which a weighted graph mining algorithm is applied to extract frequent subgraphs, which are then further processed to produce feature vectors (one per document) for classification. Weighted subgraph mining is used to ensure classification effectiveness and computational efficiency; only the most significant subgraphs are extracted. The approach is validated and evaluated using several popular classification algorithms together with a real world textual data set. The results demonstrate that the approach can outperform existing text classification algorithms on some dataset. When the size of dataset increased, further processing on extracted frequent features is essential. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Jiang, Chuntao and Coenen, Frans and Sanderson, Robert and Zito, Michele},
doi = {10.1016/j.knosys.2009.11.010},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Text classification using graph mining-based feature extraction - Jiang et al. - 2010.pdf:pdf},
isbn = {978-1-84882-982-4},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Feature extraction,Graph mining,Graph representation,Text classification,Weighted graph mining},
number = {4},
pages = {302--308},
title = {{Text classification using graph mining-based feature extraction}},
volume = {23},
year = {2010}
}
@article{Vitale2012a,
abstract = {We propose a novel approach to the classification of short texts based on two factors: the use of Wikipedia-based annotators that have been recently introduced to detect the main topics present in an input text, represented via Wikipedia pages, and the design of a novel classification algorithm that measures the similarity between the input text and each output category by deploying only their annotated topics and the Wikipedia link-structure. Our approach waives the common practice of expanding the feature-space with new dimensions derived either from explicit or from latent semantic analysis. As a consequence it is simple and maintains a compact intelligible representation of the output categories. Our experiments show that it is efficient in construction and query time, accurate as state-of-the-art classifiers (see e.g. Phan et al. WWW '08), and robust with respect to concept drifts and input sources.},
author = {Vitale, Daniele and Ferragina, Paolo and Scaiella, Ugo},
doi = {10.1007/978-3-642-28997-2_32},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Classification of short texts by deploying topical annotations - Vitale, Ferragina, Scaiella - 2012(2).pdf:pdf},
isbn = {9783642289965},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {376--387},
title = {{Classification of short texts by deploying topical annotations}},
volume = {7224 LNCS},
year = {2012}
}
@article{Kersting2013,
abstract = {Color refinement is a basic algorithmic routine for graph isomorphism testing and has recently been used for computing graph kernels as well as for lifting belief propagation and linear programming. So far, color refinement has been treated as a combinatorial problem. Instead, we treat it as a nonlinear continuous optimization problem and prove that it implements a conditional gradient optimizer that can be turned into graph clustering approaches using hashing and truncated power iterations. This shows that color refinement is easy to understand in terms of random walks, easy to implement (matrix-matrix/vector multiplications) and readily parallelizable. We support our theoretical results with experiments on real-world graphs with millions of edges. Copyright {\textcopyright} 2014, Association for the Advancement of Artificial Intelligence.},
author = {Kersting, Kristian and Mladenov, Martin and Garnett, Roman and Grohe, Martin},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Power Iterated Color Refinement - Kersting et al. - 2013.pdf:pdf},
isbn = {9781577356790},
journal = {28th AAAI Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms},
pages = {1904--1910},
title = {{Power Iterated Color Refinement}},
year = {2013}
}
@misc{Kulharia,
author = {Kulharia, Viveka and Ghosh, Arnab},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph Kernels (Presentation) - Kulharia, Ghosh - Unknown.pdf:pdf},
title = {{Graph Kernels (Presentation)}}
}
@article{Hamilton2017,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
archivePrefix = {arXiv},
arxivId = {1706.02216},
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
eprint = {1706.02216},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Representation Learning on Graphs Methods and Applications - Hamilton, Ying, Leskovec - 2017.pdf:pdf},
pages = {1--23},
title = {{Representation Learning on Graphs: Methods and Applications}},
url = {http://arxiv.org/abs/1706.02216},
year = {2017}
}
@article{Whitesides2004,
abstract = {Insights into conducting research and the writing of scientific papers are given by Prof. Whitesides in this short essay. The manuscript and its guidelines has been circulated within the Whitesides' research group since 1989.},
archivePrefix = {arXiv},
arxivId = {1003.3921v1},
author = {Whitesides, George M.},
doi = {10.1002/adma.200400767},
eprint = {1003.3921v1},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Whitesides' Group Writing a paper - Whitesides - 2004.pdf:pdf},
isbn = {3825208028},
issn = {09359648},
journal = {Advanced Materials},
number = {15 SPEC. ISS.},
pages = {1375--1377},
pmid = {19782018},
title = {{Whitesides' Group: Writing a paper}},
volume = {16},
year = {2004}
}
@article{Kock2015,
abstract = {Should P values associated with path coefficients, as well as with other coefficients such as weights and loadings, be one-tailed or two-tailed? This question is answered in the context of structural equation modeling employing the partial least squares method (PLS-SEM), based on an illustrative model of the effect of e-collaboration technology use on job performance. A one- tailed test is recommended if the coefficient is assumed to have a sign (positive or negative), which should be reflected in the hypothesis that refers to the corresponding association. If no assumptions are made about coefficient sign, a two-tailed test is recommended. These recommendations apply to many other statistical methods that employ P values; including path analyses in general, with or without latent variables, plus univariate and multivariate regression analyses},
archivePrefix = {arXiv},
arxivId = {http://ehis.ebscohost.com/},
author = {Kock, Ned},
doi = {10.4018/ijec.2015040101},
eprint = {/ehis.ebscohost.com/},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/One-tailed or two-tailed P values in PLS-SEM - Kock - 2015.pdf:pdf},
isbn = {14707853},
issn = {1548-3673},
journal = {International Journal of e-Collaboration},
keywords = {e-collaboration,indicator,latent variable,monte carlo simulation,one-tailed test,partial least squares,structural equation modeling,two-tailed test},
number = {2},
pages = {1--7},
pmid = {27996447},
primaryClass = {http:},
title = {{One-tailed or two-tailed P values in PLS-SEM ?}},
volume = {11},
year = {2015}
}
@article{Page1998,
abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
archivePrefix = {arXiv},
arxivId = {1111.4503v1},
author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
doi = {10.1.1.31.1768},
eprint = {1111.4503v1},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/The PageRank Citation Ranking Bringing Order to the Web - Page et al. - 1998.pdf:pdf},
isbn = {9781424433803},
issn = {1752-0509},
journal = {World Wide Web Internet And Web Information Systems},
number = {1999-66},
pages = {1--17},
pmid = {20840727},
title = {{The PageRank Citation Ranking: Bringing Order to the Web}},
url = {http://ilpubs.stanford.edu:8090/422},
volume = {54},
year = {1998}
}
@inproceedings{Yanardag2015,
abstract = {In this paper, we present Deep Graph Kernels, a unified frame-work to learn latent representations of sub-structures for graphs, inspired by latest advancements in language modeling and deep learning. Our framework leverages the dependency information be-tween sub-structures by learning their latent representations. We demonstrate instances of our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree ker-nels, and Shortest-Path graph kernels. Our experiments on several benchmark datasets show that Deep Graph Kernels achieve signif-icant improvements in classification accuracy over state-of-the-art graph kernels.},
author = {Yanardag, Pinar and Vishwanathan, S.V.N.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
doi = {10.1145/2783258.2783417},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Deep Graph Kernels - Yanardag, Vishwanathan - 2015.pdf:pdf},
isbn = {9781450336642},
issn = {9781450336642},
pages = {1365--1374},
title = {{Deep Graph Kernels}},
url = {http://dl.acm.org/citation.cfm?doid=2783258.2783417},
year = {2015}
}
@article{Meurer2017,
abstract = {{\textless}p{\textgreater}SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provide additional examples and further outline details of the architecture and features of SymPy.{\textless}/p{\textgreater}},
author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and {\v{C}}ert{\'{i}}k, Ondřej and Kirpichev, Sergey B. and Rocklin, Matthew and Kumar, AMiT and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Rou{\v{c}}ka, {\v{S}}t{\v{e}}p{\'{a}}n and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
doi = {10.7717/peerj-cs.103},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/SymPy symbolic computing in Python - Meurer et al. - 2017.pdf:pdf},
issn = {2376-5992},
journal = {PeerJ Computer Science},
pages = {e103},
title = {{SymPy: symbolic computing in Python}},
url = {https://peerj.com/articles/cs-103},
volume = {3},
year = {2017}
}
@article{Jitkrittum2017a,
abstract = {We propose a novel adaptive test of goodness-of-fit, with computational cost linear in the number of samples. We learn the test features that best indicate the differences between observed samples and a reference model, by minimizing the false negative rate. These features are constructed via Stein's method, meaning that it is not necessary to compute the normalising constant of the model. We analyse the asymptotic Bahadur efficiency of the new test, and prove that under a mean-shift alternative, our test always has greater relative efficiency than a previous linear-time kernel test, regardless of the choice of parameters for that test. In experiments, the performance of our method exceeds that of the earlier linear-time test, and matches or exceeds the power of a quadratic-time kernel test. In high dimensions and where model structure may be exploited, our goodness of fit test performs far better than a quadratic-time two-sample test based on the Maximum Mean Discrepancy, with samples drawn from the model.},
archivePrefix = {arXiv},
arxivId = {1705.07673},
author = {Jitkrittum, Wittawat and Xu, Wenkai and Szabo, Zoltan and Fukumizu, Kenji and Gretton, Arthur},
eprint = {1705.07673},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/A Linear-Time Kernel Goodness-of-Fit Test - Jitkrittum et al. - 2017.pdf:pdf},
number = {Nips},
pages = {0--3},
title = {{A Linear-Time Kernel Goodness-of-Fit Test}},
url = {http://arxiv.org/abs/1705.07673},
year = {2017}
}
@article{Read1977,
abstract = {The graph isomorphism problemto devise a good algorithm for determining if two graphs are isomorphicis of considerable practical importance, and is also of theoretical interest due to its relationship to the concept of NP-completeness. No efficient (i.e., polynomial-bound) algorithm for graph isomorphism is known, and it has been conjectured that no such algorithm can exist. Many papers on the subject have appeared, but progress has been slight; in fact, the intractable nature of the problem and the way that many graph theorists have been led to devote much time to it, recall those aspects of the four-color conjecture which prompted Harary to rechristen it the four-color disease. This paper surveys the present state of the art of isomorphism testing, discusses its relationship to NP-completeness, and indicates some of the difficulties inherent in this particularly elusive and challenging problem. A comprehensive bibliography of papers relating to the graph isomorphism problem is given.},
author = {Read, Ronald C. and Corneil, Derek G.},
doi = {10.1002/jgt.3190010410},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/The graph isomorphism disease - Read, Corneil - 1977.pdf:pdf},
isbn = {1097-0118},
issn = {10970118},
journal = {Journal of Graph Theory},
number = {4},
pages = {339--363},
title = {{The graph isomorphism disease}},
volume = {1},
year = {1977}
}
@article{Vishwanathan2010,
abstract = {We present a unified framework to study graph kernels, special cases of which include the random walk (G{\"{a}}rtner et al., 2003; Borgwardt et al., 2005) and marginalized (Kashima et al., 2003, 2004; Mah{\'{e}} et al., 2004) graph kernels. Through reduction to a Sylvester equation we improve the time complexity of kernel computation between unlabeled graphs with n vertices from O(n 6) to O(n 3). We find a spectral decomposition approach even more efficient when computing entire kernel ma-trices. For labeled graphs we develop conjugate gradient and fixed-point methods that take O(dn 3) time per iteration, where d is the size of the label set. By extending the necessary linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) we obtain the same result for d-dimensional edge ker-nels, and O(n 4) in the infinite-dimensional case; on sparse graphs these algorithms only take O(n 2) time per iteration in all cases. Experiments on graphs from bioinformatics and other application domains show that these techniques can speed up computation of the kernel by an order of mag-nitude or more. We also show that certain rational kernels (Cortes et al., 2002, 2003, 2004) when specialized to graphs reduce to our random walk graph kernel. Finally, we relate our framework to R-convolution kernels (Haussler, 1999) and provide a kernel that is close to the optimal assignment kernel of Fr{\"{o}}hlich et al. (2006) yet provably positive semi-definite.},
archivePrefix = {arXiv},
arxivId = {0807.0093},
author = {Vishwanathan, S.V.N. and Schraudolph, Nicol and Kondor, Risi and Borgwardt, K.M.},
doi = {10.1142/9789812772435_0002},
eprint = {0807.0093},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Graph Kernels - Vishwanathan et al. - 2010.pdf:pdf},
isbn = {978-981-270-417-7},
issn = {1793-5091},
journal = {Journal of Machine Learning Research},
keywords = {()},
pages = {1201--1242},
pmid = {17992741},
title = {{Graph Kernels}},
volume = {11},
year = {2010}
}
@article{Shervashidze2009,
abstract = {In this article, we propose fast subtree kernels on graphs. On graphs with n nodes and m edges and maximum degree d, these kernels comparing subtrees of height h can be computed in O(mh), whereas the classic subtree kernel by Ramon {\&} G{\"{a}}rtner scales as O(n 2 4 d h). Key to this efficiency is the observation that the Weisfeiler-Lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct. Our fast subtree kernels can deal with labeled graphs, scale up easily to large graphs and outperform state-of-the-art graph ker-nels on several classification benchmark datasets in terms of accuracy and runtime.},
author = {Shervashidze, Nino and Borgwardt, Karsten M},
file = {:Users/davidgengenbach/Documents/Mendeley Desktop/Fast Subtree Kernels on Graphs - Shervashidze, Borgwardt - 2009.pdf:pdf},
isbn = {978-1-615-67911-9},
journal = {23rd Annual Conference on Neural Information Processing Systems},
pages = {1660--1668},
title = {{Fast Subtree Kernels on Graphs}},
year = {2009}
}
