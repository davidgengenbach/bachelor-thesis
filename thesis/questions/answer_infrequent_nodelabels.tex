In Figure \ref{fig:percentage_distribution_concept_occurrences} we can see that, depending on the dataset, between 75\% to 90\% of all node labels occur only once per dataset.
This is also common in texts where most words also only occur once per dataset.
 
\begin{figure}[htb!]
    \centering{\includegraphics[width=\linewidth]{assets/figures/tmp/percentage_distribution_concept_occurrences.pdf}}
    \caption[Statistics: Distribution concept occurrence]{Concept map node label occurrences per dataset. $|n_v| = i$ stands for the percentage of labels with $i$ occurrences in the dataset, eg. when $|n_v| = 1$ has 50\%, it would mean that 50\% of all unique concepts only occur once per dataset.}\label{fig:percentage_distribution_concept_occurrences}
\end{figure}

In text-based approaches, infrequent words are either ignored or filtered out to reduce the vocabulary and subsequently the dimension of the feature vector
Yet, for our approach, infrequent node labels might pose a far greater problem.
As noted before, in our work, we capitalize on the Weisfeiler-Lehman graph kernel to extract useful features for subsequent classification.
In the context of WL, infrequent node labels might become a problem since a match becomes less likely with fewer occurring words, or a match even becomes impossible as in the case of node labels which occur only once.
When simply creating a feature vector by counting the node labels in each graph, infrequent node labels would not pose a problem.
WL on the other hand relies on exact matches of neighborhoods.
Thus, a node label which only occurs once would ``taint" its neighborhood, effectively making matches in its neighborhoods impossible.

In Table \ref{table:results_infrequent_nodes} we see the results of removing infrequent labels.

\begin{table}[htb!]
    \centering
    \begin{tabular}{lrrr}
\toprule
        &  \multicolumn{2}{c}{F1 macro} &  \\
         &  Plain &  Removed &  $p$-value \\
        \midrule
            ling-spam       & 0.8105 & 0.8064 & 0.6835 \\
            ng20            & *0.4157 & 0.3536 & 0.0002 \\
            nyt\_200         & 0.7601 & 0.7640 & 0.8466 \\
            r8              & 0.6774 & 0.6421 & 0.0720 \\
            review\_polarity & 0.6025 & 0.6066 & 0.8868 \\
            rotten\_imdb     & *0.6373 & 0.5494 & 0.0002 \\
            ted\_talks       & 0.2449 & *0.3687 & 0.0062 \\
        \bottomrule
    \end{tabular}
    \caption[Results: Remove infrequent node labels]{Classification results with infrequent nodes removed.}\label{table:results_infrequent_nodes}
\end{table}

As we can see, removing the node labels results in a lower score for most datasets, except for the \textit{ted\_talks} corpus.

\answersummary{
    TODO    
}