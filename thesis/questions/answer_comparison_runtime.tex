Until now, we only looked at the classification scores of our classification approaches.
Another aspect of great real-world importance is the runtime and memory consumption of these approaches.
In table \ref{table:runtime_and_classifier_size}, we report empirical data on both the runtime\footnote{Machine specification. CPU: \textit{Intel(R) Core(TM) i7-2675QM CPU @ 2.20GHz, 4-Core, RAM: \textit{8GB}}} and final classifier size of the used pipelines for each dataset.
For this data, we use simple approaches for both graphs and texts: for the \textbf{texts} we vectorize the text documents with BoW with Tfidf.
For \textbf{graphs}, we vectorize the graphs with the plain Weisfeiler-Lehman graph kernel with $h = 4$ iterations.
For both the text- and graph approach, we then train a SVM on the whole dataset, then predict the labels for the whole dataset. Afterwards we save the models, ie. the internal coefficients of the SVM, and record the size of the saved model.
We also record the runtime of both approaches.

As we can see, the graph kernel creates higher dimensional feature vectors for the objects than the text-based BoW approach. 
This observation both give possible explanations for both the higher classifier model size and the run-time.
One could circumvent the higher dimensionality of features extracted by WL by applying dimensionality reduction.
Yet, this would also incur an additional compute overhead and add complexity to the approach.
Preliminary tests with truncated SVD \cite{Mathematics2009} and PCA \cite{Jolliffe2002} to reduce the dimensionality of the graph features resulted in out-of-memory exceptions.

Note, that the reported runtime and memory consumption for the graph-based pipeline does not incorporate the creation of the concept maps, only the creation of the feature maps and subsequent training/prediction.
The creation of concept maps from text with the code provided in \cite{Falke2017b} takes well over a day for our datasets.
For instance, the concept map creation for the documents  in one of the middle-sized datasets, \textit{ling-spam} with 1.2 million words in total and less than 2900 documents, took over 25 hours and had a peak memory usage of 83 Gigabyte\footnote{Machine specification. CPU: \textit{2 $\times$ Intel(R) Xeon(R) CPU X5650 \@ 2.67GHz, 6-Core}, RAM: \textit{192GB}}.
The runtime of the concept map creation could most likely be reduced by parallelizing parts of its pipeline.
For example, the extraction of relevant concepts and their relation from a text is independent from the same extraction of another text, so this stage has neither data- nor functional dependencies.

\begin{table}[htb!]
    \centering
    \begin{tabular}{lrr|rr|rr|rr}
\toprule
        & \multicolumn{2}{c|}{Classifier Size} &  \multicolumn{2}{c|}{Runtime} &  \multicolumn{2}{c|}{Feature Runtime} &  \multicolumn{2}{c}{\# Features} \\
        \midrule
        &  Graph &  Text &  Graph &  Text & Graph &  Text  & Graph &  Text \\
        \midrule
ling-spam       & 11 & 4 & 5 & 2 & 3 & 2 & 246 & 61 \\
ng20            & 143 & 29 & 115 & 10 & 15 & 5 & 750 & 134 \\
nyt\_200         & 90 & 9 & 59 & 8 & 7 & 6 & 1061 & 88 \\
r8              & 28 & 3 & 19 & 2 & 6 & 1 & 282 & 25 \\
review\_polarity & 17 & 3 & 7 & 2 & 3 & 2 & 369 & 40 \\
rotten\_imdb     & 4 & 1 & 5 & 0 & 4 & 0 & 80 & 21 \\
ted\_talks       & 17 & 3 & 2 & 2 & 1 & 2 & 254 & 34 \\
        \bottomrule
    \end{tabular}
\caption[Table: Runtime, classifier size and \# features for graph- and text based classification.]{
    Runtime, classifier size and number of features of text- and graph based classification.
    The feature runtime corresponds to the feature extraction runtime, both for WL for graphs and Tfidf for texts.
    The runtime is reported in \textit{seconds}, classifier size in \textit{Megabytes}, \textit{\# Features} in thousands.
}
\label{table:runtime_and_classifier_size}
\end{table}

\answersummary{
    Graph-based classification using WL incurs both a runtime and memory consumption overhead to text-based classification.
    One explanation for both the higher memory consumption and runtime is the higher dimensionality of the feature vectors created by WL.
    Applying dimensionality reduction might be a solution to circumvent these issues, yet it has to be noted that the feature vectors are very sparse and are most likely not normalized, so \textit{PCA} is not an option.
    The creation of concept maps from text also incurs a high runtime and memory consumption overhead.
}