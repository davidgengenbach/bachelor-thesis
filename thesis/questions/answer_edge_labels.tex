To evaluate how important the edge labels are, we first look at the occurrences of edge labels, ie. how often a unique edge label occurs in the concept maps of a whole dataset.
In Table \ref{table:edge_label_occurrences} we see that, on average, 76\% of all unique edge labels occur only once per dataset. For comparison, the percentage of unique words which only occur once in a text, on average, is often well below 50\% for our datasets.

\begin{table}[htb!]
    \centering
    \begin{tabular}{lrr}
\toprule
        dataset &  $ \%_{unique} $ & $ \%_{all}$  \\
        \midrule
        ling-spam       & 69 & 27 \\
        ng20            & 73 & 30 \\
        nyt\_middle      & 73 & 32 \\
        nyt\_small       & 78 & 43 \\
        review\_polarity & 80 & 39 \\
        rotten\_imdb     & 80 & 47 \\
        tagmynews       & 75 & 39 \\
        ted\_talks       & 80 & 39 \\
        \midrule
        \O           & 76 & 37 \\
        \bottomrule
    \end{tabular}
    \caption[Statistics: Percentage of concept map labels occurring once]{Percentage of concept map edge labels occurring only once in the whole dataset.
        $ \%_{unique} $ corresponds to the percentage of edge labels which only occur once to the number of unique labels in the whole dataset, eg. $ \%_{unique} = 50\% $ would mean that 50\% of all unique edge labels occur only once.
        $ \%_{all}$ stands for the percentage of edge labels which occur only once to all labels (with duplicates), eg. $ \%_{all} = 50\%$ would mean that 50\% of all edge labels occur only once in the dataset.}\label{table:edge_label_occurrences}
\end{table}

When examining the most occurring edge labels per dataset, most of these most-frequent edge labels consist of stopwords or non-descriptive words like ``is", ``has", ``are".
These most-frequent edge labels, together with the edge labels which occur only once, form the bulk of all edge labels.

As a test for the importance of edge labels, we evaluate \textbf{(a)} a graph kernel which uses the edge labels against \textbf{(b)} a graph kernel which does not use edge labels.
For both, \textbf{(a)} and \textbf{(b)}, we use a graph kernel which first converts the graphs into a text and then vectorizes the text with BoW, introduced in Question \ref{question:importance_structure}.
For \textbf{(a)} we use the edge labels for the text, for \textbf{(b)} we omit them.

\begin{table}[htb!]
    \centering
\begin{tabular}{lrrr}
\toprule
    {} & \multicolumn{2}{c}{Edge Labels}  & p-value \\
    {}  & without & with & {} \\
    \midrule
        ling-spam       & \textbf{0.898} & 0.896 & 0.830 \\
        ng20            & 0.620 & \textbf{0.624} & 0.502 \\
        nyt\_200         & \textbf{*0.900} & 0.875 & 0.031 \\
        r8              & 0.882 & \textbf{0.883} & 0.959 \\
        review\_polarity & 0.700 & \textbf{0.727} & 0.271 \\
        rotten\_imdb     & 0.817 & \textbf{0.823} & 0.533 \\
        ted\_talks       & \textbf{0.410} & 0.363 & 0.164 \\
    \bottomrule
\end{tabular}
\caption[Results: Graph Kernel with and without edge labels]{Classification results concept maps using a graph kernel with and without edge labels.  The star * signifies results where the model is significantly better under the permutation test ($p = 0.05$)}\label{table:edge_label_classification}
\end{table}

As we can see in Table \ref{table:edge_label_classification}, omitting or using the edge labels results in non-significant differences in classification performance.
For some datasets, omitting the edge label actually even improves the classification score.
These facts all indicate that edge labels, while crucial and useful for text-summarization and the subsequent use by a human, seem to have limited use in graph classification.

\answersummary{
        The bulk of all edge labels either occurs only once per dataset or consists of stopwords.
        In our experiments, omitting or using the edge labels actually leads to comparable performances.
}