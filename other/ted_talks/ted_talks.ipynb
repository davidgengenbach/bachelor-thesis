{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED talk transcripts\n",
    "\n",
    "From this [Kaggle Competition](https://www.kaggle.com/rounakbanik/ted-talks/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mechanicalsoup\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from glob import glob\n",
    "import scipy\n",
    "import collections\n",
    "import helper\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "sns.set('notebook', style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 5)\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "\n",
    "HTML_FOLDER = 'data/html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/transcripts.csv')\n",
    "def get_word_count(t):\n",
    "    return len(t.split(' '))\n",
    "df['word_count'] = df.transcript.apply(get_word_count)\n",
    "df['url'] = df.url.str.strip()\n",
    "df.url.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve tags and html for the transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_data(folder = HTML_FOLDER):\n",
    "    html_data = collections.defaultdict(lambda: [])\n",
    "    for file in glob('{}/*.html'.format(folder)):\n",
    "        with open(file) as f:\n",
    "            html = f.read()\n",
    "        url, html = [x.strip() for x in html.split('\\n\\n', 1)]\n",
    "        html_data['url'].append(url)\n",
    "        html_data['html'].append(html)\n",
    "    df_html = pd.DataFrame(html_data)\n",
    "    df_html.url.drop_duplicates(inplace=True)\n",
    "    return df_html\n",
    "\n",
    "df_html = get_html_data()\n",
    "\n",
    "df_ = df.merge(right = df_html, on = 'url', validate = 'one_to_one')\n",
    "df_['tags'] = df_.html.apply(helper.get_tags_from_html)\n",
    "df_['num_tags'] = df_.tags.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df.word_count.plot(kind = 'hist', bins = 120, ax = ax, title = 'Histogram of #words per document')\n",
    "ax.axvline(df.word_count.median(), c = 'red');\n",
    "ax.set_xlabel('Word count per document')\n",
    "ax.grid('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_.num_tags.plot(kind = 'hist', bins = 70, title = 'Histogram of # tags per document', ax = ax)\n",
    "ax.grid('off')\n",
    "ax.set_xlabel('# tags of document')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tags_as_list(all_tags):\n",
    "    all_tags_list = []\n",
    "    for tags in all_tags:\n",
    "        all_tags_list += list(tags)\n",
    "    return all_tags_list\n",
    "\n",
    "def get_tags_as_set(all_tags):\n",
    "    return [set(x) for x in all_tags]\n",
    "\n",
    "def get_tag_mappings(tags):\n",
    "    tag_2_idx = {tag: idx for idx, tag in enumerate(sorted(tags))}\n",
    "    idx_2_tag = {idx: tag for tag, idx in tag_2_idx.items()}\n",
    "    return tag_2_idx, idx_2_tag\n",
    "\n",
    "def get_correlation_matrix(all_tags, tag_2_idx, symmetric = True):\n",
    "    num_unique_tags = len(tag_2_idx.keys())\n",
    "    cooccurrence_mat = scipy.sparse.lil_matrix((num_unique_tags, num_unique_tags))\n",
    "    for tags in all_tags:\n",
    "        for i, tag in enumerate(tags[:-1]):\n",
    "            for j, tag2 in enumerate(tags[i+1:]):\n",
    "                cooccurrence_mat[tag_2_idx[tag], tag_2_idx[tag2]] += 1\n",
    "    cooccurrence_mat = cooccurrence_mat.todense()\n",
    "    if symmetric:\n",
    "        cooccurrence_mat = np.maximum(cooccurrence_mat, cooccurrence_mat.T)\n",
    "    return cooccurrence_mat\n",
    "\n",
    "def get_number_of_occurrences(tag, all_tags_list):\n",
    "    number_of_occurrences = collections.Counter(all_tags_list)\n",
    "    assert tag in number_of_occurrences, 'Tag \"{}\" not in all_tags_list'.format(tag)\n",
    "    return number_of_occurrences[tag]\n",
    "\n",
    "def merge_with_most_correlated(tag, cooccurrence_mat, tag_2_idx, idx_2_tag, correlation_treshold , all_tags_list):\n",
    "    idx = tag_2_idx[tag]\n",
    "    max_correlated_idx = np.argmax(cooccurrence_mat[idx])\n",
    "    val = cooccurrence_mat[idx, max_correlated_idx]\n",
    "    if val < correlation_treshold:\n",
    "        return None\n",
    "    target_tag = idx_2_tag[max_correlated_idx]\n",
    "    tag_1_occ = get_number_of_occurrences(tag, all_tags_list)\n",
    "    tag_2_occ = get_number_of_occurrences(target_tag, all_tags_list)\n",
    "    \n",
    "    if tag_1_occ > tag_2_occ:\n",
    "        return None\n",
    "    \n",
    "    return target_tag\n",
    "\n",
    "\n",
    "def get_merge_map(tags, all_tags_list, _cooccurrence_map, tag_2_idx, idx_2_tag, correlation_threshold = 3):\n",
    "    merge_map = {}\n",
    "    for tag in tags:\n",
    "        tag_ = merge_with_most_correlated(tag, _cooccurrence_map, tag_2_idx,  idx_2_tag, correlation_treshold=correlation_threshold, all_tags_list = all_tags_list)\n",
    "        merge_map[tag] = tag_\n",
    "    return merge_map\n",
    "\n",
    "def invert_merge_map(merge_map):\n",
    "    out = collections.defaultdict(lambda: [])\n",
    "    for from_, to_ in merge_map.items():\n",
    "        out[to_].append(from_)\n",
    "    return out\n",
    "\n",
    "def get_labels_after_merge(labels, merge_map):\n",
    "    out = []\n",
    "    for tags in labels:\n",
    "        tags = set(tags)\n",
    "        for _from, _to in merge_map.items():\n",
    "            if _from in tags:\n",
    "                tags.remove(_from, )\n",
    "                if _to is not None:\n",
    "                    tags.add(_to)\n",
    "        out.append(tags)\n",
    "    return out\n",
    "\n",
    "def get_unique_tags(all_tags):\n",
    "    t = set()\n",
    "    for tags in all_tags:\n",
    "        t |= set(tags)\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = df_.tags\n",
    "all_tags_list = get_all_tags_as_list(all_tags)\n",
    "all_tags_set = get_tags_as_set(all_tags)\n",
    "sorted_tags = sorted(list(set(all_tags_list)))\n",
    "tag_2_idx, idx_2_tag = get_tag_mappings(sorted_tags)\n",
    "cooccurrence_mat = get_correlation_matrix(all_tags, tag_2_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merges(num_iterations, correlation_threshold, _all_tags_set):\n",
    "    _all_tags_list = [list(x) for x in _all_tags_set]\n",
    "    _all_tags_flattened = get_all_tags_as_list(_all_tags_list)\n",
    "    _sorted_tags = list(get_unique_tags(_all_tags_set))\n",
    "    _tag_2_idx, _idx_2_tag = get_tag_mappings(_sorted_tags)\n",
    "    _cooccurrence_map = get_correlation_matrix(all_tags = _all_tags_list, tag_2_idx=_tag_2_idx)\n",
    "    for i in range(num_iterations):\n",
    "        merge_map = get_merge_map(_sorted_tags, _all_tags_flattened, _cooccurrence_map, _tag_2_idx, _idx_2_tag, correlation_threshold=correlation_threshold)\n",
    "        inverted_merge_map = invert_merge_map(merge_map)\n",
    "        new_labels = get_labels_after_merge(_all_tags_set, merge_map)\n",
    "        _sorted_tags = list(get_unique_tags(new_labels))\n",
    "        _all_tags_list = get_all_tags_as_list(new_labels)\n",
    "        _all_tags_set = [set(x) for x in new_labels]\n",
    "        _tag_2_idx, _idx_2_tag = get_tag_mappings(_sorted_tags)\n",
    "        _cooccurrence_map = get_correlation_matrix(all_tags = [list(x) for x in new_labels], tag_2_idx=_tag_2_idx)\n",
    "    return _sorted_tags, _all_tags_list\n",
    "\n",
    "_sorted_tags_, _all_tags_list = get_merges(\n",
    "    _all_tags_set=copy.copy(all_tags_set),\n",
    "    num_iterations=2,\n",
    "    correlation_threshold=16\n",
    ")\n",
    "\n",
    "counter = collections.Counter()\n",
    "for tags, (idx, df__) in zip(_all_tags_list, df_.iterrows()):\n",
    "    counter[len(tags)] += 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "pd.DataFrame(_all_tags_list, columns = ['tag']).tag.value_counts().to_frame().tag.plot(kind = 'bar', ax = ax)\n",
    "fig.tight_layout()\n",
    "pd.DataFrame(list(counter.items()), columns = ('num_tags', 'num_docs')).set_index('num_tags').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = np.sum(cooccurrence_mat, axis = 1)\n",
    "for i, tag in enumerate(_sorted_tags[:-1]):\n",
    "    for tag2 in _sorted_tags[i:]:\n",
    "        if tag == tag2: continue\n",
    "        max_ = np.max(cooccurrence_mat[tag_2_idx[tag]])\n",
    "        print(tag, tag2, max_, cooccurrence_mat[tag_2_idx[tag], tag_2_idx[tag2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "ax.imshow(cooccurrence_mat, cmap = plt.get_cmap('cubehelix'))\n",
    "ax.grid('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_idxmax = np.argsort(correlations)\n",
    "correlations = np.squeeze(np.asarray(correlations))\n",
    "print([(idx_2_tag[idx], correlations[idx]) for idx in correlations_idxmax[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlated_tags(tag):\n",
    "    idx = tag_2_idx[tag]\n",
    "    row = np.asarray(cooccurrence_mat[idx])[0]\n",
    "    tag_indices_sorted = np.argsort(row)\n",
    "    return list(reversed([(idx_2_tag[x], row[x]) for x in tag_indices_sorted if row[x] > 0]))\n",
    "\n",
    "with open('data/correlated_tags.txt', 'w') as f:\n",
    "    for tag in reversed(correlations_idxmax):\n",
    "        tag = idx_2_tag[tag]\n",
    "        f.write('{}\\n'.format(tag))\n",
    "        correlated_tags = get_correlated_tags(tag)\n",
    "        els = min(10, len(correlated_tags))\n",
    "        for t in correlated_tags[:els]:\n",
    "            f.write('\\t{}\\n'.format(t))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = np.squeeze(np.asarray(np.sum(cooccurrence_mat, axis = 1)))\n",
    "def plot_correlations(correlations, log = False):\n",
    "    ax = pd.DataFrame(correlations, columns = ['correlation']).sort_values('correlation').plot(kind = 'bar', logy = log)\n",
    "    ax.grid('off')\n",
    "    ax.set_xticks([])\n",
    "    return ax\n",
    "\n",
    "plot_correlations(correlations / 2)\n",
    "plot_correlations(correlations[(correlations > 50) & (correlations < 150)] / 2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
