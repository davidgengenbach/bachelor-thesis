{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED talk transcripts\n",
    "\n",
    "From this [Kaggle Competition](https://www.kaggle.com/rounakbanik/ted-talks/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mechanicalsoup\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import numpy as npii\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from glob import glob\n",
    "import scipy\n",
    "import collections\n",
    "import helper\n",
    "import numpy as np\n",
    "import copy\n",
    "from itertools import chain\n",
    "\n",
    "sns.set('notebook', style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (13, 5)\n",
    "plt.rcParams['axes.titlesize'] = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = helper.get_transcripts_with_tags_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save transcripts with urls as class as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X, Y = [], []\n",
    "\n",
    "for idx, df_ in df.iterrows():\n",
    "    y = df_.url_clean\n",
    "    x = df_.transcript\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "    \n",
    "with open('data/dataset_ted_talks.npy', 'wb') as f:\n",
    "    pickle.dump((X, Y), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of # words per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df.word_count.plot(kind = 'hist', bins = 120, ax = ax, title = 'Histogram of # words per document')\n",
    "ax.axvline(df.word_count.median(), c = 'red');\n",
    "ax.set_xlabel('Word count per document')\n",
    "ax.grid('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of # tags per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df.num_tags.plot(kind = 'hist', bins = 70, title = 'Histogram of # tags per document', ax = ax)\n",
    "ax.grid('off')\n",
    "ax.set_xlabel('# tags of document')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge tags\n",
    "\n",
    "This tries to merge correlated tags. A tag is correlated if they co-occur in tags for a talk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tags_as_list(all_tags):\n",
    "    return list(chain.from_iterable(all_tags))\n",
    "\n",
    "def get_tags_as_set(all_tags):\n",
    "    return [set(x) for x in all_tags]\n",
    "\n",
    "def get_tag_mappings(tags):\n",
    "    tag_2_idx = {tag: idx for idx, tag in enumerate(sorted(tags))}\n",
    "    idx_2_tag = {idx: tag for tag, idx in tag_2_idx.items()}\n",
    "    return tag_2_idx, idx_2_tag\n",
    "\n",
    "def get_correlation_matrix(all_tags, tag_2_idx, symmetric = True):\n",
    "    num_unique_tags = len(tag_2_idx.keys())\n",
    "    cooccurrence_mat = scipy.sparse.lil_matrix((num_unique_tags, num_unique_tags))\n",
    "    for tags in all_tags:\n",
    "        for i, tag in enumerate(tags[:-1]):\n",
    "            for j, tag2 in enumerate(tags[i+1:]):\n",
    "                cooccurrence_mat[tag_2_idx[tag], tag_2_idx[tag2]] += 1\n",
    "    cooccurrence_mat = cooccurrence_mat.todense()\n",
    "    if symmetric:\n",
    "        cooccurrence_mat = np.maximum(cooccurrence_mat, cooccurrence_mat.T)\n",
    "    return cooccurrence_mat\n",
    "\n",
    "def get_number_of_occurrences(tag, all_tags_list):\n",
    "    number_of_occurrences = collections.Counter(all_tags_list)\n",
    "    assert tag in number_of_occurrences, 'Tag \"{}\" not in all_tags_list'.format(tag)\n",
    "    return number_of_occurrences[tag]\n",
    "\n",
    "def merge_with_most_correlated(tag, cooccurrence_mat, tag_2_idx, idx_2_tag, correlation_treshold , all_tags_list):\n",
    "    idx = tag_2_idx[tag]\n",
    "    max_correlated_idx = np.argmax(cooccurrence_mat[idx])\n",
    "    val = cooccurrence_mat[idx, max_correlated_idx]\n",
    "    if val < correlation_treshold:\n",
    "        return None\n",
    "    target_tag = idx_2_tag[max_correlated_idx]\n",
    "    tag_1_occ = get_number_of_occurrences(tag, all_tags_list)\n",
    "    tag_2_occ = get_number_of_occurrences(target_tag, all_tags_list)\n",
    "    \n",
    "    if tag_1_occ > tag_2_occ:\n",
    "        return None\n",
    "    \n",
    "    return target_tag\n",
    "\n",
    "\n",
    "def get_merge_map(tags, all_tags_list, _cooccurrence_map, tag_2_idx, idx_2_tag, correlation_threshold = 3):\n",
    "    merge_map = {}\n",
    "    for tag in tags:\n",
    "        tag_ = merge_with_most_correlated(tag, _cooccurrence_map, tag_2_idx,  idx_2_tag, correlation_treshold=correlation_threshold, all_tags_list = all_tags_list)\n",
    "        merge_map[tag] = tag_\n",
    "    return merge_map\n",
    "\n",
    "def invert_merge_map(merge_map):\n",
    "    out = collections.defaultdict(lambda: [])\n",
    "    for from_, to_ in merge_map.items():\n",
    "        out[to_].append(from_)\n",
    "    return out\n",
    "\n",
    "def get_labels_after_merge(labels, merge_map):\n",
    "    out = []\n",
    "    for tags in labels:\n",
    "        tags = set(tags)\n",
    "        for _from, _to in merge_map.items():\n",
    "            if _from in tags:\n",
    "                tags.remove(_from, )\n",
    "                if _to is not None:\n",
    "                    tags.add(_to)\n",
    "        out.append(tags)\n",
    "    return out\n",
    "\n",
    "def get_unique_tags(all_tags):\n",
    "    t = set()\n",
    "    for tags in all_tags:\n",
    "        t |= set(tags)\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = df.tags\n",
    "all_tags_list = get_all_tags_as_list(all_tags)\n",
    "all_tags_set = get_tags_as_set(all_tags)\n",
    "sorted_tags = sorted(list(set(all_tags_list)))\n",
    "tag_2_idx, idx_2_tag = get_tag_mappings(sorted_tags)\n",
    "cooccurrence_mat = get_correlation_matrix(all_tags, tag_2_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merges(num_iterations, correlation_threshold, _all_tags_set):\n",
    "    _all_tags_list = [list(x) for x in _all_tags_set]\n",
    "    _all_tags_flattened = get_all_tags_as_list(_all_tags_list)\n",
    "    _sorted_tags = list(get_unique_tags(_all_tags_set))\n",
    "    _tag_2_idx, _idx_2_tag = get_tag_mappings(_sorted_tags)\n",
    "    _cooccurrence_map = get_correlation_matrix(all_tags = _all_tags_list, tag_2_idx=_tag_2_idx)\n",
    "    for i in range(num_iterations):\n",
    "        merge_map = get_merge_map(_sorted_tags, _all_tags_flattened, _cooccurrence_map, _tag_2_idx, _idx_2_tag, correlation_threshold=correlation_threshold)\n",
    "        inverted_merge_map = invert_merge_map(merge_map)\n",
    "        new_labels = get_labels_after_merge(_all_tags_set, merge_map)\n",
    "        _sorted_tags = list(get_unique_tags(new_labels))\n",
    "        _all_tags_list = get_all_tags_as_list(new_labels)\n",
    "        _all_tags_set = [set(x) for x in new_labels]\n",
    "        _tag_2_idx, _idx_2_tag = get_tag_mappings(_sorted_tags)\n",
    "        _cooccurrence_map = get_correlation_matrix(all_tags = [list(x) for x in new_labels], tag_2_idx=_tag_2_idx)\n",
    "    return _sorted_tags, _all_tags_list\n",
    "\n",
    "_sorted_tags_, _all_tags_list = get_merges(\n",
    "    _all_tags_set=copy.copy(all_tags_set),\n",
    "    num_iterations=2,\n",
    "    correlation_threshold=10\n",
    ")\n",
    "\n",
    "counter = collections.Counter()\n",
    "for tags in _all_tags_list:\n",
    "    counter[len(tags)] += 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "pd.DataFrame(_all_tags_list, columns = ['tag']).tag.value_counts().to_frame().tag.plot(kind = 'bar', ax = ax)\n",
    "fig.tight_layout()\n",
    "df__ = pd.DataFrame(list(counter.items()), columns = ('num_tags', 'num_docs')).set_index('num_tags').sort_index()\n",
    "print(df__.num_docs.sum())\n",
    "df__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "ax.imshow(cooccurrence_mat, cmap = plt.get_cmap('cubehelix'))\n",
    "ax.grid('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_idxmax = np.argsort(correlations)\n",
    "correlations = np.squeeze(np.asarray(correlations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlated_tags(tag):\n",
    "    idx = tag_2_idx[tag]\n",
    "    row = np.asarray(cooccurrence_mat[idx])[0]\n",
    "    tag_indices_sorted = np.argsort(row)\n",
    "    return list(reversed([(idx_2_tag[x], row[x]) for x in tag_indices_sorted if row[x] > 0]))\n",
    "\n",
    "with open('data/correlated_tags.txt', 'w') as f:\n",
    "    for tag in reversed(correlations_idxmax):\n",
    "        tag = idx_2_tag[tag]\n",
    "        f.write('{}\\n'.format(tag))\n",
    "        correlated_tags = get_correlated_tags(tag)\n",
    "        els = min(10, len(correlated_tags))\n",
    "        for t in correlated_tags[:els]:\n",
    "            f.write('\\t{}\\n'.format(t))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove un-frequent tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = list(chain.from_iterable(df.tags.values))\n",
    "tag_counts = collections.Counter(all_tags)\n",
    "\n",
    "df_tag_counts = pd.DataFrame(list(tag_counts.items()), columns=['label', 'occurrences'])\n",
    "\n",
    "lim = (0, df_tag_counts.occurrences.max())\n",
    "\n",
    "ax = df_tag_counts.occurrences.plot(kind='hist', bins=300, title='Histogram of tag occurrences')\n",
    "ax.set_xlabel('number of occurrences of single tag');\n",
    "ax.set_xlim(*lim)\n",
    "\n",
    "\n",
    "too_frequent = df_tag_counts.occurrences.quantile(0.999)\n",
    "too_unfrequent = df_tag_counts.occurrences.quantile(0.01)\n",
    "\n",
    "too_frequent = 1000\n",
    "too_unfrequent = 10\n",
    "\n",
    "mask_clipped = (df_tag_counts.occurrences < too_frequent) & (df_tag_counts.occurrences > too_unfrequent)\n",
    "df_tag_counts_clipped = df_tag_counts[mask_clipped]\n",
    "\n",
    "\n",
    "tags_unwanted = set(df_tag_counts[mask_clipped == False].label.values)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "df_tag_counts_clipped.occurrences.plot(kind='hist', bins=300, ax=ax)\n",
    "ax.set_xlim(*lim);\n",
    "\n",
    "\n",
    "num_tags = len(df_tag_counts)\n",
    "num_tags_unwanted = len(tags_unwanted)\n",
    "print('# tags:\\t\\t\\t{}'.format(num_tags))\n",
    "print('# unwanted tags:\\t{}'.format(num_tags_unwanted))\n",
    "print('# after tags:\\t\\t{}'.format(num_tags - num_tags_unwanted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags_clean'] = df.tags.apply(lambda x: set(x) - tags_unwanted)\n",
    "df['num_tags_clean'] = df.tags_clean.apply(len)\n",
    "\n",
    "assert not len(df[df.num_tags_clean == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most often tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 30\n",
    "most_often = df_tag_counts.sort_values('occurrences')[-n_top:].set_index('label').sort_index()\n",
    "display(most_often.T)\n",
    "most_often_tags = most_often.index.values\n",
    "most_often_ids = [tag_2_idx[label] for label in most_often_tags]\n",
    "most_often_occs = most_often.occurrences.values\n",
    "indices = np.ix_(most_often_ids, most_often_ids)\n",
    "most_often_coo = cooccurrence_mat[indices]\n",
    "\n",
    "ind = list(range(n_top))\n",
    "most_often_coo[ind, ind] = most_often_occs\n",
    "\n",
    "most_often_coo /= most_often_occs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          classes=None,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          round_confusion=2,\n",
    "                          x_rotation=90,\n",
    "                          show_non_horizontal_percent=True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import itertools\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix.\n",
    "    Taken from: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    cmap = plt.cm.Blues\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(im)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(classes, rotation=x_rotation)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if not show_non_horizontal_percent and i != j:\n",
    "            continue\n",
    "        val = int(round(cm[i, j], round_confusion) * 100) if round_confusion else cm[i, j]\n",
    "        val = '{}%'.format(val)\n",
    "        ax.text(j, i, val,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    ax.grid(False)\n",
    "    fig.tight_layout()\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = np.squeeze(np.asarray(most_often_coo.sum(axis = 1)))\n",
    "\n",
    "correlations = sorted(list(zip(most_often_tags, correlations)), key=lambda x: x[1])\n",
    "used_tags, _ = zip(*correlations[:10])\n",
    "\n",
    "most_often_tags = used_tags\n",
    "most_often_ids = [tag_2_idx[label] for label in most_often_tags]\n",
    "most_often_occs = [most_often.loc[x].occurrences for x in most_often_tags]\n",
    "\n",
    "indices = np.ix_(most_often_ids, most_often_ids)\n",
    "most_often_coo = cooccurrence_mat[indices]\n",
    "most_often_coo /= most_often_occs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = fontdict=dict(horizontalalignment='center', verticalalignment='center')\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "im = ax.imshow(most_often_coo, cmap=plt.get_cmap('hot_r'))\n",
    "fig.colorbar(im)\n",
    "ax.grid(False)\n",
    "for row_idx, row in enumerate(np.asarray(most_often_coo)):\n",
    "    for cell_idx, cell in enumerate(row):        \n",
    "        if row_idx == cell_idx:\n",
    "            text = most_often_occs[row_idx]\n",
    "        else:\n",
    "            text = '{:.0f}%'.format(cell * 100)\n",
    "        ax.text(row_idx, cell_idx, text, text_dict, color = 'blue')\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "for idx, tag in enumerate(most_often_tags):\n",
    "    ax.text(idx, -0.8, tag, fontdict=text_dict)\n",
    "    ax.text(-0.6, idx, tag, fontdict=dict(text_dict, **dict(horizontalalignment='right')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_tags = ['entertainment', 'health', 'innovation']\n",
    "#allowed_tags = ['entertainment', 'tedx']\n",
    "\n",
    "allowed_tags = ['economics', 'environment', 'brain', 'entertainment']\n",
    "\n",
    "for tag in allowed_tags:\n",
    "    assert tag in df_tag_counts.values\n",
    "    print('{:22} {}'.format(tag, most_often[most_often.index == tag].occurrences.values[0]))\n",
    "\n",
    "def get_common_label(labels, allowed_labels = allowed_tags):\n",
    "    common_labels =  list(set(labels)  & set(allowed_labels))\n",
    "    assert len(common_labels) == 1\n",
    "    return common_labels[0]\n",
    "    \n",
    "def filter_tags(tags):\n",
    "    return len(set(tags)  & set(allowed_tags)) == 1\n",
    "    \n",
    "df_filtered = df[df.tags.apply(filter_tags)]\n",
    "df_filtered['label'] = df_filtered.tags.apply(get_common_label)\n",
    "\n",
    "print('\\n\\nElements after filter: {}'.format(len(df_filtered)))\n",
    "\n",
    "vals = df_filtered[['url_clean', 'label', 'transcript']].rename(columns={'url_clean': 'url'}).set_index('url')\n",
    "\n",
    "with open('data/df_dataset.npy', 'wb') as f:\n",
    "    pickle.dump(vals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "X = df_filtered.transcript.values\n",
    "Y = df_filtered.label.values\n",
    "\n",
    "pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('vectorizer', None),\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n",
    "param_grid = dict(\n",
    "    #vectorizer=[sklearn.feature_extraction.text.CountVectorizer(), sklearn.feature_extraction.text.TfidfVectorizer()],\n",
    "    vectorizer=[sklearn.feature_extraction.text.TfidfVectorizer()],\n",
    "    classifier=[sklearn.svm.LinearSVC(class_weight='balanced', C=1), sklearn.svm.LinearSVC(class_weight='balanced', C=0.1)],\n",
    "    classifier__C=[1e-1, 1]\n",
    ")\n",
    "\n",
    "dummy_clf = sklearn.dummy.DummyClassifier()\n",
    "dummy_clf.fit([[0]] * len(Y), Y)\n",
    "Y_pred_dummy = dummy_clf.predict([[0]] * len(Y))\n",
    "dummy_score = sklearn.metrics.f1_score(Y, Y_pred_dummy, average='macro')\n",
    "\n",
    "gscv = sklearn.model_selection.GridSearchCV(pipeline, param_grid=param_grid, cv=3, scoring='f1_macro', verbose=0)\n",
    "\n",
    "gscv_result = gscv.fit(X, Y)\n",
    "Y_pred = gscv.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(allowed_tags)\n",
    "print('#elements {}'.format(len(df_filtered)))\n",
    "display(dummy_score)\n",
    "pd.DataFrame(gscv_result.cv_results_)[['param_classifier', 'param_vectorizer', 'mean_test_score', 'mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(allowed_tags)\n",
    "cm = sklearn.metrics.confusion_matrix(Y, Y_pred, labels = classes)\n",
    "plot_confusion_matrix(cm, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags_clean = list(chain.from_iterable(df.tags_clean.values))\n",
    "all_tags_ = df.tags_clean.values\n",
    "\n",
    "def get_tag_vector(tags, idx, mat, mapping=tag_2_idx):\n",
    "    non_zero_elements = [mapping[t] for t in tags]\n",
    "    mat[idx, non_zero_elements] = 1\n",
    "    \n",
    "mapping = tag_2_idx\n",
    "num_unique_labels = len(mapping)\n",
    "tag_mat = scipy.sparse.lil_matrix((len(all_tags_), num_unique_labels), dtype=bool)\n",
    "tag_vectors = [get_tag_vector(t, idx, tag_mat) for idx, t in enumerate(all_tags_)]\n",
    "tag_mat = tag_mat.tocsr()\n",
    "assert len(tag_mat.nonzero()[0]) == len(all_tags_clean)\n",
    "assert len(df) == tag_mat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(tag_mat.todense())\n",
    "fig.tight_layout()\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "results = {}\n",
    "X = df.transcript.values\n",
    "X_vec = sklearn.feature_extraction.text.CountVectorizer().fit_transform(X)\n",
    "X_vec_tfidf = sklearn.feature_extraction.text.TfidfVectorizer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm, sklearn.pipeline, sklearn.feature_extraction, sklearn.model_selection, sklearn.cluster, sklearn.dummy\n",
    "\n",
    "cluster_clf = sklearn.cluster.KMeans(n_init=300, max_iter=1000)\n",
    "\n",
    "for n_clusters in [3]:\n",
    "    print('Starting:', n_clusters)\n",
    "    cluster_clf.set_params(n_clusters=n_clusters)\n",
    "    Y = cluster_clf.fit_predict(tag_mat)\n",
    "    \n",
    "    pd.DataFrame(list(collections.Counter(Y).items()), columns=['label', 'occurrences']).set_index('label').sort_index().occurrences.plot(kind='barh')\n",
    "    plt.show()\n",
    "    print('Fitted clusters')\n",
    "    pipeline = sklearn.pipeline.Pipeline([\n",
    "        ('classifier', None)\n",
    "    ])\n",
    "\n",
    "    param_grid = dict(\n",
    "        classifier=[sklearn.svm.LinearSVC(class_weight='balanced', C=1), sklearn.dummy.DummyClassifier()]\n",
    "    )\n",
    "    \n",
    "    gscv = sklearn.model_selection.GridSearchCV(pipeline, param_grid=param_grid, cv=3, scoring='f1_macro', verbose=0)\n",
    "\n",
    "    gscv_result = gscv.fit(X_vec, Y)\n",
    "    display(pd.DataFrame(gscv_result.cv_results_))\n",
    "    Y_pred = gscv.predict(X_vec)\n",
    "    results[n_clusters] = gscv_result\n",
    "    print(n_clusters, gscv_result.best_score_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
