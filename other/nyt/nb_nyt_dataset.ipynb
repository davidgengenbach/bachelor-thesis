{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('files.txt') as f:\n",
    "    files = [x.strip() for x in f.read().splitlines() if x.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(pd.to_datetime(files), columns=['date']).date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "s.plot(ax = ax, marker='o', linestyle='none', markersize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "file = 'meta.csv'\n",
    "header = ['filename', 'title', 'print_section', 'desk', 'online_sections', 'word_count']\n",
    "\n",
    "with open(file) as f:\n",
    "    reader = csv.DictReader(f, fieldnames=header)\n",
    "    rows = list(reader)\n",
    "\n",
    "df = pd.DataFrame(rows).set_index('filename')\n",
    "for c in df:\n",
    "    df[df[c] == '_'] = np.nan\n",
    "df['word_count'] = pd.to_numeric(df.word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by wordcount\n",
    "\n",
    "Use only articles with more than 3000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_with_word_counts(df, low, high):\n",
    "    return df[(df.word_count > low) & (df.word_count < high)]\n",
    "\n",
    "df_filtered = get_articles_with_word_counts(df, 3000, df.word_count.quantile(0.9999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "df_filtered.word_count.plot(kind='hist', ax = ax, bins=120, title='Histogram of # words per article - after word count filter, #articles: {}'.format(len(df_filtered)))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out by the online section\n",
    "\n",
    "Remove all articles that have been posted in multiple online sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df_filtered[df_filtered.online_sections.str.contains(';') == False].online_sections.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remove all articles that belong to a online section that has less than 250 articles in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_online_section = f[f.online_sections > 250].index.values\n",
    "df_filtered_filtered = df_filtered[df_filtered.online_sections.apply(lambda x: x in filtered_online_section)]\n",
    "df_filtered_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot articles per class after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "df_filtered_filtered.online_sections.value_counts().plot(kind='barh', ax = ax, title='# articles per class')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save filenames to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filtered_articles.txt', 'w') as f:\n",
    "    f.write('\\n'.join(df_filtered_filtered.index.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get filtered elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'filtered_articles/'\n",
    "filtered_files = glob('{}*/*/*/*.xml'.format(prefix))\n",
    "filtered_files_ = ['/'.join(x.rsplit('/', 4)[-4:]) for x in filtered_files]\n",
    "# Test whether all articles are there\n",
    "assert len(filtered_files_) == len(set(filtered_files_) & set(df_filtered_filtered.index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def get_body_of_article(file):\n",
    "    assert os.path.exists(file)\n",
    "    with open(file) as f:\n",
    "        content = f.read()\n",
    "    body = re.findall(r'<block class=\"full_text\">(.+?)</block>', content, re.DOTALL | re.MULTILINE)\n",
    "    assert len(body) == 1\n",
    "    body = body[0].strip().replace('<p>', '').replace('</p>', '')\n",
    "    return body\n",
    "\n",
    "bodies = {}\n",
    "for idx, file in enumerate(filtered_files):\n",
    "    sys.stdout.write('\\r{:9}/{}'.format(idx + 1, len(filtered_files)))\n",
    "    body = get_body_of_article(file)\n",
    "    bodies[file.replace(prefix, '')] = body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_sorted = []\n",
    "for filename, df_ in df_filtered_filtered.iterrows():\n",
    "    assert filename in bodies\n",
    "    bodies_sorted.append(bodies[filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_filtered['body'] = bodies_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X = df_filtered_filtered.body.values\n",
    "Y = df_filtered_filtered.online_sections.values\n",
    "\n",
    "with open('dataset_nyt.npy', 'wb') as f:\n",
    "    pickle.dump((X, Y), f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
