{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "import pickle\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import dummy\n",
    "import sys\n",
    "import os\n",
    "import helper\n",
    "\n",
    "USED_FOLDER = '2017-09-13_21:33'\n",
    "#USED_FOLDER = None\n",
    "\n",
    "result_folders = [x for x in glob('data/results/2017*') if os.path.isdir(x)]\n",
    "\n",
    "folder = 'data/results/{}'.format(USED_FOLDER) if USED_FOLDER else result_folders[-1]\n",
    "\n",
    "print('Using result folder: {}'.format(folder))\n",
    "\n",
    "df_all_ = None\n",
    "for result_file in helper.log_progress(glob('{}/*.npy'.format(folder))):\n",
    "    with open(result_file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    dataset = result_file.split('/')[-1].rsplit('.', 2)[0]\n",
    "    is_graph_dataset = 'graph' in dataset\n",
    "    is_cooccurrence_dataset = 'cooccurrence' in dataset\n",
    "    \n",
    "    dataset_name = dataset\n",
    "    \n",
    "    if is_graph_dataset:\n",
    "        is_label_dropped = 'same-label' in result_file\n",
    "        result['same_label'] = is_label_dropped\n",
    "        is_relabeled = 'relabeled' in result_file\n",
    "        result['relabeled'] = is_relabeled\n",
    "        if is_relabeled:\n",
    "            topn = result_file.split('topn-')[1].split('_')[0]\n",
    "            threshold = result_file.split('threshold-')[1].split('_')[0]\n",
    "            result['topn'] = int(topn)\n",
    "            result['threshold'] = float(threshold)\n",
    "        result['wl_iteration'] = dataset.split('.')[-1]\n",
    "        parts = dataset.split('_')\n",
    "        if is_cooccurrence_dataset:\n",
    "            dataset_name = parts[-1].split('_')[0].split('.')[0]\n",
    "            result['words'] = parts[4]\n",
    "            result['window_size'] = parts[3]\n",
    "        # GML\n",
    "        else:\n",
    "            dataset_name = parts[3].split('.')[0]\n",
    "            result['words'] = 'concepts'\n",
    "        result['type'] = 'cooccurrence' if is_cooccurrence_dataset else 'concept-graph'\n",
    "    else:\n",
    "        result['type'] = 'text'\n",
    "        dataset_name = dataset.split('_')[1]\n",
    "        #result['words'] = ['all' if x['preprocessing'] != None else 'only-nouns'  for x in result['params']]\n",
    "        result['words'] = ['all' for x in result['params']]\n",
    "        #result['words'] = result['params']['preprocessing']\n",
    "    result['classifier'] = [None] * len(result['params'])\n",
    "    \n",
    "    for idx, param in enumerate(result['params']):\n",
    "        result['classifier'][idx] = type(param['clf']).__name__\n",
    "        del param['clf']\n",
    "        \n",
    "    result['filename'] = result_file\n",
    "    result['dataset'] = dataset_name\n",
    "    \n",
    "    is_lemmatized = '_lemmatized_' in result_file\n",
    "    result['lemmatized'] = is_lemmatized\n",
    "    \n",
    "    if dataset_name.endswith('-single') or dataset_name.endswith('-ana'):\n",
    "        dataset_name = dataset_name.rsplit('-', 1)[0]\n",
    "    del result['param_clf']\n",
    "\n",
    "\n",
    "    \n",
    "    result_df = pd.DataFrame(result).sort_values(by = 'dataset', ascending = False)\n",
    "    df_all_ = result_df if df_all_ is None else df_all_.append(result_df)\n",
    "\n",
    "assert df_all_ is not None\n",
    "assert df_all_.shape[0]\n",
    "        \n",
    "df_all_['threshold'] = pd.to_numeric(df_all_['threshold'])\n",
    "# Only keep datasets where there are all three types (text, co-occurence and concept-graph) of results\n",
    "df_all = df_all_.groupby('dataset').filter(lambda x: len(x.type.value_counts()) == 3).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set('notebook', 'whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_OF_INTEREST = ['classifier', 'dataset', 'filename', 'lemmatized', 'accuracy', 'f1', 'precision', 'recall', 'mean_train_f1_macro', 'param_preprocessing', 'param_scaler', 'params', 'relabeled', 'same_label', 'std_test_accuracy', 'std_test_f1_macro', 'std_test_precision_macro', 'std_test_recall_macro', 'threshold', 'topn', 'window_size', 'wl_iteration', 'words']\n",
    "\n",
    "def plot_best_by_type(df_all, df, title = '', fontsize = 12, figsize = (6, 3), top = 0.85):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    els = df_all.iloc[df['mean_test_f1_macro'].idxmax()]\n",
    "    els = els.set_index('type')\n",
    "    els = els.rename(columns = {'mean_test_f1_macro': 'f1', 'mean_test_accuracy': 'accuracy', 'mean_test_precision_macro': 'precision', 'mean_test_recall_macro': 'recall'})\n",
    "    els[['f1', 'accuracy', 'precision', 'recall']].plot(kind = 'barh', ax = ax, xlim = (0, 1.5), xerr=[els.std_test_f1_macro * 2,  els.std_test_accuracy * 2,  els.std_test_precision_macro * 2,  els.std_test_recall_macro * 2])\n",
    "    #    df['mean_test_f1_macro'].max().plot(kind = 'barh', ax = ax, xlim = (0, 1))#)\n",
    "    ax.set_xticks(np.linspace(0, 1, 11))\n",
    "    ax.grid(axis = 'y')\n",
    "    for set_type, x in els.iterrows():\n",
    "        out = '{:20}best f1-score: {x.f1:.4f}\\twords: {x.words:14}'.format(set_type, x = x)\n",
    "        if set_type != 'text':\n",
    "            out += 'wl iteration: {x.wl_iteration:<8} relabeled: {x.relabeled:<6} is_same_label: {x.same_label:<6}'.format(x = x)\n",
    "        else:\n",
    "            #out += 'stop words:   {}'.format(x.params[\"count_vectorizer__stop_words\"])\n",
    "            pass\n",
    "        if set_type == 'cooccurrence':\n",
    "            out += 'window_size: {x.window_size}'.format(x = x)\n",
    "        print(out)\n",
    "    display(els[COLUMNS_OF_INTEREST])\n",
    "    if title and title != '':\n",
    "        fig.suptitle(title, fontsize = fontsize)\n",
    "    fig.tight_layout()\n",
    "    if title and title != '':\n",
    "        fig.subplots_adjust(top = top)\n",
    "    return fig, ax\n",
    "\n",
    "for name, df_dataset in sorted(df_all.groupby('dataset'), key = lambda x: x[0]):\n",
    "    df_dataset_grouped_by_type = df_dataset.groupby('type')\n",
    "    print(name)\n",
    "    use_title = False\n",
    "    fig, ax = plot_best_by_type(df_all, df_dataset_grouped_by_type, 'Dataset: {}'.format(name) if use_title else None)\n",
    "    fig.savefig('tmp/results/dataset-{}-best.png'.format(name), dpi = 150)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    df_graphs_grouped = df_all[df_all.type != 'text'].groupby('dataset')\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize = (10, 6))\n",
    "    for idx, (dataset_name, df_dataset) in enumerate(df_graphs_grouped):\n",
    "        ax = axes[int(idx / 2), idx % 2]\n",
    "        grouped = df_dataset.groupby('relabeled')\n",
    "        els = df_all.iloc[grouped.mean_test_f1_macro.idxmax()]\n",
    "        els = els.set_index('relabeled')\n",
    "        #threshold = els[els.threshold > 0].iloc[0].threshold\n",
    "        els = els.rename(columns = {'mean_test_f1_macro': 'f1', 'mean_test_accuracy': 'accuracy', 'mean_test_precision_macro': 'precision', 'mean_test_recall_macro': 'recall'})\n",
    "        els[['f1']].plot(kind = 'barh', ax = ax, legend = False, xlim = (0, 1), title = dataset_name)\n",
    "        ax.set_xlabel('f1')\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Merged labels')\n",
    "    fig.subplots_adjust(top = 0.9)\n",
    "    fig.savefig('tmp/results/relabeled.png', dpi = 150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_distributions(df, df_all, title = None, figsize = (9, 4)):\n",
    "    fig, axes_indexed = plt.subplots(nrows = 2, ncols=2, figsize = figsize)\n",
    "\n",
    "    axes = []\n",
    "    for ax_row in axes_indexed:\n",
    "        axes += list(ax_row)\n",
    "    #, 'relabeled'\n",
    "    for val, ax in zip(['wl_iteration', 'window_size', 'words', 'type'], axes):\n",
    "        if len(df.groupby(val).size()) == 0:\n",
    "            continue\n",
    "        grouped = df.groupby(val)\n",
    "        els = df_all.iloc[grouped['mean_test_f1_macro'].idxmax()]\n",
    "        els = els.set_index(val)\n",
    "        els = els.rename(columns = {'mean_test_f1_macro': 'f1', 'mean_test_accuracy': 'accuracy', 'mean_test_precision_macro': 'precision', 'mean_test_recall_macro': 'recall'})\n",
    "        els[['f1', 'accuracy', 'precision', 'recall']].plot(kind = 'barh', ax = ax, xlim=(0, 2))\n",
    "        \n",
    "        ax.set_xlabel('f1 macro score')\n",
    "    plt.suptitle(title, size = 18)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    return fig, axes\n",
    "    \n",
    "dpi = 150\n",
    "\n",
    "if 0 == 1:\n",
    "    fig, _  = plot_distributions(df_all, df_all, title = 'Mean over all datasets')\n",
    "    fig.savefig('tmp/results/all.png', dpi = dpi)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    for name, df_dataset in df_all.groupby('dataset'):\n",
    "        if len(df_dataset.type.value_counts()) < 3:\n",
    "            continue\n",
    "        fig, _ = plot_distributions(df_dataset, df_all, title = 'Dataset: {}'.format(name))\n",
    "        fig.savefig('tmp/results/dataset-{}.png'.format(name), dpi = dpi)\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/check-w2v-results.json') as f:\n",
    "    w2v_results = json.load(f)\n",
    " \n",
    "per_embedding_type = {}\n",
    "for dataset, value in w2v_results.items():\n",
    "    print(dataset)\n",
    "    for embedding_raw, cache_files in sorted(value.items(), key = lambda x: x[0]):\n",
    "        embedding = embedding_raw.split('/')[-1].rsplit('.', 2)[0]\n",
    "        if len(cache_files.keys()) != 2: continue\n",
    "        print('\\t{}'.format(embedding))\n",
    "        if embedding not in per_embedding_type:\n",
    "            per_embedding_type[embedding] = {}\n",
    "        per_embedding_type[embedding][dataset] = []\n",
    "        for dataset_file, counts in sorted(cache_files.items(), key = lambda x: x[0]):\n",
    "            not_found_ratio = int(counts['counts']['not_found'] / counts['num_labels'] * 100)\n",
    "            if embedding == 'trained' and 'coo' in  dataset_file:\n",
    "                print('Yes', counts['counts']['not_found'], not_found_ratio, '%', counts['not_found_sample'])\n",
    "            is_gml = 'dataset_graph_gml' in dataset_file\n",
    "            per_embedding_type[embedding][dataset].append((is_gml, not_found_ratio))\n",
    "            print('\\t\\t{:4} missing  {:3>}%'.format('gml' if is_gml else 'co', not_found_ratio))\n",
    "        per_embedding_type[embedding][dataset] = per_embedding_type[embedding][dataset][0][1]  #sum(y for x, y in per_embedding_type[embedding][dataset]) / 2\n",
    "df = pd.DataFrame(per_embedding_type)\n",
    "df#[df.index == \"ng20\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import dataset_helper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "currently = ['ling-spam', 'webkb', 'ng20', 'reuters-21578']\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset in currently: continue\n",
    "    print(dataset)\n",
    "    continue\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    dataset_helper.plot_dataset_class_distribution(X, Y, 'Class distribution: {}'.format(dataset))\n",
    "    plt.show()\n",
    "    print('{}\\n#Docs:\\t{}\\t# Classes:\\t{}'.format(dataset, len(X), len(set(Y))))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "38px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
