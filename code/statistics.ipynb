{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "import pickle\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set('notebook', 'white')\n",
    "\n",
    "df_all_ = None\n",
    "for result_file in glob('data/results/*.npy'):\n",
    "    print(result_file)\n",
    "    with open(result_file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    dataset = result_file.split('/')[-1].rsplit('.', 2)[0]\n",
    "    is_graph_dataset = 'graph' in dataset\n",
    "    is_cooccurrence_dataset = 'cooccurrence' in dataset\n",
    "    dataset_name = dataset\n",
    "    if is_graph_dataset:\n",
    "        result['relabeled'] = 'relabeled' in result_file\n",
    "        result['wl_iteration'] = int(dataset.split('.')[-1])\n",
    "        parts = dataset.split('_')\n",
    "        if is_cooccurrence_dataset:\n",
    "            dataset_name = parts[-1].split('_')[0].split('.')[0]\n",
    "            result['words'] = parts[4]\n",
    "            result['window_size'] = parts[3]\n",
    "        # GML\n",
    "        else:\n",
    "            dataset_name = parts[3].split('.')[0]\n",
    "            result['words'] = 'concepts'\n",
    "        result['type'] = 'cooccurrence' if is_cooccurrence_dataset else 'concept-graph'\n",
    "    else:\n",
    "        result['type'] = 'text'\n",
    "        dataset_name = dataset.split('_')[1]\n",
    "        result['words'] = ['all' if x['preprocessing'] != None else 'only-nouns'  for x in result['params']]\n",
    "        #result['words'] = result['params']['preprocessing']\n",
    "    for param in result['params']:\n",
    "        del param['clf']\n",
    "    if dataset_name.endswith('-single'):\n",
    "        dataset_name = dataset_name.rsplit('-', 1)[0]\n",
    "    del result['param_clf']\n",
    "    result['dataset'] = dataset_name\n",
    "    result_df = pd.DataFrame(result).sort_values(by = 'dataset', ascending = False)\n",
    "    if df_all_ is None:\n",
    "        df_all_ = result_df\n",
    "    else:\n",
    "        df_all_ = df_all_.append(result_df)\n",
    "#df_all['window_size'].fillna('concept-graph', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_[df_all_.dataset == 'ng20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set('notebook', 'white')\n",
    "def plot_distributions(df, title = None, figsize = (8, 5)):\n",
    "    fig, axes_indexed = plt.subplots(nrows = 2, ncols=2, figsize = figsize)\n",
    "\n",
    "    axes = []\n",
    "    for ax_row in axes_indexed:\n",
    "        axes += list(ax_row)\n",
    "    for val, ax in zip(['wl_iteration', 'window_size', 'words', 'type'], axes):\n",
    "        if len(df.groupby(val).size()) == 0:\n",
    "            continue\n",
    "        df.groupby(val).mean_test_score.mean().plot(kind = 'barh', ax = ax, xlim = (0, 1))\n",
    "        ax.set_xlabel('f1 macro score')\n",
    "    plt.suptitle(title, size = 18)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    return fig, axes\n",
    "    \n",
    "df_all = df_all_.groupby('dataset').filter(lambda x: len(x.type.value_counts()) == 3).reset_index(drop=True)\n",
    "fig, _  = plot_distributions(df_all, title = 'Mean over all datasets')\n",
    "fig.savefig('tmp/results/all.png', dpi = 100)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "for name, df_dataset in df_all.groupby('dataset'):\n",
    "    if len(df_dataset.type.value_counts()) < 3:\n",
    "        continue\n",
    "    fig, _ = plot_distributions(df_dataset, title = 'Dataset: {}'.format(name))\n",
    "    fig.savefig('tmp/results/dataset-{}.png'.format(name), dpi = 100)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_best_by_type(df_all, df, title = '', fontsize = 12, figsize = (5, 2), top = 0.85):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    els = df_all.iloc[df['mean_test_score'].idxmax()]\n",
    "    df['mean_test_score'].max().plot(kind = 'barh', ax = ax, xlim = (0, 1), xerr=els.std_test_score * 2)\n",
    "    display(els.std_test_score)\n",
    "    ax.set_xlabel('f1 score')\n",
    "    for idx, x in els.iterrows():\n",
    "        out = '{x.type:20}best f1-score: {x.mean_test_score:.4f}\\twords: {x.words:14}'.format(x = x)\n",
    "        if x.type != 'text':\n",
    "            out += 'wl iteration: {x.wl_iteration:<8.0f} relabeled: {x.relabeled:<6}'.format(x = x)\n",
    "        else:\n",
    "            out += 'stop words:   {}'.format(x.params[\"count_vectorizer__stop_words\"])\n",
    "        if x.type == 'cooccurrence':\n",
    "            out += 'window_size: {x.window_size}'.format(x = x)\n",
    "        print(out)\n",
    "    fig.suptitle(title, fontsize = fontsize)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top = top)\n",
    "    return fig, ax\n",
    "\n",
    "for name, df_dataset in sorted(df_all.groupby('dataset'), key = lambda x: x[0]):\n",
    "    df_dataset_grouped_by_type = df_dataset.groupby('type')\n",
    "    print(name)\n",
    "    fig, ax = plot_best_by_type(df_all, df_dataset_grouped_by_type, 'Dataset: {}'.format(name))\n",
    "    fig.savefig('tmp/results/dataset-{}-best.png'.format(name), dpi = 150)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_ds = df_all[df_all.type != 'text'].groupby('dataset')\n",
    "\n",
    "df_all.iloc[df_by_ds.mean_test_score.idxmax()]\n",
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/check-w2v-results.json') as f:\n",
    "    w2v_results = json.load(f)\n",
    " \n",
    "per_embedding_type = {}\n",
    "for dataset, value in w2v_results.items():\n",
    "    print(dataset)\n",
    "    for embedding_raw, cache_files in sorted(value.items(), key = lambda x: x[0]):\n",
    "        embedding = embedding_raw.split('/')[-1].rsplit('.', 2)[0]\n",
    "        if len(cache_files.keys()) != 2: continue\n",
    "        print('\\t{}'.format(embedding))\n",
    "        if embedding not in per_embedding_type:\n",
    "            per_embedding_type[embedding] = {}\n",
    "        per_embedding_type[embedding][dataset] = []\n",
    "        for dataset_file, counts in sorted(cache_files.items(), key = lambda x: x[0]):\n",
    "            not_found_ratio = int(counts['counts']['not_found'] / counts['num_labels'] * 100)\n",
    "            if embedding == 'trained' and 'coo' in  dataset_file:\n",
    "                print('Yes', counts['counts']['not_found'], not_found_ratio, '%', counts['not_found_sample'])\n",
    "            is_gml = 'dataset_graph_gml' in dataset_file\n",
    "            per_embedding_type[embedding][dataset].append((is_gml, not_found_ratio))\n",
    "            print('\\t\\t{:4} missing  {:3>}%'.format('gml' if is_gml else 'co', not_found_ratio))\n",
    "        per_embedding_type[embedding][dataset] = per_embedding_type[embedding][dataset][0][1]  #sum(y for x, y in per_embedding_type[embedding][dataset]) / 2\n",
    "df = pd.DataFrame(per_embedding_type)\n",
    "df#[df.index == \"ng20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "currently = ['ling-spam', 'webkb', 'ng20', 'reuters-21578']\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset in currently: continue\n",
    "    print(dataset)\n",
    "    continue\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    dataset_helper.plot_dataset_class_distribution(X, Y, 'Class distribution: {}'.format(dataset))\n",
    "    plt.show()\n",
    "    print('{}\\n#Docs:\\t{}\\t# Classes:\\t{}'.format(dataset, len(X), len(set(Y))))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "38px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
