{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "EXPORT_DPI = 100\n",
    "EXPORT_FIG_SIZE = (8, 4)\n",
    "EXPORT_FIG_SIZE_BIG = (10, 7)\n",
    "EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT = EXPORT_FIG_SIZE\n",
    "EXPORT_FIG_WIDTH_BIG, EXPORT_FIG_HEIGHT_BIG = EXPORT_FIG_SIZE_BIG\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 80\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set('notebook', 'whitegrid', palette = 'deep')\n",
    "plt.rcParams['figure.figsize'] = EXPORT_FIG_SIZE_BIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import graph_helper, dataset_helper\n",
    "import networkx as nx\n",
    "from transformers import fast_wl_pipeline, text_pipeline\n",
    "from transformers.tuple_selector import TupleSelector\n",
    "import sklearn\n",
    "from sklearn import pipeline\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import collections\n",
    "from utils.significance_test_utils import *\n",
    "\n",
    "def get_classifier():\n",
    "    graph_fast_wl_grid_params = {\n",
    "        'fast_wl__h': [5],\n",
    "        'fast_wl__phi_dim': [None],\n",
    "        'fast_wl__round_to_decimals': [10],\n",
    "        'phi_picker__return_iteration': ['stacked'],\n",
    "        'phi_picker__use_zeroth': [False]\n",
    "    }\n",
    "\n",
    "    grid_params_combined = dict({\n",
    "        'classifier': []\n",
    "    }, **dict({'features__fast_wl_pipeline__feature_extraction__' + k: val for k, val in\n",
    "               graph_fast_wl_grid_params.items()}, **dict(\n",
    "        features__fast_wl_pipeline__feature_extraction__fast_wl__phi_dim=[]\n",
    "    )))\n",
    "\n",
    "    combined_features = sklearn.pipeline.FeatureUnion([\n",
    "        ('tfidf', sklearn.pipeline.Pipeline([\n",
    "            ('selector', TupleSelector(tuple_index=1)),\n",
    "            ('tfidf', text_pipeline.get_pipeline()),\n",
    "        ])),\n",
    "        ('fast_wl_pipeline', sklearn.pipeline.Pipeline([\n",
    "            ('selector', TupleSelector(tuple_index=0, v_stack=False)),\n",
    "            ('feature_extraction', fast_wl_pipeline.get_pipeline())\n",
    "        ]))\n",
    "    ], transformer_weights = dict(\n",
    "        tfidf=1,\n",
    "        fast_wl_pipeline=1\n",
    "    ))\n",
    "\n",
    "    pipeline = sklearn.pipeline.Pipeline([\n",
    "        ('features', combined_features),\n",
    "        ('scaler', sklearn.preprocessing.MaxAbsScaler()),\n",
    "        ('classifier', None)\n",
    "    ])\n",
    "    \n",
    "    return pipeline, grid_params_combined\n",
    "\n",
    "cv = sklearn.model_selection.StratifiedKFold(\n",
    "    n_splits=3,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "dataset = 'ling-spam'\n",
    "dataset = 'ng20'\n",
    "#dataset = None\n",
    "for graph_cache_file in dataset_helper.get_all_cached_graph_datasets(dataset):\n",
    "    if 'concept' not in graph_cache_file or 'v2' not in graph_cache_file: continue\n",
    "    #if 'cooccurrence' not in graph_cache_file: continue\n",
    "    print(graph_cache_file)\n",
    "    X_combined, Y_combined = graph_helper.get_filtered_text_graph_dataset(graph_cache_file)\n",
    "\n",
    "    graphs = [g for (g, _, _) in X_combined]\n",
    "    empty_graphs = len([1 for g in graphs if nx.number_of_nodes(g) == 0 or nx.number_of_edges(g) == 0])\n",
    "    num_vertices = sum([nx.number_of_nodes(g) for g in graphs]) + empty_graphs\n",
    "    fast_wl_pipeline.convert_graphs_to_tuples(graphs)\n",
    "    X_combined = [(graph, text) for (graph, text, _) in X_combined]\n",
    "    \n",
    "    clfs, params = get_classifier()\n",
    "    clf = sklearn.linear_model.PassiveAggressiveClassifier(class_weight = 'balanced', max_iter = 10000, verbose = 0, tol = 1e-5)\n",
    "    #clf = sklearn.svm.LinearSVC(class_weight = 'balanced', max_iter = 10000, verbose = 1, tol = 1e-6)\n",
    "    params['classifier'] = [clf]\n",
    "    #params['features__transformer_weights'] = [{'tfidf': 1, 'fast_wl_pipeline': 1}, {'tfidf': 1, 'fast_wl_pipeline': 0}]\n",
    "    #\n",
    "    #params['features__fast_wl_pipeline__feature_extraction__fast_wl__h'] = [1, 10]\n",
    "    params['features__fast_wl_pipeline__feature_extraction__phi_picker__use_zeroth'] = [True, False]\n",
    "    params['features__fast_wl_pipeline__feature_extraction__fast_wl__phi_dim'] = [num_vertices]\n",
    "    \n",
    "    grid = sklearn.model_selection.ParameterGrid(params)\n",
    "    \n",
    "    assert len(grid) == 2\n",
    "    \n",
    "    results = []\n",
    "    for train, test in cv.split(X_combined, Y_combined):\n",
    "        X_train, Y_train, X_test, Y_test = np.array(X_combined)[train], np.array(Y_combined)[train], np.array(X_combined)[test], np.array(Y_combined)[test]\n",
    "        result = Result(Y_test, [])\n",
    "        for params_ in grid:\n",
    "            print('set_params', params_)\n",
    "            clfs.set_params(**params_)\n",
    "            clfs.fit(X_train, Y_train)\n",
    "            Y_pred = clfs.predict(X_test)\n",
    "            result.y_preds.append(Y_pred)\n",
    "        results.append(result)\n",
    "        break\n",
    "    break\n",
    "        \n",
    "        \n",
    "#f1 = sklearn.metrics.f1_score(y_true=Y_test, y_pred=Y_pred, average='macro')\n",
    "#clf_ = clfs.named_steps['classifier']\n",
    "#num_text_features = len(clfs.named_steps['features'].transformer_list[0][1].named_steps['tfidf'].named_steps['TfidfTransformer'].vocabulary_.keys())\n",
    "#num_graph_features = coefs.shape[1] - num_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAILS = 10000\n",
    "for result in results:\n",
    "    for metric_name, metric in metrics:\n",
    "        fig, ax = plot_randomzation_test_distribution(result, metric, metric_name, num_trails=NUM_TRAILS)\n",
    "        fig.savefig('tmp/confidence_{}.png'.format(metric_name))\n",
    "        plt.show()\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_coefficients(classifier, feature_names, top_features=20):\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "\n",
    "    sorted_coefs_graphs = np.argsort(coef[num_text_features:])\n",
    "    top_positive_graph_coefs = sorted_coefs_graphs[-top_features:]\n",
    "    top_negative_graph_coefs = sorted_coefs_graphs[:top_features]\n",
    "    top_graph_coefs = np.hstack([top_negative_graph_coefs, top_positive_graph_coefs])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    colors = ['r' if c < 0 else 'b' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_graph_coefs + num_text_features], color=colors)\n",
    "    plt.title('Graph features')\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    plt.title('All features')\n",
    "    #plt.xticks(top_coefficients, rotation=60, ha='right')\n",
    "    if feature_names is not None:\n",
    "        feature_names = np.array(feature_names)\n",
    "        plt.xticks(np.arange(0, 2 * top_features), [feature_names[x] if x < len(feature_names) else '..' for x in top_coefficients], rotation=60, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('#text features: {}, #graph features: {}'.format(num_text_features, num_graph_features))\n",
    "    \n",
    "coefs = clf_.coef_.ravel()\n",
    "text_features = coefs[:num_text_features]\n",
    "graph_features = coefs[num_text_features:]\n",
    "tfidf_transformer = clfs.named_steps['features'].transformer_list[0][1].named_steps['tfidf'].named_steps['TfidfTransformer']\n",
    "fast_wl_transformer = clfs.named_steps['features'].transformer_list[0]\n",
    "text_feature_names = tfidf_transformer.get_feature_names()\n",
    "\n",
    "plot_coefficients(clf_, text_feature_names, top_features=50)\n",
    "#clfs.named_steps['features'].transformer_list[1][1].named_steps['feature_extraction'].named_steps['phi_picker'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'coefs': coefs1\n",
    "df_graphs = df[df.index >= num_text_features]\n",
    "df_texts = df[df.index < num_text_features]\n",
    "df['type'] = df.index.map(lambda x: 'text' if x < num_text_features else 'graph')\n",
    "fig, ax = plt.subplots()\n",
    "for type_, df_ in df.groupby('type'):\n",
    "    df_.coefs.plot(kind = 'hist', logy = True, bins = 100, ax = ax, label = type_, alpha = 0.8)\n",
    "ax.legend()\n",
    "#df_graphs.plot(kind = 'hist', logy = True, bins = 100)\n",
    "#df_texts.plot(kind = 'hist', logy = True, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dataset_helper\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "data = collections.defaultdict(lambda: [])\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    data['dataset'].append(dataset)\n",
    "    data['# documents'].append(len(X))\n",
    "    data['# classes'].append(len(set(Y)))\n",
    "df = pd.DataFrame(data).set_index('dataset').sort_index()\n",
    "df['ana'] = df.index.str.contains('-ana')\n",
    "df = df[df.ana == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_latex()\n",
    "print(df[df.index != 'small'][['# classes', '# documents']].to_latex().replace('\\\\toprule', '').replace('\\\\bottomrule', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocessing\n",
    "X, Y = dataset_helper.get_dataset('ng20')\n",
    "\n",
    "import unicodedata, re\n",
    "\n",
    "control_chars = '\\x00-\\x1f\\x7f-\\x9f'\n",
    "\n",
    "control_char_re = re.compile('[%s]' % re.escape(control_chars))\n",
    "\n",
    "def remove_non_printable(text):\n",
    "    return control_char_re.sub('', text)\n",
    "\n",
    "NUM_REGEXP = re.compile(r'\\d+')\n",
    "def number_to_placeholder(text):\n",
    "    return NUM_REGEXP.sub(' NUMBER ', text)\n",
    "\n",
    "def preprocess(t):\n",
    "    fns = [remove_non_printable, number_to_placeholder, preprocessing.ana_preprocessing]\n",
    "    for fn in fns:\n",
    "        t = fn(t)\n",
    "    return t\n",
    "\n",
    "for x in X[:10]:\n",
    "    print('*' * 100)\n",
    "    print('\\n' * 2)\n",
    "    print('Before: {}'.format(x))\n",
    "    x = preprocess(x)\n",
    "    print('After: {}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "concept_map = nx.DiGraph()\n",
    "concept_map.add_edge('Caffeine','a mild CNS stimulant', {'label': 'is'})\n",
    "concept_map.add_edge('a mild CNS stimulant','ADHD symptoms', {'label': 'reduces'})\n",
    "concept_map.add_edge('Herbal supplements','ADHD symptoms', {'label': 'are used to treat'})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
