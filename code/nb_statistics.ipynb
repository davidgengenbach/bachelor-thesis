{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(\"notebook/js/notebook\").Notebook.prototype.scroll_to_bottom = function () {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_scroll {\n",
    "    height: 34em !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import dummy\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "from utils import helper\n",
    "from utils import results_helper\n",
    "\n",
    "EXPORT_DPI = 100\n",
    "EXPORT_FIG_SIZE = (8, 4)\n",
    "EXPORT_FIG_SIZE_BIG = (10, 7)\n",
    "EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT = EXPORT_FIG_SIZE\n",
    "EXPORT_FIG_WIDTH_BIG, EXPORT_FIG_HEIGHT_BIG = EXPORT_FIG_SIZE_BIG\n",
    "\n",
    "plt.rcParams['figure.figsize'] = EXPORT_FIG_SIZE_BIG\n",
    "sns.set('notebook', 'whitegrid', palette='deep')\n",
    "\n",
    "pd.options.display.max_rows = 80\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ce4613591f4ec1aa2d5f33dd7fb727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=6)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_all = results_helper.get_results(use_already_loaded=False, exclude_filter = 'relabeled', filter_out_non_complete_datasets = False)\n",
    "#df_all = results_helper.get_results(folder ='2017-10-04_19-22', use_already_loaded=False, exclude_filter = 'relabeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>combined</th>\n",
       "      <th>dataset</th>\n",
       "      <th>filename</th>\n",
       "      <th>is_ana</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>mean_test_precision_macro</th>\n",
       "      <th>mean_test_recall_macro</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_train_f1_macro</th>\n",
       "      <th>mean_train_precision_macro</th>\n",
       "      <th>mean_train_recall_macro</th>\n",
       "      <th>param_TfidfTransformer__stop_words</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>params</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>std_test_f1_macro</th>\n",
       "      <th>std_test_precision_macro</th>\n",
       "      <th>std_test_recall_macro</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "      <th>std_train_f1_macro</th>\n",
       "      <th>std_train_precision_macro</th>\n",
       "      <th>std_train_recall_macro</th>\n",
       "      <th>type</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>ling-spam</td>\n",
       "      <td>result___ling-spam__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.322499</td>\n",
       "      <td>4.862954</td>\n",
       "      <td>0.839959</td>\n",
       "      <td>0.492262</td>\n",
       "      <td>0.919479</td>\n",
       "      <td>0.518711</td>\n",
       "      <td>0.846872</td>\n",
       "      <td>0.531096</td>\n",
       "      <td>0.922418</td>\n",
       "      <td>0.539505</td>\n",
       "      <td>None</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.154692</td>\n",
       "      <td>0.274132</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.008829</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>ling-spam</td>\n",
       "      <td>result___ling-spam__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.418963</td>\n",
       "      <td>4.502319</td>\n",
       "      <td>0.992395</td>\n",
       "      <td>0.986103</td>\n",
       "      <td>0.992875</td>\n",
       "      <td>0.979631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}</td>\n",
       "      <td>0.056530</td>\n",
       "      <td>0.155933</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>ling-spam</td>\n",
       "      <td>result___ling-spam__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.689906</td>\n",
       "      <td>4.525297</td>\n",
       "      <td>0.995506</td>\n",
       "      <td>0.991843</td>\n",
       "      <td>0.994803</td>\n",
       "      <td>0.988981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.478518</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>ling-spam</td>\n",
       "      <td>result___ling-spam__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.533967</td>\n",
       "      <td>4.574579</td>\n",
       "      <td>0.892153</td>\n",
       "      <td>0.728422</td>\n",
       "      <td>0.942776</td>\n",
       "      <td>0.675711</td>\n",
       "      <td>0.923435</td>\n",
       "      <td>0.828112</td>\n",
       "      <td>0.957966</td>\n",
       "      <td>0.769737</td>\n",
       "      <td>english</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.139399</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.009514</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>0.028219</td>\n",
       "      <td>0.006119</td>\n",
       "      <td>0.017070</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.018538</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>ling-spam</td>\n",
       "      <td>result___ling-spam__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.252536</td>\n",
       "      <td>3.885462</td>\n",
       "      <td>0.992741</td>\n",
       "      <td>0.986660</td>\n",
       "      <td>0.993982</td>\n",
       "      <td>0.979822</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>english</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}</td>\n",
       "      <td>0.118175</td>\n",
       "      <td>0.318478</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>ling-spam</td>\n",
       "      <td>result___ling-spam__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.156979</td>\n",
       "      <td>3.447291</td>\n",
       "      <td>0.994815</td>\n",
       "      <td>0.990581</td>\n",
       "      <td>0.991930</td>\n",
       "      <td>0.989384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>english</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.167043</td>\n",
       "      <td>0.086252</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.004175</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.008708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>7.498418</td>\n",
       "      <td>19.419134</td>\n",
       "      <td>0.840391</td>\n",
       "      <td>0.819746</td>\n",
       "      <td>0.873994</td>\n",
       "      <td>0.822632</td>\n",
       "      <td>0.912944</td>\n",
       "      <td>0.894409</td>\n",
       "      <td>0.931799</td>\n",
       "      <td>0.896283</td>\n",
       "      <td>None</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.334325</td>\n",
       "      <td>0.975790</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>15.849879</td>\n",
       "      <td>15.800766</td>\n",
       "      <td>0.921949</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>0.921666</td>\n",
       "      <td>0.919201</td>\n",
       "      <td>0.998619</td>\n",
       "      <td>0.998617</td>\n",
       "      <td>0.998633</td>\n",
       "      <td>0.998604</td>\n",
       "      <td>None</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}</td>\n",
       "      <td>0.364095</td>\n",
       "      <td>0.955036</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>13.258274</td>\n",
       "      <td>16.482947</td>\n",
       "      <td>0.917008</td>\n",
       "      <td>0.915265</td>\n",
       "      <td>0.916646</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999421</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.999435</td>\n",
       "      <td>None</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.159861</td>\n",
       "      <td>0.926188</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>7.441572</td>\n",
       "      <td>16.426196</td>\n",
       "      <td>0.873652</td>\n",
       "      <td>0.857881</td>\n",
       "      <td>0.887894</td>\n",
       "      <td>0.859612</td>\n",
       "      <td>0.941502</td>\n",
       "      <td>0.930483</td>\n",
       "      <td>0.947497</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>english</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.209927</td>\n",
       "      <td>0.463736</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>11.004735</td>\n",
       "      <td>13.707918</td>\n",
       "      <td>0.921736</td>\n",
       "      <td>0.920062</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>0.919336</td>\n",
       "      <td>0.998751</td>\n",
       "      <td>0.998756</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.998746</td>\n",
       "      <td>english</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}</td>\n",
       "      <td>0.507767</td>\n",
       "      <td>0.176502</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>9.422088</td>\n",
       "      <td>13.339632</td>\n",
       "      <td>0.916636</td>\n",
       "      <td>0.914925</td>\n",
       "      <td>0.916161</td>\n",
       "      <td>0.914333</td>\n",
       "      <td>0.999389</td>\n",
       "      <td>0.999396</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>english</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.361014</td>\n",
       "      <td>0.140053</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.810751</td>\n",
       "      <td>15.157657</td>\n",
       "      <td>0.661254</td>\n",
       "      <td>0.637286</td>\n",
       "      <td>0.752431</td>\n",
       "      <td>0.638553</td>\n",
       "      <td>0.804997</td>\n",
       "      <td>0.781758</td>\n",
       "      <td>0.880473</td>\n",
       "      <td>0.781921</td>\n",
       "      <td>None</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.478153</td>\n",
       "      <td>0.721478</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>0.022281</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.460817</td>\n",
       "      <td>12.545869</td>\n",
       "      <td>0.751831</td>\n",
       "      <td>0.743999</td>\n",
       "      <td>0.750632</td>\n",
       "      <td>0.742460</td>\n",
       "      <td>0.971719</td>\n",
       "      <td>0.974428</td>\n",
       "      <td>0.981018</td>\n",
       "      <td>0.971465</td>\n",
       "      <td>None</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>0.780496</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.649528</td>\n",
       "      <td>13.152813</td>\n",
       "      <td>0.733206</td>\n",
       "      <td>0.725342</td>\n",
       "      <td>0.732044</td>\n",
       "      <td>0.723935</td>\n",
       "      <td>0.972382</td>\n",
       "      <td>0.975348</td>\n",
       "      <td>0.982292</td>\n",
       "      <td>0.972240</td>\n",
       "      <td>None</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.508435</td>\n",
       "      <td>0.650435</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.857727</td>\n",
       "      <td>13.933661</td>\n",
       "      <td>0.719994</td>\n",
       "      <td>0.692343</td>\n",
       "      <td>0.767655</td>\n",
       "      <td>0.699739</td>\n",
       "      <td>0.862888</td>\n",
       "      <td>0.844237</td>\n",
       "      <td>0.892882</td>\n",
       "      <td>0.845615</td>\n",
       "      <td>english</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.032257</td>\n",
       "      <td>1.094810</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.011493</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.000844</td>\n",
       "      <td>10.226442</td>\n",
       "      <td>0.753104</td>\n",
       "      <td>0.745213</td>\n",
       "      <td>0.752201</td>\n",
       "      <td>0.743713</td>\n",
       "      <td>0.971029</td>\n",
       "      <td>0.973888</td>\n",
       "      <td>0.980728</td>\n",
       "      <td>0.970826</td>\n",
       "      <td>english</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}</td>\n",
       "      <td>0.778216</td>\n",
       "      <td>0.711553</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>ng20</td>\n",
       "      <td>result___ng20__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.996374</td>\n",
       "      <td>9.740768</td>\n",
       "      <td>0.737610</td>\n",
       "      <td>0.729888</td>\n",
       "      <td>0.736426</td>\n",
       "      <td>0.728484</td>\n",
       "      <td>0.971480</td>\n",
       "      <td>0.974562</td>\n",
       "      <td>0.981730</td>\n",
       "      <td>0.971364</td>\n",
       "      <td>english</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.665647</td>\n",
       "      <td>0.901312</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>reuters-21578</td>\n",
       "      <td>result___reuters-21578__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.984442</td>\n",
       "      <td>14.102912</td>\n",
       "      <td>0.556197</td>\n",
       "      <td>0.039903</td>\n",
       "      <td>0.057021</td>\n",
       "      <td>0.045365</td>\n",
       "      <td>0.579533</td>\n",
       "      <td>0.044355</td>\n",
       "      <td>0.076181</td>\n",
       "      <td>0.049652</td>\n",
       "      <td>None</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.365475</td>\n",
       "      <td>0.706276</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>reuters-21578</td>\n",
       "      <td>result___reuters-21578__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.934645</td>\n",
       "      <td>7.019310</td>\n",
       "      <td>0.686525</td>\n",
       "      <td>0.356152</td>\n",
       "      <td>0.349373</td>\n",
       "      <td>0.386565</td>\n",
       "      <td>0.840188</td>\n",
       "      <td>0.665007</td>\n",
       "      <td>0.612957</td>\n",
       "      <td>0.830827</td>\n",
       "      <td>None</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}</td>\n",
       "      <td>1.729896</td>\n",
       "      <td>0.298428</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>0.008580</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.013430</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>reuters-21578</td>\n",
       "      <td>result___reuters-21578__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.820983</td>\n",
       "      <td>7.865394</td>\n",
       "      <td>0.671669</td>\n",
       "      <td>0.343279</td>\n",
       "      <td>0.339568</td>\n",
       "      <td>0.406724</td>\n",
       "      <td>0.832868</td>\n",
       "      <td>0.626270</td>\n",
       "      <td>0.589555</td>\n",
       "      <td>0.790931</td>\n",
       "      <td>None</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.982253</td>\n",
       "      <td>0.977216</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.014853</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>0.017292</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.007272</td>\n",
       "      <td>0.009196</td>\n",
       "      <td>0.014428</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>reuters-21578</td>\n",
       "      <td>result___reuters-21578__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.380662</td>\n",
       "      <td>13.197366</td>\n",
       "      <td>0.590036</td>\n",
       "      <td>0.048861</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.055593</td>\n",
       "      <td>0.616639</td>\n",
       "      <td>0.055635</td>\n",
       "      <td>0.077876</td>\n",
       "      <td>0.061704</td>\n",
       "      <td>english</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.478151</td>\n",
       "      <td>0.330112</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>reuters-21578</td>\n",
       "      <td>result___reuters-21578__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.422208</td>\n",
       "      <td>5.659060</td>\n",
       "      <td>0.686375</td>\n",
       "      <td>0.355446</td>\n",
       "      <td>0.348107</td>\n",
       "      <td>0.386546</td>\n",
       "      <td>0.840900</td>\n",
       "      <td>0.665221</td>\n",
       "      <td>0.613221</td>\n",
       "      <td>0.830849</td>\n",
       "      <td>english</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}</td>\n",
       "      <td>2.099469</td>\n",
       "      <td>0.047542</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.009157</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.013414</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>reuters-21578</td>\n",
       "      <td>result___reuters-21578__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.375702</td>\n",
       "      <td>6.652930</td>\n",
       "      <td>0.673694</td>\n",
       "      <td>0.343610</td>\n",
       "      <td>0.337055</td>\n",
       "      <td>0.407096</td>\n",
       "      <td>0.835723</td>\n",
       "      <td>0.634001</td>\n",
       "      <td>0.595249</td>\n",
       "      <td>0.798483</td>\n",
       "      <td>english</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.934052</td>\n",
       "      <td>0.407804</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.016899</td>\n",
       "      <td>0.011642</td>\n",
       "      <td>0.026313</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.013097</td>\n",
       "      <td>0.009024</td>\n",
       "      <td>0.015372</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.857739</td>\n",
       "      <td>1.836620</td>\n",
       "      <td>0.783829</td>\n",
       "      <td>0.635061</td>\n",
       "      <td>0.851505</td>\n",
       "      <td>0.664772</td>\n",
       "      <td>0.833852</td>\n",
       "      <td>0.714770</td>\n",
       "      <td>0.883683</td>\n",
       "      <td>0.724973</td>\n",
       "      <td>None</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.022693</td>\n",
       "      <td>0.168132</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>1.172179</td>\n",
       "      <td>1.828050</td>\n",
       "      <td>0.908109</td>\n",
       "      <td>0.895440</td>\n",
       "      <td>0.899961</td>\n",
       "      <td>0.891719</td>\n",
       "      <td>0.996282</td>\n",
       "      <td>0.996027</td>\n",
       "      <td>0.996044</td>\n",
       "      <td>0.996032</td>\n",
       "      <td>None</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}</td>\n",
       "      <td>0.256079</td>\n",
       "      <td>0.078709</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.011156</td>\n",
       "      <td>0.008195</td>\n",
       "      <td>0.014112</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>1.214120</td>\n",
       "      <td>1.959098</td>\n",
       "      <td>0.897793</td>\n",
       "      <td>0.884318</td>\n",
       "      <td>0.891091</td>\n",
       "      <td>0.878673</td>\n",
       "      <td>0.998081</td>\n",
       "      <td>0.997833</td>\n",
       "      <td>0.997926</td>\n",
       "      <td>0.997752</td>\n",
       "      <td>None</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.083583</td>\n",
       "      <td>0.108080</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>1.146721</td>\n",
       "      <td>2.107750</td>\n",
       "      <td>0.784069</td>\n",
       "      <td>0.636867</td>\n",
       "      <td>0.850918</td>\n",
       "      <td>0.665965</td>\n",
       "      <td>0.834212</td>\n",
       "      <td>0.715342</td>\n",
       "      <td>0.883620</td>\n",
       "      <td>0.725597</td>\n",
       "      <td>english</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.177006</td>\n",
       "      <td>0.079244</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.004786</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.003136</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>1.069056</td>\n",
       "      <td>1.802142</td>\n",
       "      <td>0.904511</td>\n",
       "      <td>0.891041</td>\n",
       "      <td>0.895481</td>\n",
       "      <td>0.887587</td>\n",
       "      <td>0.996282</td>\n",
       "      <td>0.996027</td>\n",
       "      <td>0.996044</td>\n",
       "      <td>0.996032</td>\n",
       "      <td>english</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}</td>\n",
       "      <td>0.044027</td>\n",
       "      <td>0.084423</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>0.010919</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb-ana__text.npy</td>\n",
       "      <td>True</td>\n",
       "      <td>1.049074</td>\n",
       "      <td>1.756304</td>\n",
       "      <td>0.893954</td>\n",
       "      <td>0.878622</td>\n",
       "      <td>0.887191</td>\n",
       "      <td>0.871814</td>\n",
       "      <td>0.997961</td>\n",
       "      <td>0.997725</td>\n",
       "      <td>0.997999</td>\n",
       "      <td>0.997468</td>\n",
       "      <td>english</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.013223</td>\n",
       "      <td>0.173041</td>\n",
       "      <td>0.009794</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>0.015404</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.575297</td>\n",
       "      <td>10.497942</td>\n",
       "      <td>0.486464</td>\n",
       "      <td>0.142642</td>\n",
       "      <td>0.436221</td>\n",
       "      <td>0.170768</td>\n",
       "      <td>0.518673</td>\n",
       "      <td>0.183932</td>\n",
       "      <td>0.478554</td>\n",
       "      <td>0.196524</td>\n",
       "      <td>None</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>0.236580</td>\n",
       "      <td>0.301671</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.025487</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.456896</td>\n",
       "      <td>9.874100</td>\n",
       "      <td>0.792482</td>\n",
       "      <td>0.692390</td>\n",
       "      <td>0.710347</td>\n",
       "      <td>0.697294</td>\n",
       "      <td>0.980361</td>\n",
       "      <td>0.979800</td>\n",
       "      <td>0.971785</td>\n",
       "      <td>0.988187</td>\n",
       "      <td>None</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}</td>\n",
       "      <td>1.065123</td>\n",
       "      <td>0.689088</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>0.023704</td>\n",
       "      <td>0.027594</td>\n",
       "      <td>0.017868</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.496310</td>\n",
       "      <td>9.484221</td>\n",
       "      <td>0.772057</td>\n",
       "      <td>0.688349</td>\n",
       "      <td>0.711762</td>\n",
       "      <td>0.674154</td>\n",
       "      <td>0.995286</td>\n",
       "      <td>0.993219</td>\n",
       "      <td>0.993242</td>\n",
       "      <td>0.993224</td>\n",
       "      <td>None</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.542244</td>\n",
       "      <td>0.610434</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>0.018810</td>\n",
       "      <td>0.008307</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.476050</td>\n",
       "      <td>9.675917</td>\n",
       "      <td>0.495166</td>\n",
       "      <td>0.154166</td>\n",
       "      <td>0.442327</td>\n",
       "      <td>0.177712</td>\n",
       "      <td>0.538373</td>\n",
       "      <td>0.207740</td>\n",
       "      <td>0.483627</td>\n",
       "      <td>0.212943</td>\n",
       "      <td>english</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}</td>\n",
       "      <td>1.110260</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.028426</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.141648</td>\n",
       "      <td>7.509450</td>\n",
       "      <td>0.786198</td>\n",
       "      <td>0.690358</td>\n",
       "      <td>0.711778</td>\n",
       "      <td>0.692851</td>\n",
       "      <td>0.981207</td>\n",
       "      <td>0.980180</td>\n",
       "      <td>0.972395</td>\n",
       "      <td>0.988316</td>\n",
       "      <td>english</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}</td>\n",
       "      <td>0.151729</td>\n",
       "      <td>0.199376</td>\n",
       "      <td>0.012348</td>\n",
       "      <td>0.029134</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>False</td>\n",
       "      <td>webkb</td>\n",
       "      <td>result___webkb__text.npy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.431739</td>\n",
       "      <td>7.744815</td>\n",
       "      <td>0.764201</td>\n",
       "      <td>0.671986</td>\n",
       "      <td>0.710232</td>\n",
       "      <td>0.651283</td>\n",
       "      <td>0.995407</td>\n",
       "      <td>0.993643</td>\n",
       "      <td>0.994050</td>\n",
       "      <td>0.993262</td>\n",
       "      <td>english</td>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)</td>\n",
       "      <td>{'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}</td>\n",
       "      <td>0.415513</td>\n",
       "      <td>0.249070</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>text</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  combined        dataset  \\\n",
       "0   MultinomialNB                False     ling-spam       \n",
       "1   LinearSVC                    False     ling-spam       \n",
       "2   PassiveAggressiveClassifier  False     ling-spam       \n",
       "3   MultinomialNB                False     ling-spam       \n",
       "4   LinearSVC                    False     ling-spam       \n",
       "5   PassiveAggressiveClassifier  False     ling-spam       \n",
       "6   MultinomialNB                False     ng20            \n",
       "7   LinearSVC                    False     ng20            \n",
       "8   PassiveAggressiveClassifier  False     ng20            \n",
       "9   MultinomialNB                False     ng20            \n",
       "10  LinearSVC                    False     ng20            \n",
       "11  PassiveAggressiveClassifier  False     ng20            \n",
       "12  MultinomialNB                False     ng20            \n",
       "13  LinearSVC                    False     ng20            \n",
       "14  PassiveAggressiveClassifier  False     ng20            \n",
       "15  MultinomialNB                False     ng20            \n",
       "16  LinearSVC                    False     ng20            \n",
       "17  PassiveAggressiveClassifier  False     ng20            \n",
       "18  MultinomialNB                False     reuters-21578   \n",
       "19  LinearSVC                    False     reuters-21578   \n",
       "20  PassiveAggressiveClassifier  False     reuters-21578   \n",
       "21  MultinomialNB                False     reuters-21578   \n",
       "22  LinearSVC                    False     reuters-21578   \n",
       "23  PassiveAggressiveClassifier  False     reuters-21578   \n",
       "24  MultinomialNB                False     webkb           \n",
       "25  LinearSVC                    False     webkb           \n",
       "26  PassiveAggressiveClassifier  False     webkb           \n",
       "27  MultinomialNB                False     webkb           \n",
       "28  LinearSVC                    False     webkb           \n",
       "29  PassiveAggressiveClassifier  False     webkb           \n",
       "30  MultinomialNB                False     webkb           \n",
       "31  LinearSVC                    False     webkb           \n",
       "32  PassiveAggressiveClassifier  False     webkb           \n",
       "33  MultinomialNB                False     webkb           \n",
       "34  LinearSVC                    False     webkb           \n",
       "35  PassiveAggressiveClassifier  False     webkb           \n",
       "\n",
       "                            filename is_ana  mean_fit_time  mean_score_time  \\\n",
       "0   result___ling-spam__text.npy      NaN    2.322499       4.862954          \n",
       "1   result___ling-spam__text.npy      NaN    2.418963       4.502319          \n",
       "2   result___ling-spam__text.npy      NaN    2.689906       4.525297          \n",
       "3   result___ling-spam__text.npy      NaN    2.533967       4.574579          \n",
       "4   result___ling-spam__text.npy      NaN    2.252536       3.885462          \n",
       "5   result___ling-spam__text.npy      NaN    2.156979       3.447291          \n",
       "6   result___ng20-ana__text.npy       True   7.498418       19.419134         \n",
       "7   result___ng20-ana__text.npy       True   15.849879      15.800766         \n",
       "8   result___ng20-ana__text.npy       True   13.258274      16.482947         \n",
       "9   result___ng20-ana__text.npy       True   7.441572       16.426196         \n",
       "10  result___ng20-ana__text.npy       True   11.004735      13.707918         \n",
       "11  result___ng20-ana__text.npy       True   9.422088       13.339632         \n",
       "12  result___ng20__text.npy           NaN    6.810751       15.157657         \n",
       "13  result___ng20__text.npy           NaN    13.460817      12.545869         \n",
       "14  result___ng20__text.npy           NaN    11.649528      13.152813         \n",
       "15  result___ng20__text.npy           NaN    6.857727       13.933661         \n",
       "16  result___ng20__text.npy           NaN    11.000844      10.226442         \n",
       "17  result___ng20__text.npy           NaN    8.996374       9.740768          \n",
       "18  result___reuters-21578__text.npy  NaN    3.984442       14.102912         \n",
       "19  result___reuters-21578__text.npy  NaN    24.934645      7.019310          \n",
       "20  result___reuters-21578__text.npy  NaN    13.820983      7.865394          \n",
       "21  result___reuters-21578__text.npy  NaN    4.380662       13.197366         \n",
       "22  result___reuters-21578__text.npy  NaN    21.422208      5.659060          \n",
       "23  result___reuters-21578__text.npy  NaN    10.375702      6.652930          \n",
       "24  result___webkb-ana__text.npy      True   0.857739       1.836620          \n",
       "25  result___webkb-ana__text.npy      True   1.172179       1.828050          \n",
       "26  result___webkb-ana__text.npy      True   1.214120       1.959098          \n",
       "27  result___webkb-ana__text.npy      True   1.146721       2.107750          \n",
       "28  result___webkb-ana__text.npy      True   1.069056       1.802142          \n",
       "29  result___webkb-ana__text.npy      True   1.049074       1.756304          \n",
       "30  result___webkb__text.npy          NaN    4.575297       10.497942         \n",
       "31  result___webkb__text.npy          NaN    7.456896       9.874100          \n",
       "32  result___webkb__text.npy          NaN    6.496310       9.484221          \n",
       "33  result___webkb__text.npy          NaN    5.476050       9.675917          \n",
       "34  result___webkb__text.npy          NaN    6.141648       7.509450          \n",
       "35  result___webkb__text.npy          NaN    5.431739       7.744815          \n",
       "\n",
       "    mean_test_accuracy  mean_test_f1_macro  mean_test_precision_macro  \\\n",
       "0   0.839959            0.492262            0.919479                    \n",
       "1   0.992395            0.986103            0.992875                    \n",
       "2   0.995506            0.991843            0.994803                    \n",
       "3   0.892153            0.728422            0.942776                    \n",
       "4   0.992741            0.986660            0.993982                    \n",
       "5   0.994815            0.990581            0.991930                    \n",
       "6   0.840391            0.819746            0.873994                    \n",
       "7   0.921949            0.920055            0.921666                    \n",
       "8   0.917008            0.915265            0.916646                    \n",
       "9   0.873652            0.857881            0.887894                    \n",
       "10  0.921736            0.920062            0.921429                    \n",
       "11  0.916636            0.914925            0.916161                    \n",
       "12  0.661254            0.637286            0.752431                    \n",
       "13  0.751831            0.743999            0.750632                    \n",
       "14  0.733206            0.725342            0.732044                    \n",
       "15  0.719994            0.692343            0.767655                    \n",
       "16  0.753104            0.745213            0.752201                    \n",
       "17  0.737610            0.729888            0.736426                    \n",
       "18  0.556197            0.039903            0.057021                    \n",
       "19  0.686525            0.356152            0.349373                    \n",
       "20  0.671669            0.343279            0.339568                    \n",
       "21  0.590036            0.048861            0.068646                    \n",
       "22  0.686375            0.355446            0.348107                    \n",
       "23  0.673694            0.343610            0.337055                    \n",
       "24  0.783829            0.635061            0.851505                    \n",
       "25  0.908109            0.895440            0.899961                    \n",
       "26  0.897793            0.884318            0.891091                    \n",
       "27  0.784069            0.636867            0.850918                    \n",
       "28  0.904511            0.891041            0.895481                    \n",
       "29  0.893954            0.878622            0.887191                    \n",
       "30  0.486464            0.142642            0.436221                    \n",
       "31  0.792482            0.692390            0.710347                    \n",
       "32  0.772057            0.688349            0.711762                    \n",
       "33  0.495166            0.154166            0.442327                    \n",
       "34  0.786198            0.690358            0.711778                    \n",
       "35  0.764201            0.671986            0.710232                    \n",
       "\n",
       "    mean_test_recall_macro  mean_train_accuracy  mean_train_f1_macro  \\\n",
       "0   0.518711                0.846872             0.531096              \n",
       "1   0.979631                1.000000             1.000000              \n",
       "2   0.988981                1.000000             1.000000              \n",
       "3   0.675711                0.923435             0.828112              \n",
       "4   0.979822                1.000000             1.000000              \n",
       "5   0.989384                1.000000             1.000000              \n",
       "6   0.822632                0.912944             0.894409              \n",
       "7   0.919201                0.998619             0.998617              \n",
       "8   0.914634                0.999416             0.999421              \n",
       "9   0.859612                0.941502             0.930483              \n",
       "10  0.919336                0.998751             0.998756              \n",
       "11  0.914333                0.999389             0.999396              \n",
       "12  0.638553                0.804997             0.781758              \n",
       "13  0.742460                0.971719             0.974428              \n",
       "14  0.723935                0.972382             0.975348              \n",
       "15  0.699739                0.862888             0.844237              \n",
       "16  0.743713                0.971029             0.973888              \n",
       "17  0.728484                0.971480             0.974562              \n",
       "18  0.045365                0.579533             0.044355              \n",
       "19  0.386565                0.840188             0.665007              \n",
       "20  0.406724                0.832868             0.626270              \n",
       "21  0.055593                0.616639             0.055635              \n",
       "22  0.386546                0.840900             0.665221              \n",
       "23  0.407096                0.835723             0.634001              \n",
       "24  0.664772                0.833852             0.714770              \n",
       "25  0.891719                0.996282             0.996027              \n",
       "26  0.878673                0.998081             0.997833              \n",
       "27  0.665965                0.834212             0.715342              \n",
       "28  0.887587                0.996282             0.996027              \n",
       "29  0.871814                0.997961             0.997725              \n",
       "30  0.170768                0.518673             0.183932              \n",
       "31  0.697294                0.980361             0.979800              \n",
       "32  0.674154                0.995286             0.993219              \n",
       "33  0.177712                0.538373             0.207740              \n",
       "34  0.692851                0.981207             0.980180              \n",
       "35  0.651283                0.995407             0.993643              \n",
       "\n",
       "    mean_train_precision_macro  mean_train_recall_macro  \\\n",
       "0   0.922418                    0.539505                  \n",
       "1   1.000000                    1.000000                  \n",
       "2   1.000000                    1.000000                  \n",
       "3   0.957966                    0.769737                  \n",
       "4   1.000000                    1.000000                  \n",
       "5   1.000000                    1.000000                  \n",
       "6   0.931799                    0.896283                  \n",
       "7   0.998633                    0.998604                  \n",
       "8   0.999408                    0.999435                  \n",
       "9   0.947497                    0.930000                  \n",
       "10  0.998768                    0.998746                  \n",
       "11  0.999385                    0.999407                  \n",
       "12  0.880473                    0.781921                  \n",
       "13  0.981018                    0.971465                  \n",
       "14  0.982292                    0.972240                  \n",
       "15  0.892882                    0.845615                  \n",
       "16  0.980728                    0.970826                  \n",
       "17  0.981730                    0.971364                  \n",
       "18  0.076181                    0.049652                  \n",
       "19  0.612957                    0.830827                  \n",
       "20  0.589555                    0.790931                  \n",
       "21  0.077876                    0.061704                  \n",
       "22  0.613221                    0.830849                  \n",
       "23  0.595249                    0.798483                  \n",
       "24  0.883683                    0.724973                  \n",
       "25  0.996044                    0.996032                  \n",
       "26  0.997926                    0.997752                  \n",
       "27  0.883620                    0.725597                  \n",
       "28  0.996044                    0.996032                  \n",
       "29  0.997999                    0.997468                  \n",
       "30  0.478554                    0.196524                  \n",
       "31  0.971785                    0.988187                  \n",
       "32  0.993242                    0.993224                  \n",
       "33  0.483627                    0.212943                  \n",
       "34  0.972395                    0.988316                  \n",
       "35  0.994050                    0.993262                  \n",
       "\n",
       "   param_TfidfTransformer__stop_words  \\\n",
       "0   None                                \n",
       "1   None                                \n",
       "2   None                                \n",
       "3   english                             \n",
       "4   english                             \n",
       "5   english                             \n",
       "6   None                                \n",
       "7   None                                \n",
       "8   None                                \n",
       "9   english                             \n",
       "10  english                             \n",
       "11  english                             \n",
       "12  None                                \n",
       "13  None                                \n",
       "14  None                                \n",
       "15  english                             \n",
       "16  english                             \n",
       "17  english                             \n",
       "18  None                                \n",
       "19  None                                \n",
       "20  None                                \n",
       "21  english                             \n",
       "22  english                             \n",
       "23  english                             \n",
       "24  None                                \n",
       "25  None                                \n",
       "26  None                                \n",
       "27  english                             \n",
       "28  english                             \n",
       "29  english                             \n",
       "30  None                                \n",
       "31  None                                \n",
       "32  None                                \n",
       "33  english                             \n",
       "34  english                             \n",
       "35  english                             \n",
       "\n",
       "                                                                                                                                                                                                                                                             param_classifier  \\\n",
       "0   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "1   LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "2   PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "3   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "4   LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "5   PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "6   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "7   LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "8   PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "9   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "10  LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "11  PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "12  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "13  LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "14  PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "15  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "16  LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "17  PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "18  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "19  LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "20  PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "21  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "22  LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "23  PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "24  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "25  LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "26  PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "27  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "28  LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "29  PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "30  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "31  LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "32  PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "33  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)                                                                                                                                                                                                                  \n",
       "34  LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0006,\\n     verbose=0)                                             \n",
       "35  PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',\\n              fit_intercept=True, loss='hinge', max_iter=1000, n_iter=None,\\n              n_jobs=1, random_state=None, shuffle=True, tol=0.0006,\\n              verbose=0, warm_start=False)   \n",
       "\n",
       "                                                                                      params  \\\n",
       "0   {'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}                      \n",
       "1   {'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}                          \n",
       "2   {'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}        \n",
       "3   {'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}                 \n",
       "4   {'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}                     \n",
       "5   {'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}   \n",
       "6   {'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}                      \n",
       "7   {'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}                          \n",
       "8   {'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}        \n",
       "9   {'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}                 \n",
       "10  {'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}                     \n",
       "11  {'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}   \n",
       "12  {'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}                      \n",
       "13  {'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}                          \n",
       "14  {'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}        \n",
       "15  {'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}                 \n",
       "16  {'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}                     \n",
       "17  {'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}   \n",
       "18  {'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}                      \n",
       "19  {'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}                          \n",
       "20  {'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}        \n",
       "21  {'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}                 \n",
       "22  {'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}                     \n",
       "23  {'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}   \n",
       "24  {'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}                      \n",
       "25  {'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}                          \n",
       "26  {'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}        \n",
       "27  {'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}                 \n",
       "28  {'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}                     \n",
       "29  {'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}   \n",
       "30  {'TfidfTransformer__stop_words': None, 'classifier': 'MultinomialNB'}                      \n",
       "31  {'TfidfTransformer__stop_words': None, 'classifier': 'LinearSVC'}                          \n",
       "32  {'TfidfTransformer__stop_words': None, 'classifier': 'PassiveAggressiveClassifier'}        \n",
       "33  {'TfidfTransformer__stop_words': 'english', 'classifier': 'MultinomialNB'}                 \n",
       "34  {'TfidfTransformer__stop_words': 'english', 'classifier': 'LinearSVC'}                     \n",
       "35  {'TfidfTransformer__stop_words': 'english', 'classifier': 'PassiveAggressiveClassifier'}   \n",
       "\n",
       "    std_fit_time  std_score_time  std_test_accuracy  std_test_f1_macro  \\\n",
       "0   0.154692      0.274132        0.000941           0.004956            \n",
       "1   0.056530      0.155933        0.000976           0.001756            \n",
       "2   0.013449      0.478518        0.000490           0.000940            \n",
       "3   0.139399      0.373014        0.009514           0.032599            \n",
       "4   0.118175      0.318478        0.002935           0.005554            \n",
       "5   0.167043      0.086252        0.002242           0.004175            \n",
       "6   0.334325      0.975790        0.000583           0.001435            \n",
       "7   0.364095      0.955036        0.001063           0.001504            \n",
       "8   0.159861      0.926188        0.001346           0.001415            \n",
       "9   0.209927      0.463736        0.001815           0.002332            \n",
       "10  0.507767      0.176502        0.001225           0.000900            \n",
       "11  0.361014      0.140053        0.001300           0.001552            \n",
       "12  0.478153      0.721478        0.006871           0.006354            \n",
       "13  0.397600      0.780496        0.002381           0.002773            \n",
       "14  0.508435      0.650435        0.001848           0.002355            \n",
       "15  0.032257      1.094810        0.003028           0.002111            \n",
       "16  0.778216      0.711553        0.002062           0.002826            \n",
       "17  0.665647      0.901312        0.002303           0.001403            \n",
       "18  0.365475      0.706276        0.002956           0.000887            \n",
       "19  1.729896      0.298428        0.001464           0.007989            \n",
       "20  0.982253      0.977216        0.002971           0.014853            \n",
       "21  0.478151      0.330112        0.003833           0.001404            \n",
       "22  2.099469      0.047542        0.002503           0.008677            \n",
       "23  0.934052      0.407804        0.002812           0.016899            \n",
       "24  0.022693      0.168132        0.004601           0.004580            \n",
       "25  0.256079      0.078709        0.008847           0.011156            \n",
       "26  0.083583      0.108080        0.010840           0.013235            \n",
       "27  0.177006      0.079244        0.004146           0.005905            \n",
       "28  0.044027      0.084423        0.008514           0.010919            \n",
       "29  0.013223      0.173041        0.009794           0.012070            \n",
       "30  0.236580      0.301671        0.003930           0.004272            \n",
       "31  1.065123      0.689088        0.010859           0.023704            \n",
       "32  0.542244      0.610434        0.006946           0.012470            \n",
       "33  1.110260      0.711538        0.006751           0.007359            \n",
       "34  0.151729      0.199376        0.012348           0.029134            \n",
       "35  0.415513      0.249070        0.006428           0.008701            \n",
       "\n",
       "    std_test_precision_macro  std_test_recall_macro  std_train_accuracy  \\\n",
       "0   0.000413                  0.002552               0.001737             \n",
       "1   0.002637                  0.001498               0.000000             \n",
       "2   0.001802                  0.003406               0.000000             \n",
       "3   0.004519                  0.028219               0.006119             \n",
       "4   0.000516                  0.010036               0.000000             \n",
       "5   0.000762                  0.008708               0.000000             \n",
       "6   0.003210                  0.001018               0.001844             \n",
       "7   0.001095                  0.001772               0.000271             \n",
       "8   0.000822                  0.001851               0.000164             \n",
       "9   0.001245                  0.001847               0.000797             \n",
       "10  0.000862                  0.001130               0.000135             \n",
       "11  0.001313                  0.001781               0.000135             \n",
       "12  0.022281                  0.006844               0.003828             \n",
       "13  0.003556                  0.002309               0.001062             \n",
       "14  0.002309                  0.002116               0.001291             \n",
       "15  0.011493                  0.002894               0.001588             \n",
       "16  0.002123                  0.002578               0.001075             \n",
       "17  0.000487                  0.001986               0.001382             \n",
       "18  0.008492                  0.001083               0.000984             \n",
       "19  0.007192                  0.008580               0.000763             \n",
       "20  0.008759                  0.017292               0.002847             \n",
       "21  0.002865                  0.001528               0.001725             \n",
       "22  0.008782                  0.009157               0.000794             \n",
       "23  0.011642                  0.026313               0.000700             \n",
       "24  0.003706                  0.004625               0.003966             \n",
       "25  0.008195                  0.014112               0.001187             \n",
       "26  0.009436                  0.016127               0.000611             \n",
       "27  0.003242                  0.004786               0.004471             \n",
       "28  0.007315                  0.014823               0.001187             \n",
       "29  0.007507                  0.015404               0.001031             \n",
       "30  0.025487                  0.002499               0.001816             \n",
       "31  0.027594                  0.017868               0.001110             \n",
       "32  0.018810                  0.008307               0.000148             \n",
       "33  0.028426                  0.004588               0.000890             \n",
       "34  0.034667                  0.022493               0.001061             \n",
       "35  0.015064                  0.004338               0.000087             \n",
       "\n",
       "    std_train_f1_macro  std_train_precision_macro  std_train_recall_macro  \\\n",
       "0   0.008829            0.000754                   0.004854                 \n",
       "1   0.000000            0.000000                   0.000000                 \n",
       "2   0.000000            0.000000                   0.000000                 \n",
       "3   0.017070            0.003089                   0.018538                 \n",
       "4   0.000000            0.000000                   0.000000                 \n",
       "5   0.000000            0.000000                   0.000000                 \n",
       "6   0.002870            0.000731                   0.002357                 \n",
       "7   0.000277            0.000281                   0.000274                 \n",
       "8   0.000153            0.000141                   0.000166                 \n",
       "9   0.001720            0.000582                   0.001349                 \n",
       "10  0.000133            0.000142                   0.000125                 \n",
       "11  0.000125            0.000112                   0.000139                 \n",
       "12  0.003778            0.002035                   0.003887                 \n",
       "13  0.000884            0.000562                   0.001014                 \n",
       "14  0.001002            0.000476                   0.001249                 \n",
       "15  0.001412            0.000558                   0.001715                 \n",
       "16  0.000922            0.000598                   0.001061                 \n",
       "17  0.001076            0.000533                   0.001340                 \n",
       "18  0.000689            0.004287                   0.000419                 \n",
       "19  0.007837            0.003087                   0.013430                 \n",
       "20  0.007272            0.009196                   0.014428                 \n",
       "21  0.001082            0.005851                   0.000516                 \n",
       "22  0.007568            0.003123                   0.013414                 \n",
       "23  0.013097            0.009024                   0.015372                 \n",
       "24  0.007135            0.002722                   0.005376                 \n",
       "25  0.001128            0.001176                   0.001091                 \n",
       "26  0.000805            0.000730                   0.000872                 \n",
       "27  0.007651            0.003136                   0.005768                 \n",
       "28  0.001128            0.001176                   0.001091                 \n",
       "29  0.001173            0.000977                   0.001367                 \n",
       "30  0.002751            0.000634                   0.001725                 \n",
       "31  0.000235            0.001046                   0.000606                 \n",
       "32  0.000193            0.001186                   0.001209                 \n",
       "33  0.001372            0.001721                   0.000975                 \n",
       "34  0.000527            0.001349                   0.000772                 \n",
       "35  0.000221            0.000785                   0.001215                 \n",
       "\n",
       "    type words  \n",
       "0   text  all   \n",
       "1   text  all   \n",
       "2   text  all   \n",
       "3   text  all   \n",
       "4   text  all   \n",
       "5   text  all   \n",
       "6   text  all   \n",
       "7   text  all   \n",
       "8   text  all   \n",
       "9   text  all   \n",
       "10  text  all   \n",
       "11  text  all   \n",
       "12  text  all   \n",
       "13  text  all   \n",
       "14  text  all   \n",
       "15  text  all   \n",
       "16  text  all   \n",
       "17  text  all   \n",
       "18  text  all   \n",
       "19  text  all   \n",
       "20  text  all   \n",
       "21  text  all   \n",
       "22  text  all   \n",
       "23  text  all   \n",
       "24  text  all   \n",
       "25  text  all   \n",
       "26  text  all   \n",
       "27  text  all   \n",
       "28  text  all   \n",
       "29  text  all   \n",
       "30  text  all   \n",
       "31  text  all   \n",
       "32  text  all   \n",
       "33  text  all   \n",
       "34  text  all   \n",
       "35  text  all   "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_helper.get_result_folder_df().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DummyClassifier performance per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all.classifier == 'DummyClassifier'].groupby('dataset').mean_test_f1_macro.max().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>result___ng20-ana__text.npy</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result___webkb-ana__text.npy</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result___webkb__text.npy</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result___ng20__text.npy</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result___reuters-21578__text.npy</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result___ling-spam__text.npy</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  filename\n",
       "result___ng20-ana__text.npy       6       \n",
       "result___webkb-ana__text.npy      6       \n",
       "result___webkb__text.npy          6       \n",
       "result___ng20__text.npy           6       \n",
       "result___reuters-21578__text.npy  6       \n",
       "result___ling-spam__text.npy      6       "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupy_cols = ['dataset', 'type']\n",
    "df_all.groupby(groupy_cols).mean_test_f1_macro.max().to_frame()#.sort_values('mean_test_f1_macro')#.plot(kind = 'barh')\n",
    "df_all.filename.value_counts().to_frame()\n",
    "#df_all\n",
    "#df_all[df_all.dataset == 'ng20']\n",
    "#df_all.groupby(['dataset', 'kernel']).mean_test_f1_macro.max().to_frame()\n",
    "#df_all.groupby(groupy_cols + ['kernel']).mean_test_f1_macro.max().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best classifers per type per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RENAME_COLS_MAPPING = {'mean_test_f1_macro': 'f1', 'mean_test_accuracy': 'accuracy', 'mean_test_precision_macro': 'precision', 'mean_test_recall_macro': 'recall'}\n",
    "\n",
    "UNINTERESTING_COLUMNS = [x for x in df_all.columns.tolist() if 'fit_time' in x or 'split' in x or 'std' in x or 'rank' in x]\n",
    "\n",
    "def plot_best_by_type(df_all, df, df_dataset, title = '', fontsize = 12, figsize = (6, 3), top = 0.85):\n",
    "    # Get best elements per dataset\n",
    "    els = df_all.iloc[df['mean_test_f1_macro'].idxmax()]\n",
    "    els = els.set_index('type')\n",
    "    els = els.rename(columns = RENAME_COLS_MAPPING)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    \n",
    "    std_errs = [els.std_test_f1_macro * 2,  els.std_test_accuracy * 2,  els.std_test_precision_macro * 2,  els.std_test_recall_macro * 2]\n",
    "\n",
    "    els[['f1', 'accuracy', 'precision', 'recall']].plot(kind = 'barh', ax = ax, xlim = (0, 1.5), xerr=std_errs)\n",
    "    ax.set_xticks(np.linspace(0, 1, 11))\n",
    "    \n",
    "    ax.grid(axis = 'y')\n",
    "    \n",
    "    display(els[[x for x in els.columns.tolist() if x not in UNINTERESTING_COLUMNS]])\n",
    "    \n",
    "    if title and title != '':\n",
    "        fig.suptitle(title, fontsize = fontsize)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if title and title != '':\n",
    "        fig.subplots_adjust(top = top)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "# Ignore 0th WL iteration\n",
    "for name, df_dataset in sorted(df_all[df_all.wl_iteration != 0].groupby('dataset'), key = lambda x: x[0]):\n",
    "    df_dataset_grouped_by_type = df_dataset.groupby('type')\n",
    "    print('################# {}'.format(name))\n",
    "    use_title = False\n",
    "    fig, ax = plot_best_by_type(df_all, df_dataset_grouped_by_type, df_dataset, 'Dataset: {}'.format(name) if use_title else None)\n",
    "    fig.savefig('tmp/results/dataset-{}-best.png'.format(name), dpi = 150)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data_filter_name, data_filter in [('only-concept-graphs', df_all.type == 'concept-graph'), ('only-coocurrence', df_all.type == 'cooccurrence'), ('all', df_all.type != 'YES')]:\n",
    "    for dataset_name, df in df_all[data_filter].groupby('dataset'):\n",
    "        for attr in ['type', 'kernel']:\n",
    "            # Filter out DummyClassifier\n",
    "            df = df[(df.classifier != 'DummyClassifier')]\n",
    "\n",
    "            # Ignore entries that have only one category\n",
    "            if len(df[attr].value_counts().tolist()) <= 1:\n",
    "                continue\n",
    "            \n",
    "            f1_min, f1_max = df.mean_test_f1_macro.min(), df.mean_test_f1_macro.max()\n",
    "            fig, axes = plt.subplots(figsize = EXPORT_FIG_SIZE)\n",
    "            ax = sns.violinplot(x = attr, y = 'mean_test_f1_macro', data=df, cut = 0, split = True, inner = 'quartile')\n",
    "            ax.set_ylim((0, f1_max + 0.1))\n",
    "            ax.set_ylabel('f1 macro')\n",
    "            fig.suptitle('Result distribution ({})'.format(data_filter_name));\n",
    "            ax.set_title('Dataset: {}, Attribute: {}'.format(dataset_name, attr))\n",
    "            fig.tight_layout()\n",
    "            fig.subplots_adjust(top = 0.85)\n",
    "            fig.savefig('tmp/result-distributions/{}-{}-{}.png'.format(dataset_name, data_filter_name, attr), dpi = EXPORT_DPI)\n",
    "            plt.show()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot best per parameter value per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_grouped_by_plot(df_all, groupby):\n",
    "    df_graphs_grouped = df_all[df_all.type != 'text'].groupby('dataset')\n",
    "    \n",
    "    axes = []\n",
    "    for idx, (dataset_name, df_dataset) in enumerate(df_graphs_grouped):\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize = EXPORT_FIG_SIZE)\n",
    "        # Print violinplot of f1, with graph_type as hue\n",
    "        hue = groupby if df_dataset[groupby].value_counts().count() > 1 else None\n",
    "        sns.violinplot(x = 'type', y = 'mean_test_f1_macro', hue= hue , data=df_dataset, cut = 0, split = True, inner = 'quartile', title = dataset_name, ax = ax, legend = True)\n",
    "        ax.set_title('Dataset: {}'.format(dataset_name))\n",
    "        ax.set_ylabel('f1')\n",
    "        ax.set_xlabel('TBD')\n",
    "        ax.grid('off')\n",
    "        fig.suptitle('TBD')\n",
    "        fig.tight_layout()\n",
    "        fig.subplots_adjust(top = 0.86)\n",
    "        fig.savefig('tmp/results/label-importance-{}.png'.format(dataset_name), dpi = EXPORT_DPI)\n",
    "        plt.show()\n",
    "\n",
    "if 1 == 1:\n",
    "    graphs_grouped_by_plot(df_all, 'combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def add(acc, item):\n",
    "    acc += item\n",
    "    return acc\n",
    "\n",
    "def get_vals_for_col(col):\n",
    "    return sorted(df_tmp[col].value_counts().index.tolist())\n",
    "\n",
    "cols = ['combined', 'kernel', 'lemmatized', 'relabeled', 'threshold', 'type', 'window_size', 'wl_iteration', 'words', 'classifier', 'same_label', 'topn']\n",
    "cols = ['type', 'combined', 'kernel', 'wl_iteration', 'same_label', 'dataset']\n",
    "\n",
    "df_tmp = df_all[df_all.dataset == 'ling-spam']\n",
    "\n",
    "vals = [get_vals_for_col(col) for col in cols]\n",
    "val_lenghts = [len(vals_) for vals_ in vals]\n",
    "dim = sum(val_lenghts)\n",
    "vals_flattened = functools.reduce(add, vals, [])\n",
    "\n",
    "best_of_mat = np.zeros((dim, dim), dtype=np.float32)\n",
    "\n",
    "col_counter = 0\n",
    "row_counter = 0\n",
    "\n",
    "for col_idx1, col1 in enumerate(cols):\n",
    "    vals_1 = get_vals_for_col(col1)\n",
    "    col_counter = 0\n",
    "    for col_idx2, col2 in enumerate(cols):\n",
    "        vals_2 = get_vals_for_col(col2)\n",
    "        for idx1, val1 in enumerate(vals_1):\n",
    "            for idx2, val2 in enumerate(vals_2):\n",
    "                best_of = df_tmp[(df_tmp[col1] == val1) & (df_tmp[col2] == val2)]\n",
    "                best_f1 = best_of.mean_test_f1_macro.max()\n",
    "                best_of_mat[row_counter + idx1, col_counter + idx2] = best_f1\n",
    "        col_counter += len(vals_2)\n",
    "    row_counter += len(vals_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(best_of_mat, vals, cols, ax = None, cmap='Blues', divider_color = '#FFFFFF', divider_linewidth = 6, fontdict = {'fontsize': 14, 'weight': 'bold'}):\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    vals_lengths = [len(val) for val in vals]\n",
    "    \n",
    "    # Add labels to graph\n",
    "    for idx, s in enumerate(np.cumsum(val_lenghts)):\n",
    "        for x in ['v' , 'h']:\n",
    "            getattr(plt, 'ax{}line'.format(x))(s - 0.5, color = divider_color, linewidth = divider_linewidth)\n",
    "        \n",
    "        text_offset = ((val_lenghts[idx]) / 2)\n",
    "        \n",
    "        # Add the col labels to the right\n",
    "        ax.text(dim + 0.5, s - text_offset - 0.5, cols[idx], horizontalalignment = 'left', verticalalignment = 'center', fontdict=fontdict)\n",
    "        # Add the col labels to the top\n",
    "        ax.text(s - text_offset - 0.2, - 1, cols[idx], horizontalalignment = 'center', verticalalignment = 'center', fontdict=fontdict)\n",
    "\n",
    "    # Add x- and y-ticks\n",
    "    for x in ['x' , 'y']:\n",
    "        getattr(plt, x + 'ticks')(range(len(vals_flattened)), vals_flattened)\n",
    "\n",
    "    # Rotate x-ticks\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "\n",
    "    # Mark cells where no values are available\n",
    "    for row, cell in (zip(*list(np.where(np.isnan(best_of_mat))))):\n",
    "        ax.text(row, cell, 'X', horizontalalignment = 'center', verticalalignment = 'center', fontdict=fontdict)\n",
    "\n",
    "    plt.grid('off')\n",
    "    plt.imshow(best_of_mat, cmap=cmap)\n",
    "    plt.colorbar(fraction=0.04, pad=0.2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (30, 30))\n",
    "#plot(np.tril(best_of_mat), vals, cols, ax)\n",
    "plot(best_of_mat, vals, cols, ax)\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/correlations.png', dpi = EXPORT_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot classifier performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, df_dataset in df_all.groupby('dataset'):\n",
    "    fig = plt.figure(figsize=(10, 2))\n",
    "    df_dataset.groupby('classifier').mean_test_f1_macro.max().plot(kind = 'barh', title = dataset_name)\n",
    "    plt.show()\n",
    "    #plt.close(fig)\n",
    "    #sns.violinplot(y = 'classifier', x = 'mean_test_f1_macro', data = df_dataset, cut = 0, split = True, inner = 'quartile', figsize = EXPORT_FIG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, df_dataset in df_all.groupby('dataset'):\n",
    "    fig = plt.figure(figsize=(10, 2))\n",
    "    df_dataset.groupby('lemmatized').mean_test_f1_macro.max().plot(kind = 'barh', title = dataset_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot performance per dataset and wl_iteration and graph_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gap_to_violin_plot(ax, delta = 0.03):\n",
    "    import matplotlib\n",
    "    # offset stuff\n",
    "    delta = 0.03\n",
    "    for ii, item in enumerate(ax.collections):\n",
    "        # axis contains PolyCollections and PathCollections\n",
    "        if isinstance(item, matplotlib.collections.PolyCollection):\n",
    "            # get path\n",
    "            path, = item.get_paths()\n",
    "            vertices = path.vertices\n",
    "\n",
    "            if ii % 2: # -> to right\n",
    "                vertices[:,0] += delta\n",
    "            else: # -> to left\n",
    "                vertices[:,0] -= delta\n",
    "\n",
    "\n",
    "for dataset, df_tmp in df_all[(df_all.type != 'text') & (df_all.lemmatized != True)].sort_values('wl_iteration').groupby('dataset'):\n",
    "    fig, ax = plt.subplots()\n",
    "    inner = 'quartile'\n",
    "    ax = sns.violinplot(x = 'wl_iteration', y = 'mean_test_f1_macro', hue = 'type', split = True, data = df_tmp, cut = True, inner = inner, figsize = EXPORT_FIG_SIZE)\n",
    "    \n",
    "    add_gap_to_violin_plot(ax)\n",
    "    \n",
    "    ax.set_ylabel('f1')\n",
    "    ax.set_title(dataset)\n",
    "    ax.figure.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot by parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_distributions(df, df_all, title = None, figsize = (10, 8)):\n",
    "    fig, axes_indexed = plt.subplots(nrows = 2, ncols=2, figsize = figsize)\n",
    "\n",
    "    axes = []\n",
    "    for ax_row in axes_indexed:\n",
    "        axes += list(ax_row)\n",
    "    #, 'relabeled'\n",
    "    for val, ax in zip(['wl_iteration', 'window_size', 'words', 'type'], axes):\n",
    "        if len(df.groupby(val).size()) == 0:\n",
    "            continue\n",
    "        grouped = df.groupby(val)\n",
    "        els = df_all.iloc[grouped['mean_test_f1_macro'].idxmax()]\n",
    "        els = els.set_index(val)\n",
    "        els = els.rename(columns = RENAME_COLS_MAPPING)\n",
    "        els[['f1', 'accuracy', 'precision', 'recall']].plot(kind = 'barh', ax = ax, xlim=(0, 2))\n",
    "        ax.set_xticks(np.linspace(0, 1, 11))\n",
    "        ax.grid(axis = 'y')\n",
    "        ax.set_xlim((0, 1.5))\n",
    "    \n",
    "    plt.suptitle(title, size = 18)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    return fig, axes\n",
    "    \n",
    "dpi = 150\n",
    "\n",
    "if 1 == 1:\n",
    "    fig, _  = plot_distributions(df_all, df_all, title = 'Mean over all datasets')\n",
    "    fig.savefig('tmp/results/all.png', dpi = dpi)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    for name, df_dataset in df_all.groupby('dataset'):\n",
    "        if len(df_dataset.type.value_counts()) < 3:\n",
    "            continue\n",
    "        fig, _ = plot_distributions(df_dataset, df_all, title = 'Dataset: {}'.format(name))\n",
    "        fig.savefig('tmp/results/dataset-{}.png'.format(name), dpi = dpi)\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUMP\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/check-w2v-results.json') as f:\n",
    "    w2v_results = json.load(f)\n",
    " \n",
    "per_embedding_type = {}\n",
    "for dataset, value in w2v_results.items():\n",
    "    print(dataset)\n",
    "    for embedding_raw, cache_files in sorted(value.items(), key = lambda x: x[0]):\n",
    "        embedding = embedding_raw.split('/')[-1].rsplit('.', 2)[0]\n",
    "        if len(cache_files.keys()) != 2: continue\n",
    "        print('\\t{}'.format(embedding))\n",
    "        if embedding not in per_embedding_type:\n",
    "            per_embedding_type[embedding] = {}\n",
    "        per_embemdding_type[embedding][dataset] = []\n",
    "        for dataset_file, counts in sorted(cache_files.items(), key = lambda x: x[0]):\n",
    "            not_found_ratio = int(counts['counts']['not_found'] / counts['num_labels'] * 100)\n",
    "            if embedding == 'trained' and 'coo' in  dataset_file:\n",
    "                print('Yes', counts['counts']['not_found'], not_found_ratio, '%', counts['not_found_sample'])\n",
    "            is_gml = 'dataset_graph_gml' in dataset_file\n",
    "            per_embedding_type[embedding][dataset].append((is_gml, not_found_ratio))\n",
    "            print('\\t\\t{:4} missing  {:3>}%'.format('gml' if is_gml else 'co', not_found_ratio))\n",
    "        per_embedding_type[embedding][dataset] = per_embedding_type[embedding][dataset][0][1]  #sum(y for x, y in per_embedding_type[embedding][dataset]) / 2\n",
    "df = pd.DataFrame(per_embedding_type)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {
    "height": "38px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
