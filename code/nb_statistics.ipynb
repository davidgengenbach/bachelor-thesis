{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(\"notebook/js/notebook\").Notebook.prototype.scroll_to_bottom = function () {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_scroll {\n",
    "    height: 34em !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import dummy\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "from utils import helper\n",
    "from utils import results_helper\n",
    "\n",
    "EXPORT_DPI = 100\n",
    "EXPORT_FIG_SIZE = (8, 4)\n",
    "EXPORT_FIG_SIZE_BIG = (10, 7)\n",
    "EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT = EXPORT_FIG_SIZE\n",
    "EXPORT_FIG_WIDTH_BIG, EXPORT_FIG_HEIGHT_BIG = EXPORT_FIG_SIZE_BIG\n",
    "\n",
    "plt.rcParams['figure.figsize'] = EXPORT_FIG_SIZE_BIG\n",
    "sns.set('notebook', 'whitegrid', palette='deep')\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = results_helper.get_results(use_already_loaded=False, exclude_filter = 'relabeled', filter_out_non_complete_datasets = False)\n",
    "#df_all = results_helper.get_results(folder ='2017-10-03_15-28', use_already_loaded=False, exclude_filter = 'relabeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_helper.get_result_folder_df().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DummyClassifier performance per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all.classifier == 'DummyClassifier'].groupby('dataset').mean_test_f1_macro.max().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all.type == 'text'].groupby('dataset').mean_test_f1_macro.max().apply(lambda x: '{:.3f}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_cmaps = df_all[(df_all.type  == 'concept-graph')]\n",
    "df_only_cmaps[df_only_cmaps.wl_casted == False].groupby('dataset').mean_test_f1_macro.max(), df_only_cmaps[df_only_cmaps.wl_casted == True].groupby('dataset').mean_test_f1_macro.max()\n",
    "#df_all.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data_filter_name, data_filter in [('only-concept-graphs', df_all.type == 'concept-graph'), ('only-coocurrence', df_all.type == 'cooccurrence'), ('all', df_all.type != 'YES')]:\n",
    "    for dataset_name, df in df_all[data_filter].groupby('dataset'):\n",
    "        for attr in ['type', 'kernel']:\n",
    "            # Filter out DummyClassifier\n",
    "            df = df[(df.classifier != 'DummyClassifier')]\n",
    "\n",
    "            # Ignore entries that have only one category\n",
    "            if len(df[attr].value_counts().tolist()) <= 1:\n",
    "                continue\n",
    "            \n",
    "            f1_min, f1_max = df.mean_test_f1_macro.min(), df.mean_test_f1_macro.max()\n",
    "            fig, axes = plt.subplots(figsize = EXPORT_FIG_SIZE)\n",
    "            ax = sns.violinplot(x = attr, y = 'mean_test_f1_macro', data=df, cut = 0, split = True, inner = 'quartile')\n",
    "            ax.set_ylim((0, f1_max + 0.1))\n",
    "            ax.set_ylabel('f1 macro')\n",
    "            fig.suptitle('Result distribution ({})'.format(data_filter_name));\n",
    "            ax.set_title('Dataset: {}, Attribute: {}'.format(dataset_name, attr))\n",
    "            fig.tight_layout()\n",
    "            fig.subplots_adjust(top = 0.85)\n",
    "            fig.savefig('tmp/result-distributions/{}-{}-{}.png'.format(dataset_name, data_filter_name, attr), dpi = EXPORT_DPI)\n",
    "            plt.show()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best classifers per type per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RENAME_COLS_MAPPING = {'mean_test_f1_macro': 'f1', 'mean_test_accuracy': 'accuracy', 'mean_test_precision_macro': 'precision', 'mean_test_recall_macro': 'recall'}\n",
    "\n",
    "UNINTERESTING_COLUMNS = [x for x in df_all.columns.tolist() if 'fit_time' in x or 'split' in x or 'std' in x or 'rank' in x]\n",
    "\n",
    "def plot_best_by_type(df_all, df, df_dataset, title = '', fontsize = 12, figsize = (6, 3), top = 0.85):\n",
    "    # Get best elements per dataset\n",
    "    els = df_all.iloc[df['mean_test_f1_macro'].idxmax()]\n",
    "    els = els.set_index('type')\n",
    "    els = els.rename(columns = RENAME_COLS_MAPPING)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    \n",
    "    std_errs = [els.std_test_f1_macro * 2,  els.std_test_accuracy * 2,  els.std_test_precision_macro * 2,  els.std_test_recall_macro * 2]\n",
    "\n",
    "    els[['f1', 'accuracy', 'precision', 'recall']].plot(kind = 'barh', ax = ax, xlim = (0, 1.5), xerr=std_errs)\n",
    "    ax.set_xticks(np.linspace(0, 1, 11))\n",
    "    \n",
    "    ax.grid(axis = 'y')\n",
    "    \n",
    "    display(els[[x for x in els.columns.tolist() if x not in UNINTERESTING_COLUMNS]])\n",
    "    \n",
    "    if title and title != '':\n",
    "        fig.suptitle(title, fontsize = fontsize)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if title and title != '':\n",
    "        fig.subplots_adjust(top = top)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "# Ignore 0th WL iteration\n",
    "for name, df_dataset in sorted(df_all[df_all.wl_iteration != 0].groupby('dataset'), key = lambda x: x[0]):\n",
    "    df_dataset_grouped_by_type = df_dataset.groupby('type')\n",
    "    print('################# {}'.format(name))\n",
    "    use_title = False\n",
    "    fig, ax = plot_best_by_type(df_all, df_dataset_grouped_by_type, df_dataset, 'Dataset: {}'.format(name) if use_title else None)\n",
    "    fig.savefig('tmp/results/dataset-{}-best.png'.format(name), dpi = 150)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot best per parameter value per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_grouped_by_plot(df_all, groupby):\n",
    "    df_graphs_grouped = df_all[df_all.type != 'text'].groupby('dataset')\n",
    "    \n",
    "    axes = []\n",
    "    for idx, (dataset_name, df_dataset) in enumerate(df_graphs_grouped):\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize = EXPORT_FIG_SIZE)\n",
    "        # Print violinplot of f1, with graph_type as hue\n",
    "        hue = groupby if df_dataset[groupby].value_counts().count() > 1 else None\n",
    "        sns.violinplot(x = 'type', y = 'mean_test_f1_macro', hue= hue , data=df_dataset, cut = 0, split = True, inner = 'quartile', title = dataset_name, ax = ax, legend = True)\n",
    "        ax.set_title('Dataset: {}'.format(dataset_name))\n",
    "        ax.set_ylabel('f1')\n",
    "        ax.set_xlabel('TBD')\n",
    "        ax.grid('off')\n",
    "        fig.suptitle('TBD')\n",
    "        fig.tight_layout()\n",
    "        fig.subplots_adjust(top = 0.86)\n",
    "        fig.savefig('tmp/results/label-importance-{}.png'.format(dataset_name), dpi = EXPORT_DPI)\n",
    "        plt.show()\n",
    "\n",
    "if 1 == 1:\n",
    "    graphs_grouped_by_plot(df_all, 'combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def add(acc, item):\n",
    "    acc += item\n",
    "    return acc\n",
    "\n",
    "def get_vals_for_col(col):\n",
    "    return sorted(df_tmp[col].value_counts().index.tolist())\n",
    "\n",
    "cols = ['combined', 'kernel', 'lemmatized', 'relabeled', 'threshold', 'type', 'window_size', 'wl_iteration', 'words', 'classifier', 'same_label', 'topn']\n",
    "cols = ['type', 'combined', 'kernel', 'wl_iteration', 'same_label', 'dataset']\n",
    "\n",
    "df_tmp = df_all[df_all.dataset == 'ling-spam']\n",
    "\n",
    "vals = [get_vals_for_col(col) for col in cols]\n",
    "val_lenghts = [len(vals_) for vals_ in vals]\n",
    "dim = sum(val_lenghts)\n",
    "vals_flattened = functools.reduce(add, vals, [])\n",
    "\n",
    "best_of_mat = np.zeros((dim, dim), dtype=np.float32)\n",
    "\n",
    "col_counter = 0\n",
    "row_counter = 0\n",
    "\n",
    "for col_idx1, col1 in enumerate(cols):\n",
    "    vals_1 = get_vals_for_col(col1)\n",
    "    col_counter = 0\n",
    "    for col_idx2, col2 in enumerate(cols):\n",
    "        vals_2 = get_vals_for_col(col2)\n",
    "        for idx1, val1 in enumerate(vals_1):\n",
    "            for idx2, val2 in enumerate(vals_2):\n",
    "                best_of = df_tmp[(df_tmp[col1] == val1) & (df_tmp[col2] == val2)]\n",
    "                best_f1 = best_of.mean_test_f1_macro.max()\n",
    "                best_of_mat[row_counter + idx1, col_counter + idx2] = best_f1\n",
    "        col_counter += len(vals_2)\n",
    "    row_counter += len(vals_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(best_of_mat, vals, cols, ax = None, cmap='Blues', divider_color = '#FFFFFF', divider_linewidth = 6, fontdict = {'fontsize': 14, 'weight': 'bold'}):\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    vals_lengths = [len(val) for val in vals]\n",
    "    \n",
    "    # Add labels to graph\n",
    "    for idx, s in enumerate(np.cumsum(val_lenghts)):\n",
    "        for x in ['v' , 'h']:\n",
    "            getattr(plt, 'ax{}line'.format(x))(s - 0.5, color = divider_color, linewidth = divider_linewidth)\n",
    "        \n",
    "        text_offset = ((val_lenghts[idx]) / 2)\n",
    "        \n",
    "        # Add the col labels to the right\n",
    "        ax.text(dim + 0.5, s - text_offset - 0.5, cols[idx], horizontalalignment = 'left', verticalalignment = 'center', fontdict=fontdict)\n",
    "        # Add the col labels to the top\n",
    "        ax.text(s - text_offset - 0.2, - 1, cols[idx], horizontalalignment = 'center', verticalalignment = 'center', fontdict=fontdict)\n",
    "\n",
    "    # Add x- and y-ticks\n",
    "    for x in ['x' , 'y']:\n",
    "        getattr(plt, x + 'ticks')(range(len(vals_flattened)), vals_flattened)\n",
    "\n",
    "    # Rotate x-ticks\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "\n",
    "    # Mark cells where no values are available\n",
    "    for row, cell in (zip(*list(np.where(np.isnan(best_of_mat))))):\n",
    "        ax.text(row, cell, 'X', horizontalalignment = 'center', verticalalignment = 'center', fontdict=fontdict)\n",
    "\n",
    "    plt.grid('off')\n",
    "    plt.imshow(best_of_mat, cmap=cmap)\n",
    "    plt.colorbar(fraction=0.04, pad=0.2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (30, 30))\n",
    "#plot(np.tril(best_of_mat), vals, cols, ax)\n",
    "plot(best_of_mat, vals, cols, ax)\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/correlations.png', dpi = EXPORT_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot classifier performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, df_dataset in df_all.groupby('dataset'):\n",
    "    fig = plt.figure(figsize=(10, 2))\n",
    "    df_dataset.groupby('classifier').mean_test_f1_macro.max().plot(kind = 'barh', title = dataset_name)\n",
    "    plt.show()\n",
    "    #plt.close(fig)\n",
    "    #sns.violinplot(y = 'classifier', x = 'mean_test_f1_macro', data = df_dataset, cut = 0, split = True, inner = 'quartile', figsize = EXPORT_FIG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, df_dataset in df_all.groupby('dataset'):\n",
    "    fig = plt.figure(figsize=(10, 2))\n",
    "    df_dataset.groupby('lemmatized').mean_test_f1_macro.max().plot(kind = 'barh', title = dataset_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot performance per dataset and wl_iteration and graph_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gap_to_violin_plot(ax, delta = 0.03):\n",
    "    import matplotlib\n",
    "    # offset stuff\n",
    "    delta = 0.03\n",
    "    for ii, item in enumerate(ax.collections):\n",
    "        # axis contains PolyCollections and PathCollections\n",
    "        if isinstance(item, matplotlib.collections.PolyCollection):\n",
    "            # get path\n",
    "            path, = item.get_paths()\n",
    "            vertices = path.vertices\n",
    "\n",
    "            if ii % 2: # -> to right\n",
    "                vertices[:,0] += delta\n",
    "            else: # -> to left\n",
    "                vertices[:,0] -= delta\n",
    "\n",
    "\n",
    "for dataset, df_tmp in df_all[(df_all.type != 'text') & (df_all.lemmatized != True)].sort_values('wl_iteration').groupby('dataset'):\n",
    "    fig, ax = plt.subplots()\n",
    "    inner = 'quartile'\n",
    "    ax = sns.violinplot(x = 'wl_iteration', y = 'mean_test_f1_macro', hue = 'type', split = True, data = df_tmp, cut = True, inner = inner, figsize = EXPORT_FIG_SIZE)\n",
    "    \n",
    "    add_gap_to_violin_plot(ax)\n",
    "    \n",
    "    ax.set_ylabel('f1')\n",
    "    ax.set_title(dataset)\n",
    "    ax.figure.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot by parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_distributions(df, df_all, title = None, figsize = (10, 8)):\n",
    "    fig, axes_indexed = plt.subplots(nrows = 2, ncols=2, figsize = figsize)\n",
    "\n",
    "    axes = []\n",
    "    for ax_row in axes_indexed:\n",
    "        axes += list(ax_row)\n",
    "    #, 'relabeled'\n",
    "    for val, ax in zip(['wl_iteration', 'window_size', 'words', 'type'], axes):\n",
    "        if len(df.groupby(val).size()) == 0:\n",
    "            continue\n",
    "        grouped = df.groupby(val)\n",
    "        els = df_all.iloc[grouped['mean_test_f1_macro'].idxmax()]\n",
    "        els = els.set_index(val)\n",
    "        els = els.rename(columns = RENAME_COLS_MAPPING)\n",
    "        els[['f1', 'accuracy', 'precision', 'recall']].plot(kind = 'barh', ax = ax, xlim=(0, 2))\n",
    "        ax.set_xticks(np.linspace(0, 1, 11))\n",
    "        ax.grid(axis = 'y')\n",
    "        ax.set_xlim((0, 1.5))\n",
    "    \n",
    "    plt.suptitle(title, size = 18)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    return fig, axes\n",
    "    \n",
    "dpi = 150\n",
    "\n",
    "if 1 == 1:\n",
    "    fig, _  = plot_distributions(df_all, df_all, title = 'Mean over all datasets')\n",
    "    fig.savefig('tmp/results/all.png', dpi = dpi)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    for name, df_dataset in df_all.groupby('dataset'):\n",
    "        if len(df_dataset.type.value_counts()) < 3:\n",
    "            continue\n",
    "        fig, _ = plot_distributions(df_dataset, df_all, title = 'Dataset: {}'.format(name))\n",
    "        fig.savefig('tmp/results/dataset-{}.png'.format(name), dpi = dpi)\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUMP\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/check-w2v-results.json') as f:\n",
    "    w2v_results = json.load(f)\n",
    " \n",
    "per_embedding_type = {}\n",
    "for dataset, value in w2v_results.items():\n",
    "    print(dataset)\n",
    "    for embedding_raw, cache_files in sorted(value.items(), key = lambda x: x[0]):\n",
    "        embedding = embedding_raw.split('/')[-1].rsplit('.', 2)[0]\n",
    "        if len(cache_files.keys()) != 2: continue\n",
    "        print('\\t{}'.format(embedding))\n",
    "        if embedding not in per_embedding_type:\n",
    "            per_embedding_type[embedding] = {}\n",
    "        per_embemdding_type[embedding][dataset] = []\n",
    "        for dataset_file, counts in sorted(cache_files.items(), key = lambda x: x[0]):\n",
    "            not_found_ratio = int(counts['counts']['not_found'] / counts['num_labels'] * 100)\n",
    "            if embedding == 'trained' and 'coo' in  dataset_file:\n",
    "                print('Yes', counts['counts']['not_found'], not_found_ratio, '%', counts['not_found_sample'])\n",
    "            is_gml = 'dataset_graph_gml' in dataset_file\n",
    "            per_embedding_type[embedding][dataset].append((is_gml, not_found_ratio))\n",
    "            print('\\t\\t{:4} missing  {:3>}%'.format('gml' if is_gml else 'co', not_found_ratio))\n",
    "        per_embedding_type[embedding][dataset] = per_embedding_type[embedding][dataset][0][1]  #sum(y for x, y in per_embedding_type[embedding][dataset]) / 2\n",
    "df = pd.DataFrame(per_embedding_type)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {
    "height": "38px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
