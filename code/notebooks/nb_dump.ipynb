{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WL phi feature map distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp/phi-distributions', exist_ok = True)\n",
    "\n",
    "def print_phi_distribution(title, X_phi, Y, sort_y = True, add_class_label = True, add_class_line = True, figsize = (15, 12), flip_axis = False, draw_vertice_count_line = True, h_lines = [], sns_color_palette = \"bright\", class_v_line_kwargs=dict(linestyle = 'solid', color = 'black', linewidth = 1, alpha = 0.1), class_h_line_kwargs = dict(linestyle ='solid', linewidth = 1, alpha = 0.6)):\n",
    "    if sort_y:\n",
    "        # Use a stable sort algorithm\n",
    "        Y_sorted_indices = np.argsort(Y, kind = 'mergesort')\n",
    "        Y = np.array(Y)[Y_sorted_indices]\n",
    "        X_phi = [phi[Y_sorted_indices] for phi in X_phi]\n",
    "\n",
    "    cmap_ = sns.color_palette(sns_color_palette, max(len(set(Y)), len(h_lines[0])))\n",
    "    clazz_2_color_map = dict()\n",
    "    \n",
    "    counter = 0\n",
    "    for clazz in Y:\n",
    "        if clazz not in clazz_2_color_map:\n",
    "            clazz_2_color_map[clazz] = counter\n",
    "            counter += 1\n",
    "    colors = [clazz_2_color_map[y] for y in Y]\n",
    "    occurences = {clazz: -1 for clazz in set(Y)}\n",
    "    \n",
    "    for idx, y in enumerate(Y):\n",
    "        if occurences[y] == -1: occurences[y] = idx\n",
    "\n",
    "    ax_labels = ['phi index', 'graph']\n",
    "    ax_line = 'v' if flip_axis else 'x'\n",
    "    \n",
    "    if flip_axis:\n",
    "        ax_labels = list(reversed(ax_labels))\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols = len(X_phi), figsize = figsize, sharey = True)\n",
    "    \n",
    "    axes[0].set_ylabel(ax_labels[1])\n",
    "    \n",
    "    num_graphs, num_vertices = X_phi[0].shape\n",
    "    \n",
    "    for h, (phi, ax) in enumerate(zip(X_phi, axes)):\n",
    "        non_zero = phi.nonzero()\n",
    "\n",
    "        if flip_axis:\n",
    "            non_zero = reversed(non_zero)\n",
    "\n",
    "        y, x = non_zero\n",
    "\n",
    "        # Plot hlines (eg. max phi index of class)\n",
    "        if len(h_lines):\n",
    "            for class_idx, hline in enumerate(h_lines[h]):\n",
    "                if isinstance(hline, tuple):\n",
    "                    hline, line_color = hline\n",
    "                else:\n",
    "                    line_color = cmap_[class_idx]\n",
    "                ax.axhline(hline, color = line_color, **class_h_line_kwargs)\n",
    "        \n",
    "        colors_ = [cmap_[colors[x_]] for x_ in x]\n",
    "        ax.scatter(x = x, y = y, c = colors_, cmap=cmap_, s = 1)\n",
    "        ax.set_title('Iteration: {}'.format(h))\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(ax_labels[0])\n",
    "        ax.grid('off')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # Add class occurrence lines\n",
    "        for clazz, occurence_idx in occurences.items():\n",
    "            if add_class_line:\n",
    "                getattr(ax, 'ax{}line'.format(ax_line))(occurence_idx, **class_v_line_kwargs)\n",
    "            if add_class_label:\n",
    "                x, y = 1, occurence_idx\n",
    "                if flip_axis:\n",
    "                    y, x = x, y\n",
    "                ax.text(x = 1, y = occurence_idx, s = clazz, color = 'red')\n",
    "\n",
    "        if draw_vertice_count_line:\n",
    "            ax.set_ylim(0, num_vertices * 1.03)\n",
    "            ax.axhline(num_vertices - (num_vertices / 100), color = 'red', linewidth=1)\n",
    "        \n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top = .9)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def filter(cache_file):\n",
    "    is_split = 'splitted' in cache_file\n",
    "    is_ling_spam = 'ling-spam' in cache_file\n",
    "    is_dataset = lambda dataset: filename_utils.get_dataset_from_filename(cache_file) == dataset\n",
    "    is_concept_graph = 'concept-map' in cache_file\n",
    "    is_same_label = 'same-label' in cache_file\n",
    "    return is_split# and is_same_label #and is_dataset('webkb')# and is_concept_graph\n",
    "\n",
    "\n",
    "NUM_ITERATIONS = 3\n",
    "\n",
    "for cache_file in dataset_helper.get_all_cached_graph_phi_datasets():\n",
    "    if not filter(cache_file): continue\n",
    "    print(cache_file)\n",
    "    def plot(X_phi, Y, suffix = '', h_lines = [], plot_kwargs = dict()):\n",
    "        filename = 'tmp/phi-distributions/{}{}.png'.format(cache_file.split('/')[-1], suffix)\n",
    "        assert np.array_equal(np.sort(Y), Y)\n",
    "        if os.path.exists(filename):\n",
    "            return\n",
    "        print(filename.split('/')[-1])\n",
    "        kwargs = dict(dict(sort_y = False, add_class_label = False, flip_axis = True, add_class_line = True, h_lines = h_lines), **plot_kwargs)\n",
    "        fig, ax = print_phi_distribution('File: {} {} (#vertices: {})'.format(cache_file.split('/')[-1], suffix, X_phi[0].shape[1]), X_phi, Y, **kwargs)\n",
    "        fig.savefig(filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def get_hlines(phis, Y):\n",
    "        highest_per_class = []\n",
    "        for phi in phis:\n",
    "            non_zero = phi.nonzero()\n",
    "            non_zero_y, non_zero_x = non_zero \n",
    "            highest_per_class_ = collections.defaultdict(lambda: -1)\n",
    "            for non_zero_y, non_zero_x in zip(non_zero_y, non_zero_x):\n",
    "                clazz = Y[non_zero_y]\n",
    "                highest_per_class_[clazz] = max(highest_per_class_[clazz], non_zero_x)\n",
    "            highest_per_class.append(sorted(list(highest_per_class_.values())))\n",
    "        return highest_per_class\n",
    "        \n",
    "    phi_res = dataset_helper.get_dataset_cached(cache_file, check_validity=False)\n",
    "    if len(phi_res) == 2:\n",
    "        phi_train, Y_train = phi_res\n",
    "        h_lines = get_hlines(phi_train, Y_train)\n",
    "        plot(*phi_res, h_lines = h_lines)\n",
    "    elif len(phi_res) == 6:\n",
    "        phi_train, phi_test, X_train, X_test, Y_train, Y_test = phi_res\n",
    "        h_lines = get_hlines(phi_train, Y_train)\n",
    "        kwargs = dict()\n",
    "        if 'same-label' in cache_file:\n",
    "            kwargs['draw_vertice_count_line'] = False\n",
    "        if len(set(Y_train)) > 20:\n",
    "            kwargs['add_class_line'] = False\n",
    "        for res in [(phi_train, Y_train, '_train'), [phi_test, Y_test, '_test']]:\n",
    "            plot(*res, h_lines = h_lines, plot_kwargs=kwargs)\n",
    "    else:\n",
    "        assert False\n",
    "    #break\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy saved figures into categorized folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PHI_DIST = 'tmp/phi-distributions'\n",
    "\n",
    "data = collections.defaultdict(lambda: [])\n",
    "for file in glob('{}/*.png'.format(FOLDER_PHI_DIST)):\n",
    "    data['file'].append(file)\n",
    "df = pd.DataFrame(data)\n",
    "df['dataset'] = df.file.apply(filename_utils.get_dataset_from_filename)\n",
    "df['same_label'] = df.file.str.contains('same-label') | df.file.str.contains('same_label')\n",
    "df['type'] = df.file.str.extract(r'dataset_graph_(.+?)_')\n",
    "df['window_size'] = df.file.str.extract(r'dataset_graph_cooccurrence_(.+?)_')\n",
    "df['split'] = df.file.str.contains('splitted')\n",
    "df['test_train'] = df.file.str.extract(r'phi.npy_(.+?).png$')\n",
    "df['original_file'] = df.file.str.extract(r'/([^/]+?\\.npy)').str.replace('.splitted', '')\n",
    "\n",
    "for idx, item in df.iterrows():\n",
    "    old_folder = item.file.rsplit(\"/\", 1)[0]\n",
    "    old_filename = item.file.rsplit(\"/\", 1)[1]\n",
    "    new_filename = '{old_folder}/_/{t.dataset}/{t.type}/{t.same_label}/{old_filename}'.format(t = item, old_folder = old_folder, old_filename = old_filename)\n",
    "    folder = new_filename.rsplit('/', 1)[0]\n",
    "    os.makedirs(folder, exist_ok = True)\n",
    "    shutil.copyfile(src = item.file, dst=new_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about predictions\n",
    "\n",
    "by size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graphs(X_test):\n",
    "    graph_helper.convert_graphs_to_adjs_tuples(X_test)\n",
    "    return np.array(X_test, dtype=object)\n",
    "\n",
    "prediction_results = {}\n",
    "for prediction_filename, prediction in helper.log_progress(list(results_helper.get_predictions())):\n",
    "    is_graph_type = 'graph' in prediction_filename\n",
    "    if not is_graph_type: continue\n",
    "    results = results_helper.get_result_for_prediction(prediction_filename)\n",
    "    if not results:\n",
    "        print(\"Results for predictions could not be found: {}\".format(prediction_filename))\n",
    "    prediction_results[prediction_filename] = [prediction['results'][attr] for attr in ['Y_real', 'Y_pred', 'X_test']]\n",
    "    prediction_results[prediction_filename][2] = get_graphs(prediction_results[prediction_filename][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(lambda: [])\n",
    "counter = 0\n",
    "\n",
    "for prediction_filename, (Y_real, Y_pred, X_test) in helper.log_progress(list(prediction_results.items())):\n",
    "    if 'gram' in prediction_filename: continue\n",
    "    if not 'graph' in prediction_filename: continue\n",
    "    \n",
    "    Y_real, Y_pred, X_test = np.array(Y_real, dtype=object), np.array(Y_pred, dtype=object), np.array(X_test, dtype=object)\n",
    "    \n",
    "    df_graphs = pd.DataFrame({\n",
    "        'num_nodes': [len(labels) for adj, labels  in X_test],\n",
    "        'num_edges': [len(adj.nonzero()[0]) for adj, labels  in X_test]\n",
    "    })\n",
    "    \n",
    "    third_quartile = df_graphs.num_nodes.quantile(q = 0.75)\n",
    "    \n",
    "    df_graphs_big = df_graphs[df_graphs.num_nodes >= third_quartile]\n",
    "    df_graphs = df_graphs[df_graphs.num_nodes < third_quartile]\n",
    "    \n",
    "    NUM_BIN_SMALL = 20\n",
    "    NUM_BIN_BIGGER = 1\n",
    "    \n",
    "    df_graphs['bins'], bins = pd.cut(df_graphs.num_nodes, NUM_BIN_SMALL, retbins=True)\n",
    "    df_graphs['bin_type'] = 'small'\n",
    "    df_graphs_big['bins'], bins = pd.cut(df_graphs_big.num_nodes, NUM_BIN_BIGGER, retbins=True)\n",
    "    df_graphs_big['bin_type'] = 'big'\n",
    "    df_new = df_graphs.append(df_graphs_big)\n",
    "    \n",
    "    for bin_, df_graph_quartile in df_new.sort_values('bins').groupby('bins'):\n",
    "        indices = df_graph_quartile.index.tolist()\n",
    "        Y_real_quart, Y_pred_quart, X_test_quart = [x[indices] for x in [Y_real, Y_pred, X_test]]\n",
    "        f1 = metrics.f1_score(y_true=Y_real_quart, y_pred=Y_pred_quart, average = 'macro')\n",
    "        data['file'].append(prediction_filename)\n",
    "        data['bin'].append(bin_)\n",
    "        data['bin_type'].append(df_graph_quartile.bin_type.unique()[0])\n",
    "        data['f1'].append(f1)\n",
    "        data['num_elements'].append(len(indices))\n",
    "    \n",
    "df_bins = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins['mean_bin'] = df_bins.bin.apply(lambda x: np.rint(x.mid))\n",
    "df_bins['filename_only'] = df_bins.file.apply(lambda x: x.split('/')[-1])\n",
    "df_bins['dataset'] = df_bins.filename_only.apply(filename_utils.get_dataset_from_filename)\n",
    "df_bins['type'] = df_bins.filename_only.str.extract(r'dataset_graph_(.+?)_')\n",
    "df_bins['combined'] = df_bins.filename_only.str.contains('combined')\n",
    "df_bins['same_label'] = df_bins.filename_only.str.contains('same_label')\n",
    "df_bins['kernel'] = df_bins.filename_only.apply(results_helper.get_kernel_from_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_BIN_IMGS = 'tmp/bin-scores'\n",
    "os.makedirs(FOLDER_BIN_IMGS, exist_ok=True)\n",
    "\n",
    "df_bins = df_bins.reset_index(drop = True)\n",
    "\n",
    "df_bins['graph_file'] = df_bins.filename_only.apply(lambda x: re.findall(r'(dataset_graph_.*?.npy)', x)[0])\n",
    "df_bins_filtered = df_bins.groupby('graph_file').filter(lambda x: len(x.same_label.unique()) > 1)\n",
    "df_bins_filtered = df_bins_filtered[df_bins_filtered.combined == False]\n",
    "colors = sns.color_palette('deep')\n",
    "for (graph_file), df in df_bins_filtered.groupby(['graph_file']):\n",
    "    fig, ax = plt.subplots()\n",
    "    for idx, ((filename, same_label), df_) in enumerate(df.groupby(['filename_only', 'same_label'])):\n",
    "        df_ = df_.set_index('mean_bin').sort_index()\n",
    "        df_.f1.plot(kind = 'bar', ax = ax, color = colors[idx], title = 'File: {}'.format(filename), label = 'same-label' if same_label else 'normal', alpha = 0.9)#, marker = 'o', markersize = 5)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels)\n",
    "    \n",
    "    plt.show()\n",
    "    #break\n",
    "\n",
    "    #break\n",
    "    fig.savefig('{}/{}.png'.format(FOLDER_BIN_IMGS, graph_file))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar graphs by WL feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Retrieve graph datasets (both cmap and coo)\n",
    "\n",
    "Retrieve phi feature maps for coo and cmap, then get number of non-zero elements per graph (= per row of the feature map)\n",
    "\n",
    "And calculate the gram matrix, then find the most similar graphs per graph (= per row of the gram matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ng20'\n",
    "dataset_name = 'ling-spam'\n",
    "\n",
    "results = collections.defaultdict(lambda: {})\n",
    "for graph_phi_file in dataset_helper.get_all_cached_graph_phi_datasets(dataset_name=dataset_name):\n",
    "    #if 'concept' not in graph_phi_file: continue\n",
    "    print('Processing: {}'.format(graph_phi_file.split('/')[-1]))\n",
    "    phi, Y = dataset_helper.get_dataset_cached(graph_phi_file, check_validity=False)\n",
    "    \n",
    "    for h, phi_used in enumerate(phi):\n",
    "        print('\\th={}'.format(h))\n",
    "        # Generate kernel matrix\n",
    "        gram_matrix = phi_used.dot(phi_used.T).toarray()\n",
    "        results[graph_phi_file][h] = {}\n",
    "        results[graph_phi_file][h]['phi_used'] = phi_used\n",
    "        # Vector with the number of non-zero elements per row\n",
    "        # ie. non_zero_elements[idx] = len(phi_used[idx].nonzero())\n",
    "        results[graph_phi_file][h]['non_zero_elements'] = np.squeeze(np.asarray(np.sum(phi_used, axis = 1).T))\n",
    "        results[graph_phi_file][h]['num_elements'] = phi_used.shape[0]\n",
    "        results[graph_phi_file][h]['found_counter'] = collections.Counter()\n",
    "        results[graph_phi_file][h]['most_similar_scores'] = []\n",
    "        results[graph_phi_file][h]['similarity_pairs'] = []\n",
    "\n",
    "        for idx, row in enumerate(gram_matrix):\n",
    "            indices = np.argsort(row)[-10:]\n",
    "            # Search for index of this graph in the similar graph indices,\n",
    "            # it should be the most similar graph (because it's the same graph!)\n",
    "            results[graph_phi_file][h]['found_counter']['found' if idx in indices else 'not_found'] += 1\n",
    "            results[graph_phi_file][h]['most_similar_scores'].append(row[indices].tolist())\n",
    "            results[graph_phi_file][h]['similarity_pairs'].append(indices)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sparsity of feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph_cache_file, iterations in results.items():\n",
    "    fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE_BIG)\n",
    "    if 'gml' not in graph_cache_file and 'cooccurrence_1_all_ling' not in graph_cache_file: continue\n",
    "    for iteration, metrics in iterations.items():\n",
    "        if iteration != 0: continue\n",
    "        df = pd.DataFrame(metrics['non_zero_elements'], columns=['non_zero_elements'])\n",
    "        df.non_zero_elements.plot(kind='hist', bins = 40, alpha = 0.8, ax = ax, title = 'Histogram of non-zero entries in feature map (per row/element)')\n",
    "    \n",
    "    ax.set_xlim((0, 1000))\n",
    "    ax.set_xlabel('# of non-zero elements per row')\n",
    "    ax.legend(['Co-occurence graphs', 'Concept maps'])\n",
    "    plt.show()\n",
    "    fig.savefig('tmp/feature-map-sparsity-{}.png'.format(dataset_name), dpi = EXPORT_DPI)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get graph dataset for the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_phi_file = 'data/CACHE/dataset_graph_gml_ling-spam-single.phi.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = graph_phi_file.split('/')[-1].split('.phi')[0]\n",
    "print(filename)\n",
    "candidates = [x for x in dataset_helper.get_all_cached_graph_datasets() if filename in x]\n",
    "assert len(candidates)\n",
    "X, Y = dataset_helper.get_dataset_cached(candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    fig, ax = plt.subplots()\n",
    "    choice = np.random.choice(len(X))\n",
    "    graph = nx.Graph(X[choice])\n",
    "    res = [(node, val) for node, val in nx.pagerank(graph).items()]\n",
    "    nodes = [node for node, val in res]\n",
    "    node_vals = np.array([val for node, val in res])\n",
    "    node_sizes = np.exp(node_vals * 20) * 20 / len(nodes) * 3\n",
    "    node_sizes = [0 for x in nodes]\n",
    "    nx.draw_networkx(graph, nodelist = nodes, with_labels=False, node_size = node_sizes, node_color='#000000')\n",
    "    ax.set_title('Graph#={}, connected_components={}'.format(choice, nx.number_connected_components(graph)))\n",
    "    ax.grid('off')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('#FFFFFF')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text, _ = dataset_helper.get_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot similar graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "similarity_pairs = results[graph_phi_file]['similarity_pairs']\n",
    "similarity_scores = np.array(results[graph_phi_file]['most_similar_scores'])\n",
    "phi_used = results[graph_phi_file]['phi_used']\n",
    "\n",
    "# Check that the similarity score can not be greater than the number of nodes!\n",
    "for idx, graph, most_similar, most_similar_score in zip(range(len(X)), X, similarity_pairs, similarity_scores):\n",
    "    assert max(most_similar_score) <= len(graph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_matrix = phi_used.dot(phi_used.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_pairs = results[graph_phi_file]['similarity_pairs']\n",
    "similarity_scores = np.array(results[graph_phi_file]['most_similar_scores'])\n",
    "\n",
    "def get_similarity(graph1_idx, graph2_idx):\n",
    "    return gram_matrix[graph1_idx, graph2_idx]\n",
    "\n",
    "def get_non_zero_phi_elements(idx):\n",
    "    return phi_used[idx].nonzero()[1]\n",
    "\n",
    "def plot_similar_graphs(graph_idx, num_to_plot = 2):\n",
    "    most_similar = np.argsort(gram_matrix[graph_idx])\n",
    "    filtered = [idx for idx in most_similar if nx.number_of_nodes(X[idx]) > 0 and idx != graph_idx and get_similarity(graph_idx, idx) != 0]\n",
    "    similar_graph_idxs = np.array(filtered[-num_to_plot:])\n",
    "    graph_idxs = [graph_idx] + similar_graph_idxs.tolist()\n",
    "    similarities = [get_similarity(graph_idx, idx) for idx in graph_idxs]\n",
    "    similar_labels = [set(X[idx].nodes()) & set(X[graph_idx].nodes()) for idx in graph_idxs]\n",
    "    reference_graph = X[graph_idx]\n",
    "    \n",
    "    print(phi_used.shape[0], gram_matrix.shape[0])\n",
    "    print('Graph={}'.format(graph_idx))\n",
    "    print('NonZeroPhi={}'.format(get_non_zero_phi_elements(graph_idx)))\n",
    "    print('SimilarOwn={}'.format(get_similarity(graph_idx, graph_idx)))\n",
    "    print('SimilarIdxs={}'.format(similar_graph_idxs))\n",
    "    print('Similarities={}'.format(similarities))\n",
    "    print('SimilarLabels={}'.format(similar_labels))\n",
    "    \n",
    "    for similar_graph_idx, graph, text in [(graph_idx, X[graph_idx], X_text[graph_idx]) for graph_idx in graph_idxs]:\n",
    "        graph = graph.copy()\n",
    "        graph.remove_nodes_from(set(graph.nodes()) - set(reference_graph.nodes()))\n",
    "        print(Y[graph_idx], Y[similar_graph_idx])\n",
    "        fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE)\n",
    "        nx.draw_circular(graph, ax = ax, node_size = 14, with_labels = True, node_color = '#000000')\n",
    "        ax.text(0, 0, str(similar_graph_idx))\n",
    "\n",
    "for i in range(3):\n",
    "    random_choice = 1000\n",
    "    while nx.number_of_nodes(X[random_choice]) > 10:\n",
    "        random_choice = np.random.randint(0, len(X))\n",
    "    #random_choice = 1\n",
    "    plot_similar_graphs(random_choice)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Augment edges\n",
    "\n",
    "Add an edge from node1 to node2 if they are connected by a path of length N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK_LENGTH = 2\n",
    "dataset_name = 'ng20'\n",
    "\n",
    "for graph_cache_file in dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset_name):\n",
    "    if 'coo' not in graph_cache_file or 'all' in graph_cache_file: continue\n",
    "    print(graph_cache_file)\n",
    "    X_old, Y_old = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "    \n",
    "    # TODO\n",
    "    X_old, Y_old = X_old[:10], Y_old[:10]\n",
    "    \n",
    "    X, Y = copy.deepcopy(X_old), copy.deepcopy(Y_old)\n",
    "    for idx, graph in enumerate(X):\n",
    "        if idx % 100 == 0: sys.stdout.write('\\r{:3.0f}%'.format(idx / len(X) * 100))\n",
    "        if graph.number_of_edges() == 0 or graph.number_of_nodes() == 0: continue\n",
    "        shortest_paths = nx.all_pairs_shortest_path(graph, cutoff=WALK_LENGTH)\n",
    "        for source, target_dict in shortest_paths.items():\n",
    "            for target, path in target_dict.items():\n",
    "                graph.add_edge(source, target, attr_dict = {'weight': 1 / len(path)})\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of classes per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['dataset', 'label', 'counts'])\n",
    "df['counts'] = df['counts'].astype(np.uint64)\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    if 'ana' in dataset: continue\n",
    "    #if dataset not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    label_counter = collections.Counter(Y)\n",
    "    df = df.append(pd.DataFrame([(dataset, label, count) for label, count in label_counter.items()], columns = df.columns), )\n",
    "\n",
    "for dataset, items in df.groupby('dataset'):\n",
    "    fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE_BIG)\n",
    "    num_els = items.counts.sum()\n",
    "    stdd = (items.counts / num_els).std()\n",
    "    items = items.set_index('label')\n",
    "    items.sort_values('counts').counts.plot(kind = 'barh', title = 'Dataset: {}, stdd/#docs: {:.2f}'.format(dataset, stdd))\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves concept-maps and coo-graphs graph datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuples of: (dataset_name, graph_type, (X, Y))\n",
    "# For cooccurrence graphs, it will hold a (random) choice for each window size\n",
    "graph_datasets = []\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    print('{:30} start'.format(dataset))\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset)\n",
    "    gml_graph_cache = [x for x in graph_cache_files if 'concept' in x][0]\n",
    "    coo_graph_caches = [x for x in graph_cache_files if 'cooc' in x]\n",
    "    \n",
    "    def get_window_size(graph_cache_file):\n",
    "        return graph_cache_file.split('cooccurrence_')[1].split('_')[0]\n",
    "    \n",
    "    coo_graphs_by_window_size = collections.defaultdict(lambda: [])\n",
    "    for cache_file in coo_graph_caches:\n",
    "        coo_graphs_by_window_size[get_window_size(cache_file)].append(cache_file)\n",
    "    \n",
    "    X_cmap, Y_cmap = dataset_helper.get_dataset_cached(gml_graph_cache)\n",
    "    X_cmap = [x if isinstance(x, nx.Graph) else x[0] for x in X_cmap]\n",
    "    graph_datasets.append((dataset, 'CMap', (X_cmap, Y_cmap)))\n",
    "    for window_size, cached_files in sorted(coo_graphs_by_window_size.items(), key=lambda x: x[0]):\n",
    "        # Take random element from the co-occurence graph datasets\n",
    "        coo_graph_cache = np.random.choice(cached_files)\n",
    "        print('\\tRetrieving co-occurence graphs for window_size={} ({})'.format(window_size, coo_graph_cache))\n",
    "        graph_datasets.append((dataset, 'Coo - {}'.format(window_size), dataset_helper.get_dataset_cached(coo_graph_cache)))\n",
    "    print('{:30} finished'.format(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(lambda: [])\n",
    "for dataset, graph_type, (X, Y) in graph_datasets:\n",
    "    if graph_type != 'CMap': continue\n",
    "    print(dataset, graph_type)\n",
    "    for graph in X:\n",
    "        data['dataset'].append(dataset)\n",
    "        data['graph_type'].append(graph_type)\n",
    "        data['connected_components'].append(nx.number_connected_components(graph))\n",
    "        data['mean_connected_components_size'].append(np.mean([len(x) for x in nx.connected_components(graph)]) / nx.number_of_nodes(graph))\n",
    "\n",
    "df_connected_components = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, df_dataset in df_connected_components.groupby('dataset'):\n",
    "    #break\n",
    "    for graph_type, df in df_dataset.groupby('graph_type'):\n",
    "        if graph_type != 'CMap': continue\n",
    "        \n",
    "        # Mean connected components size\n",
    "        fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE)\n",
    "        df.mean_connected_components_size.plot(kind = 'hist', bins = 60, logy = True, normed = False, title = 'Dataset: {}, Graph Type: {}\\nmedian={:.1f}, mean={:.1f}'.format(dataset, 'Concept Map', df.mean_connected_components_size.median(), df.mean_connected_components_size.mean()))\n",
    "        ax.set_xlabel('mean connected component size')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize = (EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT - 1))\n",
    "        df.connected_components.plot(kind = 'hist', bins = 60, logy = True, normed = False, title = 'Dataset: {}, Graph Type: {}\\nmedian={:.0f}, mean={:.1f}'.format(dataset, 'Concept Map', df.connected_components.median(), df.connected_components.mean()))\n",
    "        ax.set_xlabel('# connected components')\n",
    "        ax.set_ylabel('Frequency (log)')\n",
    "        #ax.set_xticks(x_ticks)\n",
    "        fig.tight_layout()\n",
    "        #for ext in ['pdf', 'png']:\n",
    "        for ext in ['png']:\n",
    "            fig.savefig('tmp/hist-connected-components-{}-{}.{}'.format(dataset, graph_type, ext))\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = collections.defaultdict(lambda: [])\n",
    "for dataset, df_ in df_connected_components.groupby('dataset'):\n",
    "    data_['dataset'].append(dataset)\n",
    "    data_['connected_components_percentage_over_1'].append(len(df_[df_.connected_components > 1]) / len(df_) * 100)\n",
    "df_ = pd.DataFrame(data_).set_index('dataset').sort_index(ascending = False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT - 2.3))\n",
    "df_.connected_components_percentage_over_1.plot(kind = 'barh', ax = ax)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.grid('off')\n",
    "ax.set_xlabel('% of graphs with more than one connected component')\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/percentage_more_than_one_connected_component.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density, #nodes, #edges histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS = 60\n",
    "alpha = 0.6\n",
    "\n",
    "graph_metrics = [\n",
    "    #('density', lambda graph: nx.density(graph) if graph.number_of_nodes() > 0 else 0.0),\n",
    "    #('number of nodes', lambda graph: graph.number_of_nodes()),\n",
    "    #('number of edges', lambda graph: graph.number_of_edges()),\n",
    "    #('connected components', lambda graph: nx.number_connected_components(graph)),\n",
    "    #('num_nodes_div_num_edges', lambda graph:  graph.number_of_nodes() / graph.number_of_edges() if graph.number_of_edges() > 0 else -99),\n",
    "    ('#edges / #nodes', lambda graph:  graph.number_of_edges() / graph.number_of_nodes() if graph.number_of_nodes() > 0 else -99)\n",
    "]\n",
    "\n",
    "for metric_name, metric in graph_metrics:\n",
    "    metric_name_clean = re.sub(r'[^a-zA-Z\\d]', '', metric_name)\n",
    "    graph_metrics = []\n",
    "    for dataset, graph_type, (X, Y) in graph_datasets:\n",
    "        graph_metrics += [(dataset, graph_type, metric(graph)) for graph in X]\n",
    "\n",
    "    df = pd.DataFrame(graph_metrics, columns = ['dataset', 'graph_type', 'graph_metric'])\n",
    "    df = df[df.graph_metric > -10]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT - 1))\n",
    "    \n",
    "    metrics_ = df.graph_metric.tolist()\n",
    "    binwidth = (max(metrics_) - min(metrics_)) / NUM_BINS\n",
    "    bins = np.arange(min(metrics_), max(metrics_) + binwidth, binwidth)\n",
    "    a = df.groupby('graph_type').graph_metric.plot(kind = 'hist',bins = bins, alpha = alpha, ax = ax, logy = True, legend = True)\n",
    "    medians = df.groupby('graph_type').graph_metric.median()\n",
    "    for median in medians:\n",
    "        left, top = ax.transAxes.transform((0, 2.2))\n",
    "        ax.axvline(median, ymax = 1, linewidth=1, alpha = alpha, color='b', linestyle='dashed')\n",
    "        #ax.text(median, top, s = '{:.2f}'.format(median), fontdict={'horizontalalignment': 'center'}) #, transform = ax.transAxes)\n",
    "    ax.set_xlabel(metric_name)\n",
    "    ax.grid('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('tmp/graph-statistics/hist-{}.pdf'.format(metric_name_clean))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot examples of graph types\n",
    "concept map and co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(graph_datasets, columns = ['dataset', 'graph_type', 'graph_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GRAPHS_PER_TYPE = 3\n",
    "\n",
    "for dataset, data in df.groupby('dataset'):\n",
    "    fig, axes = plt.subplots(ncols=data.graph_type.value_counts().size, nrows=NUM_GRAPHS_PER_TYPE)\n",
    "\n",
    "    for idx, row_ax in enumerate(axes):\n",
    "        print('Row: {}/{}'.format(idx + 1, len(axes)))\n",
    "        for (_, item), ax in zip(data.iterrows(), row_ax):\n",
    "            if idx == 0:\n",
    "                ax.set_title(item.graph_type)\n",
    "            X, Y = item.graph_dataset\n",
    "\n",
    "            random_graph = None\n",
    "            while not random_graph or nx.number_of_nodes(random_graph) not in range(5, 10):\n",
    "                random_graph = np.random.choice(X)\n",
    "                \n",
    "            nx.draw_networkx(random_graph, ax = ax, node_size = 14, with_labels = False, node_color = '#000000')#, style = 'dotted')\n",
    "            \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.grid('off')\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_color('#FFFFFF')\n",
    "    \n",
    "    fig.tight_layout(h_pad=3, w_pad = 3)\n",
    "    fig.savefig('tmp/graph-examples.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot unique word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    text = []\n",
    "    for t in X:\n",
    "        text.append(t)\n",
    "    text = ' '.join(text)\n",
    "    text = text.lower().replace('\\n', ' ')\n",
    "    words = [x.strip() for x in text.split() if x.strip() != '']\n",
    "    unique_words = set(words)\n",
    "    word_counts.append((dataset_name, len(unique_words), len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if 'ana' in dataset_name: continue\n",
    "    print(dataset_name)\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    X_pp = preprocessing.preprocess_text_spacy(X, concat = False, only_nouns = False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(word_counts, columns = ['dataset', 'unique_words', 'words']).set_index('dataset').sort_values('unique_words')\n",
    "df['unique_words_ratio'] = df.unique_words / df.words\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "df[['unique_words', 'words']].plot(kind = 'barh', logx = True, title = 'Unique word count', ax = ax)\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "df.unique_words_ratio.plot(kind = 'barh', title = '#Unique words/#words', ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge node labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(labels)\n",
    "\n",
    "for (n, treshold), lookup in results.items():\n",
    "    cliques = coreference.get_cliques_from_lookup(lookup)\n",
    "    similarity_counter = {'similar': len(lookup.keys()), 'unsimilar': num_labels - len(lookup.keys())}\n",
    "    clique_lenghts = [len(x) for x in list(cliques.values())]\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n",
    "    fig.suptitle('Treshold: {}, N={}'.format(treshold, n), fontsize = 16)\n",
    "\n",
    "    pd.DataFrame(clique_lenghts).plot(ax = axes[0], kind = 'hist', logy = True, legend = False, title = \"Histogram of clique lengths\".format(treshold))\n",
    "    pd.DataFrame(list(similarity_counter.items()), columns = ['name', 'count']).set_index('name').plot(ax = axes[1], kind = 'bar', legend = False, title = '# of labels that have been merged vs. not merged')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    fig.savefig('tmp/{:.5f}.{}.png'.format(treshold, n), dpi = 120)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by(df, by, bins = 15, title = '', figsize = (12, 5), fontsize = 16):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for n, vals in df.groupby(by):\n",
    "        labels.append(n)\n",
    "        data.append(vals.clique_length)\n",
    "    ax.hist(data, bins = bins, alpha=0.7, label=labels, log = True)\n",
    "    fig.suptitle(title, fontsize = fontsize)\n",
    "    ax.legend(loc='upper right', fontsize = fontsize)\n",
    "    ax.set_xlabel('clique sizes')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    return fig, ax\n",
    "fig, ax = plot_by(df, 'n', title = 'Clique size histogram by n (all thresholds together)')\n",
    "fig.savefig('tmp/clique_size_by_n_all_thresholds.png', dpi = 120)\n",
    "fig, ax = plot_by(df, 'threshold', title = 'Clique size histogram by threshold (all n together)')\n",
    "fig.savefig('tmp/clique_size_by_threshold_all_n.png', dpi = 120)\n",
    "fig, ax = plot_by(df[df.threshold == 0.6], 'n', title = 'Clique size histogram by n (threshold=0.6)')\n",
    "fig.savefig('tmp/clique_size_by_n_threshold_0.6.png', dpi = 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lookup_file in glob('data/embeddings/graph-embeddings/*.threshold-*.*.label-lookup.npy'):\n",
    "    threshold, topn = get_treshold_and_topn_from_lookupfilename(lookup_file)\n",
    "    with open(lookup_file, 'rb') as f:\n",
    "        lookup = pickle.load(f)\n",
    "    for key in lookup.values():\n",
    "        if not isinstance(key, (str, int)):\n",
    "            print(\"?\")\n",
    "            break\n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup=lookup, title = 'threshold={}, topn={}'.format(threshold, topn))\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_cache_file = 'dataset_graph_gml_ng20-single.npy'\n",
    "X, Y = dataset_helper.get_dataset_cached('data/CACHE/{}'.format(graph_cache_file))\n",
    "X, Y = np.array(X, dtype=object), np.array(Y, dtype=object)\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(n_splits = 40, random_state=42)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    X_test, Y_test = X[test_index], Y[test_index]\n",
    "    break\n",
    "with open('data/CACHE/dataset_graph_gml_small-single.npy', 'wb') as f:\n",
    "    pickle.dump((X_test.tolist(), Y_test.tolist()), f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label occurrence histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = collections.defaultdict(lambda: [])\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    \n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    \n",
    "    has_already = collections.defaultdict(lambda: False)\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        graph_type = graph_helper.get_graph_type_from_filename(graph_cache_file)\n",
    "        assert graph_type\n",
    "        if has_already[TYPE_CONCEPT_MAP] and has_already[TYPE_COOCCURRENCE]: break\n",
    "        if has_already[graph_type]: continue\n",
    "        has_already[graph_type] = True\n",
    "\n",
    "        print('Loading dataset: {}'.format(graph_cache_file))\n",
    "        X_old, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        label_counter = collections.Counter()\n",
    "        graph_helper.convert_graphs_to_adjs_tuples(X_old)\n",
    "        for adj, labels in X_old:\n",
    "            label_counter.update(labels)\n",
    "        counts_ = list(label_counter.values())\n",
    "        counts['dataset'] += [dataset_name] * len(counts_)\n",
    "        counts['type'] += [graph_type] * len(counts_)\n",
    "        counts['counts'] += counts_\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(counts)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "axes = df.hist(log = True, bins = 120, by = 'dataset', ax = axes)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlim((0, 10000))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/label-distribution-per-dataset.png', dpi = EXPORT_DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(lambda: [])\n",
    "for dataset, df_ in df.groupby('dataset'):\n",
    "    data['dataset'].append(dataset)\n",
    "    for t, df__ in df_.groupby('type'):\n",
    "        data['percentage_labels_once_{}'.format(t)].append(len(df__[df__.counts == 1]) / len(df__) * 100)\n",
    "\n",
    "df_ = pd.DataFrame(data).set_index('dataset').sort_index(ascending = False)\n",
    "\n",
    "if False:\n",
    "    fig, ax = plt.subplots(figsize=(EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT - 1.5))\n",
    "    df_.plot(kind = 'barh', ax = ax)\n",
    "    ax.set_xlabel('% of labels that occur once')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlim(0, 110)\n",
    "    x_ticks = np.array(range(11)) * 10\n",
    "    ax.set_xticks(x_ticks);\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('tmp/percentage_one_label_occurrence.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_.sort_index()\n",
    "data = []\n",
    "for dataset, df__ in df_connected_components.groupby('dataset'):\n",
    "    percentage = len(df__[df__.connected_components <= 1]) / len(df__)\n",
    "    data.append(percentage)\n",
    "    print(dataset, percentage)\n",
    "    #df_.loc[dataset][] = percentage\n",
    "df_['percentage_of_one_connected_component_concept-map'] = data\n",
    "df_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
