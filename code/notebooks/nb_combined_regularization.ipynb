{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(\"notebook/js/notebook\").Notebook.prototype.scroll_to_bottom = function () {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 2\n",
    "VERBOSE = 11\n",
    "\n",
    "DATASET = 'ling-spam'\n",
    "DATASET = 'ng20'\n",
    "#DATASET = 'reuters-21578'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.classifiers import get_classifiers\n",
    "\n",
    "classifiers = get_classifiers()\n",
    "classifier_param_grid = dict(\n",
    "    classifier = classifiers,\n",
    "    classifier__max_iter= [2000],\n",
    "    classifier__C = [2e-2, 1e-2],#, 1e-1],#, 1e-2, 1e-1, 1],\n",
    "    classifier__tol = [1e-4],\n",
    "    #classifier__dual = [False],\n",
    "    #classifier__penalty = ['l1']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined graph and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv(X, Y, use_cv = False):\n",
    "    if use_cv:\n",
    "        return sklearn.model_selection.StratifiedKFold(\n",
    "            n_splits=3,\n",
    "            random_state=42,\n",
    "            shuffle=True\n",
    "        )\n",
    "    else:\n",
    "        _, _, _, _, X_train_i, X_test_i = train_test_split(X, Y, test_size=0.33, is_precomputed=False)\n",
    "        return [(X_train_i, X_test_i)]\n",
    "    \n",
    "def process(X, Y, estimator, param_grid):\n",
    "    gscv = sklearn.model_selection.GridSearchCV(estimator=estimator, param_grid=param_grid, cv=get_cv(X, Y), scoring='f1_macro', n_jobs=N_JOBS, verbose=VERBOSE, refit='f1_macro')\n",
    "    gscv_result = gscv.fit(X, Y)\n",
    "    return gscv_result\n",
    "\n",
    "def get_combined_graph_datasets(graph_cache_file_filter = None):\n",
    "    for graph_cache_file in dataset_helper.get_all_cached_graph_datasets():\n",
    "        if graph_cache_file_filter and graph_cache_file_filter not in graph_cache_file: continue\n",
    "        if 'concept' not in graph_cache_file: continue\n",
    "        X_combined, Y_combined = graph_helper.get_filtered_text_graph_dataset(graph_cache_file)\n",
    "        graphs = [g for (g, _, _) in X_combined]\n",
    "        empty_graphs = len([1 for g in graphs if nx.number_of_nodes(g) == 0 or nx.number_of_edges(g) == 0])\n",
    "        num_vertices = sum([nx.number_of_nodes(g) for g in graphs]) + empty_graphs\n",
    "        X_combined = [(graph, text) for (graph, text, _) in X_combined]\n",
    "        yield graph_cache_file, X_combined, Y_combined, num_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import graph_pipeline\n",
    "from transformers import fast_wl_graph_kernel_transformer\n",
    "\n",
    "pipeline, param_grid = graph_pipeline.get_combined_pipeline()\n",
    "\n",
    "param_grid_values = dict(\n",
    "    # Graph\n",
    "    features__fast_wl_pipeline__feature_extraction__fast_wl__h = [7],\n",
    "    features__fast_wl_pipeline__feature_extraction__fast_wl__round_to_decimals = [-1],\n",
    "    features__fast_wl_pipeline__feature_extraction__fast_wl__node_weight_function = [None, fast_wl_graph_kernel_transformer.degrees_metric, fast_wl_graph_kernel_transformer.pagerank_metric],\n",
    "    features__fast_wl_pipeline__feature_extraction__phi_picker__return_iteration = ['stacked'],\n",
    "    features__fast_wl_pipeline__feature_extraction__normalizer = [sklearn.preprocessing.MaxAbsScaler()],\n",
    "    # Text\n",
    "    features__text__vectorizer__vectorizer__binary = [True],\n",
    "    features__text__vectorizer__vectorizer__ngram_range = [(1, 1)],\n",
    "    features__text__vectorizer__vectorizer__stop_words = ['english']\n",
    ")\n",
    "\n",
    "combined_param_grid = dict(param_grid, **param_grid_values)\n",
    "\n",
    "\n",
    "estimator_param_grid = dict(classifier_param_grid, **combined_param_grid)\n",
    "estimator_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.classification_tasks import train_test_split\n",
    "\n",
    "for graph_cache_file, X_combined, Y_combined, num_vertices in get_combined_graph_datasets(DATASET):\n",
    "    print(graph_cache_file)\n",
    "    estimator_param_grid['features__fast_wl_pipeline__feature_extraction__fast_wl__phi_dim'] = [num_vertices]\n",
    "    gscv_result = process(X_combined, Y_combined, pipeline, estimator_param_grid)\n",
    "    print(gscv_result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gscv_result.cv_results_)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_param_grid = dict(estimator_param_grid)\n",
    "text_vectorizer = pipeline.named_steps['features'].transformer_list[0][1].named_steps['vectorizer']\n",
    "text_only_pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('preprocessing', None),\n",
    "    ('vectorizer', text_vectorizer),\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n",
    "# Only keep text and classifier params\n",
    "text_only_param_grid = {k.replace('features__text__vectorizer__', ''): v for k, v in text_only_param_grid.items() if k.startswith('classifier') or k.startswith('features__text')}\n",
    "text_only_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset != DATASET: continue\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    pipeline = text_only_pipeline.named_steps['vectorizer']\n",
    "    gscv_result_text_only = process(X, Y, pipeline, text_only_param_grid)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gscv_result.cv_results_)\n",
    "df_text = pd.DataFrame(gscv_result_text_only.cv_results_)\n",
    "\n",
    "df_text['type'] = 'text'\n",
    "df['type'] = 'combined'\n",
    "\n",
    "df_all = df_text.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_weight_function_name(x):\n",
    "    if callable(x):\n",
    "        return x.__name__\n",
    "    if x is None:\n",
    "        return 'None'\n",
    "    return ''\n",
    "    \n",
    "\n",
    "df_all['node_weight_function'] = df_all.param_features__fast_wl_pipeline__feature_extraction__fast_wl__node_weight_function.apply(get_node_weight_function_name)\n",
    "df_all.groupby(['type', 'node_weight_function']).mean_test_score.max().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(gscv_result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
