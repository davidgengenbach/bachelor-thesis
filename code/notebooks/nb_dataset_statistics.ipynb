{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(\"notebook/js/notebook\").Notebook.prototype.scroll_to_bottom = function () {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets statistics (Text and Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "\n",
    "\n",
    "#LIMIT_DATASET = ['ng20']\n",
    "#LIMIT_DATASET = ['ling-spam']\n",
    "LIMIT_DATASET = None\n",
    "filtered_datasets = []\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    if LIMIT_DATASET and dataset not in LIMIT_DATASET: continue\n",
    "    concept_maps = [x for x in dataset_helper.get_all_cached_graph_datasets(dataset, graph_type=TYPE_CONCEPT_MAP) if 'v2' in x or 'v3' in x]\n",
    "    cooccurence_graphs = [x for x in dataset_helper.get_all_cached_graph_datasets(dataset, graph_type=TYPE_COOCCURRENCE) if '_cooccurrence_1' in x]\n",
    "    if not len(concept_maps) or not len(cooccurence_graphs): continue\n",
    "    filtered_datasets.append((dataset, concept_maps[0], cooccurence_graphs[0]))\n",
    "data = collections.defaultdict(lambda: [])\n",
    "for dataset, concept_map, cooccurence_graph in helper.log_progress(filtered_datasets):\n",
    "    print('Dataset: {}'.format(dataset))\n",
    "\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    \n",
    "    # Add general dataset statistics\n",
    "    data['dataset'].append(dataset)\n",
    "    data['num_documents'].append(len(X))\n",
    "    data['num_classes'].append(len(set(Y)))\n",
    "\n",
    "    # Add text statistics\n",
    "    count_vec = CountVectorizer()\n",
    "    doc_vecs = count_vec.fit_transform(X)\n",
    "    \n",
    "    all_words = set(count_vec.vocabulary_.keys())\n",
    "    data['document_lengths'].append([len(x) for x in X])\n",
    "    data['num_words'].append(doc_vecs.sum())\n",
    "    data['num_unique_words'].append(len(all_words))\n",
    "    data['median_doc_length'].append(np.median([len(x) for x in X]))\n",
    "    data['median_words_per_doc'].append(np.median(np.squeeze(np.asarray(doc_vecs.sum(axis = 1)))))\n",
    "    \n",
    "    # Add graph statistics\n",
    "    for graph_type, graph_cache_file in [(TYPE_CONCEPT_MAP, concept_map), (TYPE_COOCCURRENCE, cooccurence_graph)]:\n",
    "        X_graph, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        X_graph = graph_helper.get_graphs_only(X_graph)\n",
    "        trans.transform(X_graph)\n",
    "        \n",
    "        all_labels = []\n",
    "        for _, l in X_graph:\n",
    "            all_labels += l\n",
    "\n",
    "        data['num_nodes_{}'.format(graph_type)].append(len(all_labels))\n",
    "        data['num_unique_nodes_labels_{}'.format(graph_type)].append(len(set(all_labels)))\n",
    "        data['num_edges_{}'.format(graph_type)].append(sum(len(adj.nonzero()[0]) for adj, labels in X_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph statistics, eg. ratio_edges_to_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data).set_index('dataset')\n",
    "df['ratio_unique_words'] = df.num_unique_words / df.num_nodes_cooccurrence\n",
    "\n",
    "for graph_type in [TYPE_CONCEPT_MAP, TYPE_COOCCURRENCE]:\n",
    "    df['ratio_edges_to_nodes_{}'.format(graph_type)] = df['num_edges_{}'.format(graph_type)] / df['num_nodes_{}'.format(graph_type)]\n",
    "    df['ratio_nodes_to_words_{}'.format(graph_type)] = df['num_nodes_{}'.format(graph_type)] / df.num_words\n",
    "\n",
    "    \n",
    "df['ratio_unique_words'] = df.num_unique_words / df.num_nodes_cooccurrence\n",
    "df['ratio_ratio_ratio_edges_to_nodes_cmap_to_coo'] = df['ratio_edges_to_nodes_concept_map'] / df['ratio_edges_to_nodes_cooccurrence']\n",
    "for col in ['median_doc_length', 'median_words_per_doc']:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "df = df[sorted(df.columns)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes per graph type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in GRAPH_TYPES:\n",
    "    df['nodes_per_graph_{}'.format(t)] = df['num_edges_{}'.format(t)] / df.num_documents\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create latex tables for text and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAME_COLS = {\n",
    "    'dataset': {\n",
    "        'num_classes': '# classes', \n",
    "        'num_documents': '# docs', \n",
    "        'median_words_per_doc': 'median #words/doc',\n",
    "        'ratio_unique_words': '#uniq. words/#words' \n",
    "    },\n",
    "    'graphs': {\n",
    "        'ratio_nodes_to_words_concept_map': '#nodes/#words cmap',\n",
    "        'ratio_nodes_to_words_cooccurrence': '#nodes/#words coo',\n",
    "        'ratio_edges_to_nodes_concept_map': '#edges/#nodes cmap', \n",
    "        'ratio_edges_to_nodes_cooccurrence': '#edges/#nodes coo', \n",
    "        'nodes_per_graph_cooccurrence': '#nodes/graph cmap',\n",
    "        'nodes_per_graph_concept_map': '#nodes/graph coo',\n",
    "    }\n",
    "}\n",
    "\n",
    "df.loc['mean'] = df.mean()\n",
    "\n",
    "for key, rename_cols in RENAME_COLS.items():\n",
    "    df_stats = df[list(rename_cols.keys())].rename(columns = rename_cols)\n",
    "    print(df_stats.to_latex(float_format = '%.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/dataset_statistics.npy', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/dataset_statistics.npy', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Occurrence graph statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-words vs. only-nouns compressino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1\n",
    "datasets_coo = collections.defaultdict(lambda: {})\n",
    "\n",
    "cooccurrence_graph_files = [x for x in dataset_helper.get_all_cached_graph_datasets() if 'cooccurrence' in x]\n",
    "\n",
    "for graph_cf in cooccurrence_graph_files:\n",
    "    dataset = filename_utils.get_dataset_from_filename(graph_cf)\n",
    "    window_size = filename_utils.get_cooccurrence_window_size_from_filename(graph_cf)\n",
    "    words = filename_utils.get_cooccurrence_words_from_filename(graph_cf)\n",
    "    if window_size != WINDOW_SIZE: continue\n",
    "    if dataset in datasets_coo and words in datasets_coo[dataset]: continue\n",
    "    print(graph_cf)\n",
    "    X, Y = dataset_helper.get_dataset_cached(graph_cf)\n",
    "    all_labels = graph_helper.get_all_node_labels(X)\n",
    "    datasets_coo[dataset][words] = all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(lambda: [])\n",
    "for dataset, words in datasets_coo.items():\n",
    "    for word, labels in words.items():\n",
    "        data['dataset'].append(dataset)\n",
    "        data['word'].append(word.replace('-', '_').replace('all', 'all_words'))\n",
    "        data['label_count'].append(len(labels))\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df = df.pivot(index = 'dataset', columns = 'word', values = 'label_count')\n",
    "df['word_ratio'] = df.only_nouns / df.all_words\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurring concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = collections.defaultdict(lambda: {})\n",
    "dataset = 'ling-spam'\n",
    "dataset = None\n",
    "for gcf in dataset_helper.get_all_cached_graph_datasets(dataset):#graph_type=TYPE_CONCEPT_MAP):\n",
    "    dataset = filename_utils.get_dataset_from_filename(gcf)\n",
    "    graph_type = graph_helper.get_graph_type_from_filename(gcf)\n",
    "    \n",
    "    if graph_type == TYPE_CONCEPT_MAP and 'v2' not in gcf: continue\n",
    "    if graph_type in all_labels[dataset]: continue\n",
    "    \n",
    "    print('{:30} {}'.format(dataset, graph_type))\n",
    "    X, Y = dataset_helper.get_dataset_cached(gcf)\n",
    "    X = graph_helper.convert_graphs_to_adjs_tuples(X, copy = True)\n",
    "    all_labels[dataset][graph_type] = [labels for _, labels in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(lambda: [])\n",
    "for dataset, graph_types in all_labels.items():\n",
    "    for graph_type, labels in graph_types.items():\n",
    "        labels_flat = helper.flatten_array(labels)\n",
    "        c = collections.Counter(labels_flat)\n",
    "        df_cmap_occurrences = pd.DataFrame(list(c.items()), columns = ['label', 'occurrences'])\n",
    "        data['dataset'] += [dataset] * len(c.keys())\n",
    "        data['label'] += c.keys()\n",
    "        data['occurrences'] += c.values()\n",
    "        data['num_docs'] += [len(labels)] * len(c.keys())\n",
    "        data['type'] += [graph_type] * len(c.keys())\n",
    "df_occs = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occ_only_once = ((df_occs[df_occs.occurrences <= 1].groupby(['dataset', 'type']).occurrences.value_counts() / df_occs.groupby('dataset').occurrences.sum()).groupby(['dataset', 'type']).sum()).unstack()#.plot(kind = 'barh')\n",
    "df_occ_only_once = df_occ_only_once[pd.isna(df_occ_only_once['concept-map']) == False]\n",
    "print(df_occ_only_once.to_latex())\n",
    "df_occ_only_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "last = 200#df_occs.occurrences.quantile(q=0.98)\n",
    "df_occs[df_occs.occurrences < last].groupby('dataset').occurrences.hist(bins = 120, ax = ax, alpha = 0.2, label = 'dataset')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
