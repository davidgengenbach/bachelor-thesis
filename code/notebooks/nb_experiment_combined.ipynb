{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: _Combined text-/graph features vs. text-only features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_VAL = '-'\n",
    "EXPERIMENT_NAME = 'experiment_combined'\n",
    "\n",
    "experiments = [\n",
    "    EXPERIMENT_NAME + x\n",
    "    for x in ['_same_label', '_with_splitted_words']\n",
    "]\n",
    "df = results_helper.get_experiments_by_names(experiments, fetch_predictions=True)\n",
    "df_ = results_helper.get_results(include_filter='text', filter_out_non_complete_datasets=None, fetch_predictions=True)\n",
    "df = df.append(df_).reset_index().fillna('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'combined', \n",
    "group_attr = ['dataset',  'text__vectorizer', 'graph__preprocessing', 'type']\n",
    "attr = 'prediction_score_f1_macro'\n",
    "attr = 'mean_test_f1_macro'\n",
    "grouped = df.groupby(group_attr)[attr].max().to_frame().unstack()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZER_TFIDF = 'TfidfVectorizer'\n",
    "VECTORIZER_COUNT = 'CountVectorizer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import significance_test_utils\n",
    "\n",
    "for dataset, df_ in df.groupby('dataset'):\n",
    "    print(dataset)\n",
    "    best = df.loc[df[(df.dataset == dataset)].groupby('type').prediction_score_f1_macro.idxmax()]\n",
    "    \n",
    "    if len(best) != 2:\n",
    "        print('\\tNot enough data. Skipping')\n",
    "        continue\n",
    "        \n",
    "    prediction_filenames = [best.loc[best.type == name].iloc[0].prediction_file for name in ['concept_map', 'text']]\n",
    "    diffs, score_a, score_b, global_difference, confidence = results_helper.calculate_significance(prediction_filenames[0], prediction_filenames[1])\n",
    "    \n",
    "    for k, v in [('Score A', score_a), ('Score B', score_b), ('Difference', global_difference), ('Confidence', confidence)]:\n",
    "        print('\\t{:20} {:9.4f}'.format(k, v))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_test_utils.plot_randomization_test_distribution_(diffs=diffs, global_diff=global_difference, p=confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, params in sorted(experiment_helper.get_all_param_grid_config_files().items(), key=lambda x: x[0]):\n",
    "    file = file.split('/')[-1]\n",
    "    print(file)\n",
    "    for task_name, params_ in params['params_per_type'].items():\n",
    "        if 'classifier__C' not in params_: continue\n",
    "        C = params_['classifier__C']\n",
    "        if not C: continue\n",
    "        print('\\t{:40} {}'.format(task_name, C))\n",
    "    #print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in results_helper.get_predictions_files() if EXPERIMENT_NAME in x and filename_utils.get_dataset_from_filename(x) == 'ng20' and 'graph_combined' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import significance_test_utils\n",
    "\n",
    "NUM_TRAILS = 10000\n",
    "metric = significance_test_utils.f1\n",
    "\n",
    "combinations = [\n",
    "    # TfidfVectorizer\n",
    "    (\n",
    "        # Combined\n",
    "        'result___experiment_combined__ng20__graph_combined__dataset_graph_concept_map_ng20-v3.npy',\n",
    "        \n",
    "        # Text-only\n",
    "        'result___experiment_combined__ng20__text.npy',\n",
    "    )\n",
    "]\n",
    "\n",
    "filenames = []\n",
    "for a, b in combinations:\n",
    "    filenames.append(a)\n",
    "    filenames.append(b)\n",
    "\n",
    "data = collections.defaultdict(lambda: [])\n",
    "\n",
    "predictions = {k.split('/')[-1]: v['results']['results'] for k, v in results_helper.get_predictions(filenames=filenames)}\n",
    "for filenames in combinations:\n",
    "    assert np.all([x in predictions for x in filenames])\n",
    "    models = [predictions[x] for x in filenames]\n",
    "    keys = ['Y_real', 'Y_pred', 'X_test']\n",
    "    assert np.all([len(models[0][key]) == len(models[1][key]) for key in keys])\n",
    "    y_true = models[0]['Y_real']\n",
    "    y_preds = [model['Y_pred'] for model in models]\n",
    "    y_pred_a, y_pred_b = y_preds\n",
    "    metric_real = [metric(y_true, y_pred) for y_pred in y_preds]\n",
    "    diff_global = metric_real[0] - metric_real[1]\n",
    "    \n",
    "    metrics = significance_test_utils.randomization_test(y_true, y_pred_a, y_pred_b, metric=significance_test_utils.f1, num_trails=NUM_TRAILS)\n",
    "    diffs = metrics[:, 0] - metrics[:, 1]\n",
    "    confidence = significance_test_utils.get_confidence(diff_global, diffs, num_trails=NUM_TRAILS, one_tail=True)\n",
    "\n",
    "    data['num_docs'].append(len(y_pred_a))\n",
    "    data['filename_a'].append(filenames[0])\n",
    "    data['filename_b'].append(filenames[1])\n",
    "    data['confidence'].append(confidence)\n",
    "    data['diffs'].append(diffs)\n",
    "    data['diff_global'].append(diff_global)\n",
    "    data['metric_a'].append(metric_real[0])\n",
    "    data['metric_b'].append(metric_real[1])\n",
    "    data['num_trails'].append(NUM_TRAILS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(data).set_index(['filename_a', 'filename_b'])\n",
    "df_[[x for x in df_.columns if x != 'diffs']].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (f_a, f_b), df__ in df_.iterrows():\n",
    "    diffs = df__.diffs\n",
    "    fig, ax = plt.subplots(figsize=(10, 3.4))\n",
    "    significance_test_utils.plot_randomization_test_distribution_(diffs, df__.diff_global, num_trails=df__.num_trails, p=df__.confidence, metric_name='f1 macro', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = collections.Counter()\n",
    "for idx, (a, b) in enumerate(zip(*y_preds)):\n",
    "    c['same' if a == b else 'not_same'] += 1\n",
    "c"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
