{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: _Combined text-/graph features vs. text-only features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_VAL = '-'\n",
    "EXPERIMENT_NAME = 'experiment_combined'\n",
    "experiment_data = experiment_helper.get_experiment_config_for(EXPERIMENT_NAME)\n",
    "param_grid = experiment_data['params_per_type']\n",
    "df = results_helper.get_results(filter_out_experiment=EXPERIMENT_NAME, filter_out_non_complete_datasets=False)\n",
    "\n",
    "df_ = results_helper.get_results(filter_out_experiment=EXPERIMENT_NAME + '_with_splitted', filter_out_non_complete_datasets=False)\n",
    "df = df.append(df_)\n",
    "\n",
    "df__ = results_helper.get_results(filter_out_experiment=EXPERIMENT_NAME + '_same_label', filter_out_non_complete_datasets=False)\n",
    "df = df.append(df__)\n",
    "\n",
    "df = df.fillna(NA_VAL)\n",
    "pipeline_helper.remove_complex_types(pipeline_helper.flatten_nested_params(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZER_TFIDF = 'TfidfVectorizer'\n",
    "VECTORIZER_COUNT = 'CountVectorizer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_types(df, types):\n",
    "    mask = np.zeros(len(df), dtype=bool)\n",
    "    for t in types: mask |= (df['type'] == t)\n",
    "    return mask\n",
    "\n",
    "df[only_types(df, [TYPE_CONCEPT_MAP, TYPE_COOCCURRENCE, 'text']) & (df.text__vectorizer != VECTORIZER_TFIDF)].groupby(['dataset', 'features__fast_wl_pipeline__feature_extraction__graph_preprocessing', 'combined', 'type']).mean_test_f1_macro.max().to_frame().unstack().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance test for _ng20_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in results_helper.get_predictions_files() if EXPERIMENT_NAME in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import significance_test_utils\n",
    "\n",
    "NUM_TRAILS = 5000\n",
    "metric = significance_test_utils.f1\n",
    "\n",
    "\n",
    "combinations = [\n",
    "    # TfidfVectorizer\n",
    "    (\n",
    "        # Combined\n",
    "        'result___experiment_combined_with_splitted_words__ng20__graph_combined__dataset_graph_concept_map_ng20-single-v2.npy',\n",
    "        #'result___experiment__graph_combined__ng20__graph_combined__dataset_graph_concept_map_ng20-single-v2.npy',\n",
    "        \n",
    "        # Text-only\n",
    "        'result___experiment_graph_combined__ng20__text.npy'\n",
    "    )\n",
    "]\n",
    "\n",
    "filenames = []\n",
    "for a, b in combinations:\n",
    "    filenames.append(a)\n",
    "    filenames.append(b)\n",
    "\n",
    "data = collections.defaultdict(lambda: [])\n",
    "\n",
    "predictions = {k.split('/')[-1]: v['results']['results'] for k, v in results_helper.get_predictions(filenames=filenames)}\n",
    "for filenames in combinations:\n",
    "    assert np.all([x in predictions for x in filenames])\n",
    "    models = [predictions[x] for x in filenames]\n",
    "    keys = ['Y_real', 'Y_pred', 'X_test']\n",
    "    assert np.all([len(models[0][key]) == len(models[1][key]) for key in keys])\n",
    "    y_true = models[0]['Y_real']\n",
    "    y_preds = [model['Y_pred'] for model in models]\n",
    "    y_pred_a, y_pred_b = y_preds\n",
    "    \n",
    "    metric_real = [metric(y_true, y_pred) for y_pred in y_preds]\n",
    "    diff_global = metric_real[0] - metric_real[1]\n",
    "    \n",
    "    metrics = significance_test_utils.randomization_test(y_true, y_pred_a, y_pred_b, metric=significance_test_utils.f1, num_trails=NUM_TRAILS)\n",
    "    diffs = metrics[:, 0] - metrics[:, 1]\n",
    "    confidence = significance_test_utils.get_confidence(diff_global, diffs, num_trails=NUM_TRAILS)\n",
    "\n",
    "    data['filename_a'].append(filenames[0])\n",
    "    data['filename_b'].append(filenames[1])\n",
    "    data['confidence'].append(confidence)\n",
    "    data['diffs'].append(diffs)\n",
    "    data['diff_global'].append(diff_global)\n",
    "    data['metric_a'].append(metric_real[0])\n",
    "    data['metric_b'].append(metric_real[1])\n",
    "    data['num_trails'].append(NUM_TRAILS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(data).set_index(['filename_a', 'filename_b'])\n",
    "df_[[x for x in df_.columns if x != 'diffs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (f_a, f_b), df__ in df_.iterrows():\n",
    "        diffs = df__.diffs\n",
    "    fig, ax = plt.subplots(figsize=(10, 3.4))\n",
    "    significance_test_utils.plot_randomization_test_distribution_(diffs, df__.diff_global, num_trails=df__.num_trails, p=df__.confidence, metric_name='f1 macro', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[filename_utils.get_dataset_from_filename(x) for x in dataset_helper.get_all_cached_graph_datasets(graph_type=TYPE_CONCEPT_MAP)]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
