{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: _Relabel graphs_\n",
    "\n",
    "Here, we relabel graph nodes which occur only non-frequent in the dataset. Eg. label 'something' occurs only once in the training set, so relabel with another label that is similar to it and occurs more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_helper.get_experiments_by_names(['experiment_relabel_{}'.format(x) for x in [\n",
    "    '5',\n",
    "#    '7',\n",
    "    '9',\n",
    "#    '99'\n",
    "]], fetch_predictions=True)\n",
    "\n",
    "df_ = results_helper.get_results(filter_out_non_complete_datasets=None, fetch_predictions=True)\n",
    "df = df.append(df_[(df_.filename.str.contains('__graph__')) & (df_.combined == False) & (df_.type == TYPE_CONCEPT_MAP)]).fillna('-').reset_index()\n",
    "df = results_helper.filter_out_datasets(df, lambda x: 'RelabelGraphsTransformer' in x.graph_preprocessing.values)\n",
    "print('# Results: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELABELED = True\n",
    "df[(df.filename.str.contains('relabeled') == False) & (df.dataset == 'ng20')][['filename', 'params']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'graph_preprocessing__threshold',\n",
    "attr = 'prediction_score_f1_macro'\n",
    "group_attr = 'graph_preprocessing__threshold'\n",
    "#attr = 'mean_test_f1_macro'\n",
    "#df = df[df[attr] != '-']\n",
    "d = df[(np.isin(df.type, (TYPE_CONCEPT_MAP))) & (df.kernel == 'wl')].groupby(['dataset',   group_attr])[attr].max().to_frame().unstack()#.reset_index()\n",
    "#d_ = d[['mean_test_f1_macro']].mean_test_f1_macro.reset_index().set_index('dataset')\n",
    "#d_['ratio_relabeled_vs_plain'] = (d_['RelabelGraphsTransformer'] - d_['-'])\n",
    "#d_\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import significance_test_utils\n",
    "attr_ = 'graph_preprocessing'\n",
    "confidences = collections.defaultdict(list)\n",
    "for dataset, df_ in df.groupby('dataset'):\n",
    "    assert group_attr in df_.columns\n",
    "    print(dataset)\n",
    "    best_ = df_.loc[df_.groupby(attr_)[attr].idxmax()]\n",
    "    if len(best_) != 2:\n",
    "        print('\\tNot enough data. Skipping')\n",
    "        continue\n",
    "    prediction_filenames = [best_.loc[best_[attr_] == name].iloc[0].prediction_file for name in [ 'RelabelGraphsTransformer', '-']]\n",
    "    diffs, score_a, score_b, global_difference, confidence = results_helper.calculate_significance(prediction_filenames[0], prediction_filenames[1])\n",
    "    \n",
    "    for k, v in [('Score A', score_a), ('Score B', score_b), ('Difference', global_difference), ('Confidence', confidence)]:\n",
    "        print('\\t{:20} {:9.4f}'.format(k, v))\n",
    "    print()\n",
    "    confidences['dataset'].append(dataset)\n",
    "    confidences['confidence'].append(confidence)\n",
    "    d.loc[dataset, 'confidence'] = confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.to_latex(float_format='%.3f'))\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
