{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *\n",
    "import experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(lambda: [])\n",
    "cmap_cache_files = dataset_helper.get_all_cached_graph_datasets(graph_type=TYPE_CONCEPT_MAP)\n",
    "for file in helper.log_progress(cmap_cache_files):\n",
    "    dataset = filename_utils.get_dataset_from_filename(file)\n",
    "    X, Y = dataset_helper.get_dataset_cached(file)\n",
    "    X = graph_helper.get_graphs_only(X)\n",
    "    \n",
    "    all_labels = set(graph_helper.get_all_node_labels(X))\n",
    "    data['dataset'] += [dataset] * len(all_labels)\n",
    "    data['labels'] += [str(x) for x in all_labels]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['num_words'] = df['labels'].str.split().apply(len)\n",
    "df = df.set_index(['dataset', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = len(df.reset_index().dataset.unique())\n",
    "fig, axes = plt.subplots(ncols=2, nrows=int(np.ceil(num_datasets / 2)), sharex=False)\n",
    "\n",
    "for ax, (dataset, df_) in zip(axes.flatten()[:num_datasets], df.groupby('dataset')):\n",
    "    df_.reset_index().set_index('labels').num_words.plot(kind='hist', bins=50, ax=ax, title=dataset)\n",
    "    ax.grid('off')\n",
    "    if df_.num_words.max() < 20:\n",
    "        labels = list(range(1, df_.num_words.max()))\n",
    "        ax.set_xticks(labels)\n",
    "        ax.set_xticklabels(labels)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('dataset').num_words.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english')) | set([',', 'one', 'two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_splitted = collections.defaultdict(lambda: [])\n",
    "for idx, df_ in df.reset_index().iterrows():\n",
    "    dataset = df_.dataset\n",
    "    if df_.num_words > 1:\n",
    "        all_labels_splitted[dataset] += df_['labels'].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(lambda: [])\n",
    "for dataset, single_words in all_labels_splitted.items():\n",
    "    c = collections.Counter(single_words)\n",
    "    data['dataset'] += [dataset] * len(c.keys())\n",
    "    keys, vals = zip(*c.items())\n",
    "    data['label'] += keys\n",
    "    data['occurrences'] += vals\n",
    "\n",
    "df_single_word_count = pd.DataFrame(data).sort_values('occurrences', ascending=False)\n",
    "df_single_word_count_no_stopwords = df_single_word_count[df_single_word_count['label'].apply(lambda x: x not in stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = len(df_single_word_count_no_stopwords.dataset.unique())\n",
    "fig, axes = plt.subplots(ncols=2, nrows=int(np.ceil(num_datasets / 2)))\n",
    "ax = df_single_word_count_no_stopwords.hist(column='occurrences', bins=120, by='dataset', log=True, ax = axes.flatten()[:num_datasets])\n",
    "for x in axes.flatten():\n",
    "    x.set_yscale('log')\n",
    "    x.grid(True,which=\"both\",ls=\"-\")\n",
    "fig.tight_layout()\n",
    "#ax.set_xlabel('word occurrences')\n",
    "#df_single_word_count[df_single_word_count.index.map(lambda x: x not in stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocessing\n",
    "\n",
    "df_ = df.reset_index()\n",
    "df_[(df_.dataset=='ng20') & (df_.num_words > 10)]\n",
    "\n",
    "df_['label_clean'] = df_['labels'].apply(preprocessing.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[(df_.dataset=='ng20') & (df_.num_words > 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting labels into new nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.graph_multi_word_label_splitter import GraphMultiWordLabelSplitter\n",
    "\n",
    "dataset = 'review_polarity'\n",
    "graph_type = TYPE_CONCEPT_MAP\n",
    "#graph_type = TYPE_COOCCURRENCE\n",
    "cmap_cache_file = dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset, graph_type=graph_type)[0]\n",
    "X, Y = dataset_helper.get_dataset_cached(cmap_cache_file)\n",
    "X = graph_helper.get_graphs_only(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = GraphMultiWordLabelSplitter(add_self_links=False, copy=True)\n",
    "X_ = trans.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (20, 10)\n",
    "candidates = [idx for idx, graph in enumerate(X) if len(graph.nodes()) < 10]\n",
    "idx = np.random.choice(candidates)\n",
    "print(idx)\n",
    "graph = X[idx]\n",
    "graph_ = X_[idx]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "for ax, g, title in zip(axes, [graph, graph_], ['Before', 'After']):\n",
    "    pos = nx.layout.shell_layout(g)\n",
    "    nx.draw_networkx(g, pos=pos, node_size=3, ax = ax)\n",
    "    edges = [(source, node, data['name']) for source, node, data in g.edges(data=True)]\n",
    "    edges_ = {(source, node): label for source, node, label in edges}\n",
    "    nx.draw_networkx_edge_labels(g, pos=pos, edge_labels=edges_, ax = ax)\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(title)\n",
    "fig.suptitle('Splitting multi-word node labels')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.92)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
