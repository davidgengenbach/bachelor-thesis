{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined graph- and text features\n",
    "\n",
    "Here we train a _Perceptron_ on the combined features and look at the trained weights to gain an insight into the importance of the individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments\n",
    "import experiments.task_runner\n",
    "from experiments import task_runner, task_helper\n",
    "import sklearn.model_selection\n",
    "from transformers.pipelines.classifiers import get_classifier_params\n",
    "from transformers.pipelines import pipeline_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = 'graph_combined'\n",
    "dataset = 'ng20'\n",
    "graph_type = 'concept-map'\n",
    "version = 'v3'\n",
    "\n",
    "tasks = experiments.get_filtered_tasks(task_type=task_type, dataset=dataset, task_name_filter=version)\n",
    "filtered_tasks = [t for t in tasks if version in t.name]\n",
    "assert len(filtered_tasks) == 1\n",
    "\n",
    "task = filtered_tasks[0]\n",
    "X, Y, estimator, param_grid_ = task.fn()\n",
    "X, Y = np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, stratify = Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "param_grid = task_helper.add_classifier_to_params(param_grid_)\n",
    "param_grid = dict(param_grid, **dict(\n",
    "    classifier=[sklearn.linear_model.Perceptron()],\n",
    "    classifier__penalty=['l1', 'l2'],\n",
    "    features__text__vectorizer__vectorizer__binary=[True],\n",
    "    features__fast_wl_pipeline__feature_extraction__feature_extraction__fast_wl__ignore_label_order=[True],\n",
    "    features__fast_wl_pipeline__feature_extraction__feature_extraction__fast_wl__use_early_stopping=[False],\n",
    "    features__fast_wl_pipeline__feature_extraction__feature_extraction__phi_picker__use_zeroth=[True],\n",
    "    features__fast_wl_pipeline__feature_extraction__feature_extraction__fast_wl__h=[5],\n",
    "))\n",
    "\n",
    "del param_grid['classifier__C']\n",
    "for k, v in sorted(pipeline_helper.remove_complex_types(param_grid).items(), key=lambda x: x[0]):\n",
    "    print('{:100} {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "\n",
    "for params in sklearn.model_selection.ParameterGrid(param_grid):\n",
    "    penalty = params['classifier__penalty']\n",
    "    \n",
    "    print('Regularization'.format(penalty.upper()))\n",
    "    clf = sklearn.base.clone(estimator)\n",
    "    clf.set_params(**params)\n",
    "    \n",
    "    print('\\tFitting')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    print('\\tPredicting')\n",
    "    for name, y_true, x in [('train', Y_train, X_train), ('test', Y_test, X_test)]:\n",
    "        y_pred = clf.predict(x)\n",
    "        f1_score = sklearn.metrics.f1_score(y_true, y_pred, average='macro')\n",
    "        print('\\t\\tf1_macro {:6} {:.3f}'.format(name, f1_score))\n",
    "\n",
    "    coefs = np.copy(clf.named_steps['classifier'].coef_)\n",
    "    estimators.append((params, clf, coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve feature lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fast_wl_vectorizer(pipeline):\n",
    "    return get_feature_transformer(pipeline, 'fast_wl_pipeline').named_steps['feature_extraction'].named_steps['feature_extraction'].named_steps['fast_wl']\n",
    "\n",
    "def get_feature_transformer(pipeline, transformer_name):\n",
    "    return [pipe for name, pipe in pipeline.named_steps['features'].transformer_list if name == transformer_name][0]\n",
    "\n",
    "def get_text_vectorizer(pipeline):\n",
    "    return get_feature_transformer(pipeline, 'text').named_steps['vectorizer'].named_steps['vectorizer']\n",
    "\n",
    "def get_text_features(pipeline):\n",
    "    return get_text_vectorizer(pipeline).vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create combined graph/text vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = estimators[0][1]\n",
    "features = estimator.named_steps['features'].fit_transform(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = get_text_features(estimator)\n",
    "trans_fast_wl = get_fast_wl_vectorizer(estimator)\n",
    "\n",
    "len_features_combined = features.shape[1]\n",
    "len_text_features = len(text_features)\n",
    "len_graph_features_simple = trans_fast_wl.phi_list[-1].shape[1]\n",
    "len_graph_features = len_features_combined - len_text_features\n",
    "# Do not use_zeroth? Then -1\n",
    "len_graph_fast_wl_iterations = len(trans_fast_wl.phi_list)\n",
    "assert (len_graph_fast_wl_iterations * len_graph_features_simple) == len_graph_features\n",
    "assert len_graph_features + len_text_features == len_features_combined\n",
    "\n",
    "lines = []\n",
    "lines.append(len_text_features)\n",
    "lines += [len_text_features + ((i + 1) * len_graph_features_simple) for i in range(len_graph_fast_wl_iterations)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity of graph and text features\n",
    "\n",
    "The ratio between graph- and text features summed up is basically a measurement of the importance of the distinct features when also looking at the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sum = np.squeeze(np.asarray(np.sum(features, axis=0)))\n",
    "sum_text_features = np.sum(feature_sum[:len_text_features])\n",
    "sum_graph_features = np.sum(feature_sum[len_text_features:])\n",
    "\n",
    "print('Sum of all features\\n\\tgraph/text={:.3f}'.format(sum_graph_features / sum_text_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron coefficients histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_estimator = estimators[0]\n",
    "used_coefs = used_estimator[2]\n",
    "# Sum up all coefs\n",
    "coefs = np.sum(used_coefs, axis = 0)\n",
    "idx_2_text = {idx: text for text, idx in text_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lens = [0] + lines\n",
    "df_features = pd.DataFrame(columns = ['label', 'coef'])\n",
    "for idx, (start, end) in enumerate(zip(features_lens[:-1], features_lens[1:])):\n",
    "    label = 'text' if idx == 0 else 'graph'\n",
    "    els = coefs[start:end]\n",
    "    df_ = pd.DataFrame(dict(label = [label] * len(els), coef = els))\n",
    "    df_features = pd.concat([df_features, df_])\n",
    "\n",
    "hist, bin_edges = np.histogram(df_features.coef, bins = 100)\n",
    "fig, ax = plt.subplots(figsize = (EXPORT_FIG_WIDTH_BIG, EXPORT_FIG_HEIGHT_BIG - 2))\n",
    "for (feature_label, df_) in df_features.groupby('label'):\n",
    "    df_.coef.plot(kind='hist', ax = ax, label = feature_label, logy = True, alpha = 0.7, bins = bin_edges, legend = True, stacked = True)\n",
    "ax.set_xlabel('Perceptron coefficient value')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_of_coefs(coefs, indices):\n",
    "    vals = coefs[0, indices]\n",
    "    vals_plus = vals[vals > 0]\n",
    "    vals_minus = vals[vals < 0]\n",
    "    return np.sum(vals_plus), np.sum(vals_minus)\n",
    "\n",
    "feature_range = [('text', len_text_features)] + [('graph_{}'.format(i + 1), len_graph_features_simple) for i in range(len_graph_fast_wl_iterations)]\n",
    "\n",
    "data = collections.defaultdict(lambda: [])\n",
    "for params, clf, coefs in estimators:\n",
    "    current = 0\n",
    "    vals = []\n",
    "    for name, num_features in feature_range:\n",
    "        num_features -= 1\n",
    "        end = current + num_features\n",
    "        vals.append(((current, end), get_sum_of_coefs(coefs, list(range(current, end)))))\n",
    "        current = end\n",
    "    \n",
    "    for (name, num_features), ((start, end), (val_plus, val_minus)) in zip(feature_range, vals):\n",
    "        data['type'].append(params['classifier__penalty'])\n",
    "        data['label'].append(name)\n",
    "        data['plus'].append(val_plus)\n",
    "        data['minus'].append(val_minus)\n",
    "        data['start'].append(start)\n",
    "        data['end'].append(end)\n",
    "        data['num_features'].append(num_features)\n",
    "\n",
    "df_vals = pd.DataFrame(data).set_index('label')\n",
    "df_vals['absolute'] = df_vals.minus.abs() + df_vals.plus\n",
    "df_vals['val_per_feature'] = df_vals.absolute / df_vals.num_features\n",
    "df_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11, 6), nrows=2, sharex=True)\n",
    "\n",
    "for ax, (title, df_) in zip(axes, df_vals.groupby('type')):\n",
    "    if len(df_[df_.index == 'graph_all']): continue\n",
    "    df_ = df_.sort_index()\n",
    "    graph_features = df_[df_.index.str.contains('graph')]\n",
    "    sum_ = graph_features.sum().to_frame().T\n",
    "    sum_.index = ['graph_all']\n",
    "    df_ = df_.append(sum_).sort_index()\n",
    "    df_.set_index(df_.index.map(lambda x: x.replace('graph_', 'Graph ').replace('all', '(All)').title()), inplace=True)\n",
    "    df_.absolute.plot(kind='barh', ax = ax, log=True, title='Regularization: {}'.format(title.upper()))\n",
    "    for idx, (label, df__) in enumerate(df_.iterrows()):\n",
    "        val = df__.absolute\n",
    "        ax.text(val * 0.9, idx, '{:.0f}'.format(val), fontdict=dict(horizontalalignment='right', verticalalignment='center', weight='bold'), color='white')\n",
    "    \n",
    "for ax in axes:\n",
    "    ax.set_xlabel('sum of absolute coefficient values (log)')\n",
    "    ax.grid(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "save_fig(fig, 'combined_coefs_l1_l2_regularization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    gscv = sklearn.model_selection.GridSearchCV(estimator, param_grid=param_grid, scoring='f1_macro', cv=cv, verbose=2)\n",
    "    gscv_result = gscv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coef heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefs_(coefs, top=100):\n",
    "    coefs_idx = np.argsort(coefs)\n",
    "    highest = coefs_idx[-top - 1:]\n",
    "    lowest = coefs_idx[:top]\n",
    "    highest_vals = coefs[highest]\n",
    "    lowest_vals = coefs[lowest]\n",
    "    fig, ax = plt.subplots()\n",
    "    labels = np.concatenate((highest, lowest))\n",
    "    #labels = ['g' if x > len(text_features) else idx_2_text[x] for x in labels]\n",
    "    labels = ['G' if x > len(text_features) else 'T' for x in labels]\n",
    "    pd.DataFrame(dict(idx=labels, vals=list(highest_vals) + list(lowest_vals))).set_index('idx').sort_values('vals').vals.plot(kind = 'bar', ax = ax)\n",
    "    ax.grid('off')\n",
    "    return fig, ax\n",
    "\n",
    "plot_coefs_(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmaps = ['viridis', 'plasma', 'inferno', 'magma', 'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds', 'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu', 'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn',  'binary', 'gist_yarg', 'gist_gray', 'gray', 'bone', 'pink', 'spring', 'summer', 'autumn', 'winter', 'cool', 'Wistia', 'hot', 'afmhot', 'gist_heat', 'copper']\n",
    "\n",
    "def plot_coefs_heatmap(coefs, log=False, lines = [], cmap = None, fig = None, ax = None):\n",
    "    _coefs = np.copy(coefs)\n",
    "    \n",
    "    size = _coefs.shape[0]\n",
    "    new_size = int(np.floor(np.sqrt(_coefs.shape[0]))) + 1\n",
    "    added = np.power(new_size, 2) - size\n",
    "    _coefs = np.append(_coefs, [0] * added)\n",
    "    _coefs = _coefs.reshape(new_size, -1)\n",
    "    \n",
    "    \n",
    "    if log:\n",
    "        _coefs = np.log(_coefs)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    fig = ax.get_figure()\n",
    "    img = ax.imshow(_coefs, cmap=plt.get_cmap(cmap))\n",
    "    ax.grid('off')\n",
    "    \n",
    "    for line_y in lines:\n",
    "        ax.axhline(line_y / new_size)\n",
    "    \n",
    "    fig.colorbar(img)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "for cmap in cmaps:\n",
    "    ax = plot_coefs_heatmap(coefs, lines = lines, cmap=cmap)\n",
    "    ax.set_title(cmap)\n",
    "    ax.get_figure().tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
