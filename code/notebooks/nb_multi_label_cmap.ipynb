{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *\n",
    "import experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(lambda: [])\n",
    "cmap_cache_files = dataset_helper.get_all_cached_graph_datasets(graph_type=TYPE_CONCEPT_MAP)\n",
    "for file in helper.log_progress(cmap_cache_files):\n",
    "    dataset = filename_utils.get_dataset_from_filename(file)\n",
    "    X, Y = dataset_helper.get_dataset_cached(file)\n",
    "    X = graph_helper.get_graphs_only(X)\n",
    "    \n",
    "    all_labels = set(graph_helper.get_all_node_labels(X))\n",
    "    data['dataset'] += [dataset] * len(all_labels)\n",
    "    data['labels'] += [str(x) for x in all_labels]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['num_words'] = df['labels'].str.split().apply(len)\n",
    "df = df.set_index(['dataset', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = len(df.reset_index().dataset.unique())\n",
    "fig, axes = plt.subplots(ncols=2, nrows=int(np.ceil(num_datasets / 2)), sharex=False)\n",
    "\n",
    "for ax, (dataset, df_) in zip(axes.flatten()[:num_datasets], df.groupby('dataset')):\n",
    "    print(dataset)\n",
    "    df_.reset_index().set_index('labels').num_words.plot(kind='hist', bins=50, ax=ax, title=dataset)\n",
    "    ax.grid('off')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('dataset').num_words.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english')) | set([',', 'one', 'two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_splitted = collections.defaultdict(lambda: [])\n",
    "for idx, df_ in df.reset_index().iterrows():\n",
    "    dataset = df_.dataset\n",
    "    if df_.num_words > 1:\n",
    "        all_labels_splitted[dataset] += df_['labels'].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(lambda: [])\n",
    "for dataset, single_words in all_labels_splitted.items():\n",
    "    c = collections.Counter(single_words)\n",
    "    data['dataset'] += [dataset] * len(c.keys())\n",
    "    keys, vals = zip(*c.items())\n",
    "    data['label'] += keys\n",
    "    data['occurrences'] += vals\n",
    "\n",
    "df_single_word_count = pd.DataFrame(data).sort_values('occurrences', ascending=False)\n",
    "df_single_word_count_no_stopwords = df_single_word_count[df_single_word_count['label'].apply(lambda x: x not in stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = len(df_single_word_count_no_stopwords.dataset.unique())\n",
    "fig, axes = plt.subplots(ncols=2, nrows=int(np.ceil(num_datasets / 2)))\n",
    "ax = df_single_word_count_no_stopwords.hist(column='occurrences', bins=120, by='dataset', log=True, ax = axes.flatten()[:num_datasets])\n",
    "for x in axes.flatten():\n",
    "    x.set_yscale('log')\n",
    "    x.grid(True,which=\"both\",ls=\"-\")\n",
    "fig.tight_layout()\n",
    "#ax.set_xlabel('word occurrences')\n",
    "#df_single_word_count[df_single_word_count.index.map(lambda x: x not in stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocessing\n",
    "\n",
    "df_ = df.reset_index()\n",
    "df_[(df_.dataset=='ng20') & (df_.num_words > 10)]\n",
    "\n",
    "df_['label_clean'] = df_['labels'].apply(preprocessing.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[(df_.dataset=='ng20') & (df_.num_words > 10)]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
