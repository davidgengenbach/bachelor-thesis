{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(\"notebook/js/notebook\").Notebook.prototype.scroll_to_bottom = function () {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets statistics (Text and Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "preprocessor = transformers.SimplePreProcessingTransformer()\n",
    "trans = transformers.NxGraphToTupleTransformer()\n",
    "\n",
    "UNDIRECTED = True\n",
    "LIMIT_DATASET = None\n",
    "filtered_datasets = []\n",
    "for dataset in dataset_helper.get_dataset_names_with_concept_map():\n",
    "    if LIMIT_DATASET and dataset not in LIMIT_DATASET: continue\n",
    "    concept_map = dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset, graph_type=TYPE_CONCEPT_MAP)[0]\n",
    "    cooccurence_graphs = [x for x in dataset_helper.get_all_cached_graph_datasets(dataset, graph_type=TYPE_COOCCURRENCE) if '_cooccurrence_1' in x and '_all_' in x]\n",
    "    if not len(cooccurence_graphs):\n",
    "        print('WARNING: No co-occurrence graphs found for: {}'.format(dataset))\n",
    "        continue\n",
    "    \n",
    "    filtered_datasets.append((dataset, concept_map, cooccurence_graphs[0]))\n",
    "\n",
    "data = collections.defaultdict(lambda: [])\n",
    "for dataset, concept_map, cooccurence_graph in helper.log_progress(filtered_datasets):\n",
    "    print('Dataset: {}'.format(dataset))\n",
    "\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    # Add general dataset statistics\n",
    "    data['dataset'].append(dataset)\n",
    "    data['num_documents'].append(len(X))\n",
    "    data['num_classes'].append(len(set(Y)))\n",
    "\n",
    "    print('\\t', 'text')\n",
    "    X = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Add text statistics\n",
    "    count_vec = CountVectorizer()\n",
    "    doc_vecs = count_vec.fit_transform(X)\n",
    "    \n",
    "    all_words = set(count_vec.vocabulary_.keys())\n",
    "    data['document_lengths'].append([len(x) for x in X])\n",
    "    data['num_words'].append(doc_vecs.sum())\n",
    "    data['num_unique_words'].append(len(all_words))\n",
    "    data['median_doc_length'].append(np.median([len(x) for x in X]))\n",
    "    data['median_words_per_doc'].append(np.median(np.squeeze(np.asarray(doc_vecs.sum(axis = 1)))))\n",
    "    \n",
    "    # Add graph statistics\n",
    "    for graph_type, graph_cache_file in [(TYPE_CONCEPT_MAP, concept_map), (TYPE_COOCCURRENCE, cooccurence_graph)]:\n",
    "        print('\\t', graph_type)\n",
    "        X_graph, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        X_graph = graph_helper.get_graphs_only(X_graph)\n",
    "        \n",
    "        if UNDIRECTED:\n",
    "            X_graph = [nx.Graph(x) for x in X_graph]\n",
    "        \n",
    "        all_labels = [x.nodes() for x in X_graph]\n",
    "        all_labels_flat = list(chain.from_iterable(all_labels))\n",
    "        num_labels = len(all_labels_flat)\n",
    "        all_labels_unique = set(all_labels_flat)\n",
    "        num_edges = sum([nx.number_of_edges(x) for x in X_graph])\n",
    "        \n",
    "        data['num_nodes_{}'.format(graph_type)].append(num_labels)\n",
    "        data['num_unique_nodes_labels_{}'.format(graph_type)].append(len(all_labels_unique))\n",
    "        data['num_edges_{}'.format(graph_type)].append(num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/dataset_statistics.npy', 'wb') as f:\n",
    "    pickle.dump(dict(**data), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph statistics, eg. ratio_edges_to_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data).set_index('dataset')\n",
    "df['ratio_unique_words'] = df.num_unique_words / df.num_nodes_cooccurrence\n",
    "\n",
    "for graph_type in [TYPE_CONCEPT_MAP, TYPE_COOCCURRENCE]:\n",
    "    df['ratio_edges_to_nodes_{}'.format(graph_type)] = df['num_edges_{}'.format(graph_type)] / df['num_nodes_{}'.format(graph_type)]\n",
    "    df['ratio_nodes_to_words_{}'.format(graph_type)] = df['num_nodes_{}'.format(graph_type)] / df.num_words\n",
    "\n",
    "    \n",
    "df['ratio_unique_words'] = df.num_unique_words / df.num_nodes_cooccurrence\n",
    "df['ratio_ratio_ratio_edges_to_nodes_cmap_to_coo'] = df['ratio_edges_to_nodes_concept_map'] / df['ratio_edges_to_nodes_cooccurrence']\n",
    "for col in ['median_doc_length', 'median_words_per_doc']:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "df = df[sorted(df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes per graph type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in GRAPH_TYPES:\n",
    "    df['nodes_per_graph_{}'.format(t)] = df['num_nodes_{}'.format(t)] / df.num_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create latex tables for text and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words_k'] = (df.num_words / 1000).map(lambda x: '{:,}k'.format(int(x)).replace(',', '.')) \n",
    "df['num_documents_k'] = (df.num_documents).map(lambda x: '{:,}'.format(int(x)).replace(',', '.')) \n",
    "df['nodes_per_graph_concept_map_str'] = (df.num_words).map(lambda x: '{:.0f}'.format(x)) \n",
    "df['nodes_per_graph_cooccurrence_str'] = (df.num_words).map(lambda x: '{:.0f}'.format(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAME_COLS = {\n",
    "    'dataset': {\n",
    "        'num_classes': '# classes', \n",
    "        'num_documents_k': '# docs', \n",
    "        'num_words_k': '# words',\n",
    "        'median_words_per_doc': 'median #words/doc',\n",
    "        'ratio_unique_words': '#uniq. words/#words',\n",
    "    },\n",
    "    'graphs': {\n",
    "        'ratio_nodes_to_words_concept_map': '#nodes/#words cmap',\n",
    "        'ratio_nodes_to_words_cooccurrence': '#nodes/#words coo',\n",
    "        'ratio_edges_to_nodes_concept_map': '#edges/#nodes cmap', \n",
    "        'ratio_edges_to_nodes_cooccurrence': '#edges/#nodes coo', \n",
    "        'nodes_per_graph_concept_map': '#nodes/graph cmap',\n",
    "        'nodes_per_graph_cooccurrence': '#nodes/graph coo',\n",
    "    }\n",
    "}\n",
    "\n",
    "df.loc['mean'] = df.mean()\n",
    "\n",
    "no_float_cols = ['nodes_per_graph_concept_map']\n",
    "\n",
    "for key, rename_cols in RENAME_COLS.items():\n",
    "    df_ = df\n",
    "    if key == 'dataset':\n",
    "        df_ = df_[df_.index !='mean']\n",
    "    df_stats = df_[list(rename_cols.keys())].rename(columns = rename_cols)\n",
    "    print(df_stats.to_latex(float_format = '%.2f').replace('.0 &', ' &'))\n",
    "    display(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/dataset_statistics.npy', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/dataset_statistics.npy', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Occurrence graph statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-words vs. only-nouns compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1\n",
    "datasets_coo = collections.defaultdict(lambda: {})\n",
    "\n",
    "cooccurrence_graph_files = [x for x in dataset_helper.get_all_cached_graph_datasets() if 'cooccurrence' in x]\n",
    "\n",
    "cmap_datasets = dataset_helper.get_dataset_names_with_concept_map()\n",
    "for graph_cf in cooccurrence_graph_files:\n",
    "    dataset = filename_utils.get_dataset_from_filename(graph_cf)\n",
    "    if dataset not in cmap_datasets: continue\n",
    "    window_size = filename_utils.get_cooccurrence_window_size_from_filename(graph_cf)\n",
    "    words = filename_utils.get_cooccurrence_words_from_filename(graph_cf)\n",
    "    if window_size != WINDOW_SIZE: continue\n",
    "    if dataset in datasets_coo and words in datasets_coo[dataset]: continue\n",
    "    print(graph_cf)\n",
    "    X, Y = dataset_helper.get_dataset_cached(graph_cf)\n",
    "    all_labels = graph_helper.get_all_node_labels(X)\n",
    "    datasets_coo[dataset][words] = all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = collections.defaultdict(lambda: [])\n",
    "for dataset, words in datasets_coo.items():\n",
    "    for word, labels in words.items():\n",
    "        data_['dataset'].append(dataset)\n",
    "        data_['word'].append(word.replace('-', '_').replace('all', 'all_words'))\n",
    "        data_['label_count'].append(len(labels))\n",
    "    \n",
    "df = pd.DataFrame(data_)\n",
    "df = df.pivot(index = 'dataset', columns = 'word', values = 'label_count')\n",
    "df['word_ratio'] = df.only_nouns / df.all_words\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurring concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = collections.defaultdict(lambda: {})\n",
    "dataset = 'ling-spam'\n",
    "dataset = None\n",
    "for gcf in dataset_helper.get_all_cached_graph_datasets(graph_type=TYPE_CONCEPT_MAP):\n",
    "    dataset = filename_utils.get_dataset_from_filename(gcf)\n",
    "    graph_type = graph_helper.get_graph_type_from_filename(gcf)\n",
    "\n",
    "    if graph_type in all_labels[dataset]: continue\n",
    "    \n",
    "    print('{:30} {}'.format(dataset, graph_type))\n",
    "    X, Y = dataset_helper.get_dataset_cached(gcf)\n",
    "    X = graph_helper.convert_graphs_to_adjs_tuples(X, copy = True)\n",
    "    all_labels[dataset][graph_type] = [labels for _, labels in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = collections.defaultdict(lambda: [])\n",
    "for dataset, graph_types in all_labels.items():\n",
    "    for graph_type, labels in graph_types.items():\n",
    "        labels_flat = list(chain.from_iterable(labels))\n",
    "        c = collections.Counter(labels_flat)\n",
    "        df_cmap_occurrences = pd.DataFrame(list(c.items()), columns = ['label', 'occurrences'])\n",
    "        data_['dataset'] += [dataset] * len(c.keys())\n",
    "        data_['label'] += c.keys()\n",
    "        data_['occurrences'] += c.values()\n",
    "        data_['num_docs'] += [len(labels)] * len(c.keys())\n",
    "        data_['type'] += [graph_type] * len(c.keys())\n",
    "df_occs = pd.DataFrame(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occ_only_once = (\n",
    "    (\n",
    "        df_occs[df_occs.occurrences == 1].groupby(['dataset', 'type']).occurrences.value_counts()\n",
    "        / \n",
    "        df_occs.groupby('dataset').occurrences.sum()\n",
    "    ).groupby(['dataset', 'type']).sum()\n",
    ").unstack()\n",
    "\n",
    "df_occ_only_once = df_occ_only_once[pd.isna(df_occ_only_once[TYPE_CONCEPT_MAP]) == False]\n",
    "print(df_occ_only_once.to_latex())\n",
    "df_occ_only_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1, 2, 3, 4, 5, 9999999999]\n",
    "\n",
    "\n",
    "data = collections.defaultdict(list)\n",
    "for (dataset, graph_type), df_ in df_occs.groupby(['dataset', 'type']):\n",
    "    print(dataset, graph_type)\n",
    "    bins_ = pd.cut(df_.occurrences, bins=bins)\n",
    "    val_counts = bins_.value_counts()\n",
    "    sum_ = val_counts.sum()\n",
    "    for bin_, val in val_counts.iteritems():\n",
    "        data['ratio'].append(val / sum_ * 100)\n",
    "        data['dataset'].append(dataset)\n",
    "        data['type'].append(graph_type)\n",
    "        data['bin'].append(bin_)\n",
    "\n",
    "df____ = pd.DataFrame(data)\n",
    "#\n",
    "#.plot(kind='barh', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df__ = df____.pivot(index='dataset', columns='bin', values='ratio').sort_index(ascending=False)\n",
    "\n",
    "max_column = np.sort(df__.columns.map(lambda x: x.right))\n",
    "\n",
    "def get_columns_name(x):\n",
    "    val = x.right\n",
    "    is_too_big = val > 999\n",
    "    eq_sign = '\\geq' if is_too_big else '='\n",
    "    val = max_column[-2] + 1 if is_too_big else val\n",
    "    return '$|n_v|{}{}$'.format(eq_sign, val)\n",
    "    \n",
    "df__.rename(columns=get_columns_name, inplace=True)\n",
    "df__.columns.name = None\n",
    "fig, ax = plt.subplots(figsize=(8, 2.3))\n",
    "df__.plot(kind='barh', stacked=True, ax = ax)\n",
    "ax.set_xlim((0, 118))\n",
    "ax.grid(False)\n",
    "ax.set_xlabel('%')\n",
    "fig.tight_layout()\n",
    "save_fig(fig, 'percentage_distribution_concept_occurrences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "last = 200#df_occs.occurrences.quantile(q=0.98)\n",
    "df_occs[df_occs.occurrences < last].groupby('dataset').occurrences.hist(bins = 120, ax = ax, alpha = 0.2)\n",
    "ax.set_yscale('log')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
