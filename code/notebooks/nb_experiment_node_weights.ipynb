{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: _Node weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'experiment_node_weights'\n",
    "df = results_helper.get_experiments_by_names([EXPERIMENT_NAME], fetch_predictions=True)\n",
    "df_ = results_helper.get_results(filter_out_non_complete_datasets=False, fetch_predictions=True, include_filter='__graph__')\n",
    "df = df.append(df_).reset_index()\n",
    "for x in ['graph__fast_wl__node_weight_iteration_weight_function', 'graph__fast_wl__node_weight_iteration_weight_function']:\n",
    "    df[x + '_'] = df[x].apply(lambda x: x.__name__ if callable(x) else '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['dataset', 'graph__fast_wl__node_weight_function']).mean_test_f1_macro.max().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['dataset', 'normalizer']).mean_test_f1_macro.max().to_frame().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = 'mean_test_f1_macro'\n",
    "attr = 'prediction_score_f1_macro'\n",
    "df_ = df.groupby(['dataset', 'graph__fast_wl__node_weight_function'])[attr].max().to_frame().unstack()\n",
    "df_['difference_non_vs_weight'] = (df_[attr, 'none'] - df_[attr, 'nxgraph_degrees_metric'])\n",
    "#df_['mean_test_f1_macro', 'none']\n",
    "#df_.columns = df_.columns.droplevel()\n",
    "#df_.columns = ['plain', 'degrees_node_weights', 'difference']\n",
    "\n",
    "#df__ = df_[['plain', 'degrees_node_weights']]\n",
    "#print(df__.to_latex(float_format=\"%.3f\"))\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other graph datasets\n",
    "\n",
    "_Mutag_, _Enzymes_, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_helper.get_experiments_by_names(['experiment_graph_extra_plain', 'experiment_graph_extra_node_weights'], fetch_predictions=True)\n",
    "df['graph__fast_wl__node_weight_function'] = df.graph__fast_wl__node_weight_function.apply(lambda x: x.__name__ if callable(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attr = 'prediction_score_f1_macro'\n",
    "attr = 'mean_test_f1_macro'\n",
    "attr = 'prediction_score_f1_macro'\n",
    "#attr = 'prediction_score_accuracy'\n",
    "best = df.groupby(['dataset', 'graph__fast_wl__node_weight_function'])[attr].max().to_frame().unstack()\n",
    "best.columns = best.columns.droplevel()\n",
    "best.rename(columns={'-': 'plain'}, inplace=True)\n",
    "\n",
    "best = best[(best.plain != '-') & (best.adj_degrees_metric_max != '-')]\n",
    "best = best[best.adj_degrees_metric_max != None]\n",
    "best['difference'] = best.adj_degrees_metric_max - best.plain\n",
    "#print(best.to_latex())\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df[df[attr] != '-']\n",
    "df_[attr] = df_[attr].astype(np.float64)\n",
    "best_ = df_.loc[df_.groupby(['dataset', 'graph__fast_wl__node_weight_function'])['mean_test_f1_macro'].idxmax()]\n",
    "\n",
    "prediction_file = best_[best_.dataset == 'MSRC_9'].iloc[2].prediction_file\n",
    "rs = get_pickle(prediction_file)\n",
    "rs = rs['results']\n",
    "X_test = rs['X_test']\n",
    "np.array(rs['Y_real']) == rs['Y_pred']\n",
    "#best = df_.groupby(['dataset', 'graph__fast_wl__node_weight_function'])[attr].idxmax()\n",
    "#df_\n",
    "#best[best.dataset == 'MSRC_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'MSRC_9'\n",
    "tasks = [x for x in experiments.get_filtered_tasks(task_type='graph_extra', dataset=DATASET) if DATASET in x.name]\n",
    "assert len(tasks) == 1\n",
    "task = tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, estimator, params = task.fn()\n",
    "experiment_config = experiment_helper.get_experiment_config('configs/experiments/graph_extra/experiment_graph_extra_node_weights.yaml')\n",
    "param_grid = experiment_helper.prepare_param_grid(task, params, experiment_config)\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, stratify=Y, test_size=0.2, random_state=40)\n",
    "cv = sklearn.model_selection.StratifiedKFold(n_splits=4)\n",
    "gscv = sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring='f1_macro', refit=True, cv=cv, verbose=1)\n",
    "score = gscv.fit(X_train, Y_train)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_ = score.best_estimator_.predict(X_test)\n",
    "sklearn.metrics.f1_score(Y_test, Y_test_, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.where(gscv.predict(X_test) == rs['Y_real']), sklearn.metrics.f1_score(rs['Y_real'], gscv.predict(X_test), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import significance_test_utils\n",
    "attr = 'graph__fast_wl__node_weight_function'\n",
    "confidences = collections.defaultdict(list)\n",
    "for dataset, df_ in df[df.prediction_score_f1_macro !='-'].groupby('dataset'):\n",
    "    assert attr in df_.columns\n",
    "    print(dataset)\n",
    "    df_['prediction_score_f1_macro'] = df_.prediction_score_f1_macro.astype(np.float64)\n",
    "    best_ = df_.loc[df_.groupby(attr).prediction_score_f1_macro.idxmax()]\n",
    "    best_ = best_[best_.graph__fast_wl__node_weight_function != 'adj_degrees_metric']\n",
    "    #display(df_[attr].unique())\n",
    "    if len(best_) != 2:\n",
    "        print('\\tWrong number of models: {}, expected=2. Skipping'.format(len(best_)))\n",
    "        continue\n",
    "    prediction_filenames = [best_.loc[best_[attr] == name].iloc[0].prediction_file for name in ['-', 'adj_degrees_metric_max']]\n",
    "    diffs, score_a, score_b, global_difference, confidence = results_helper.calculate_significance(prediction_filenames[0], prediction_filenames[1])\n",
    "    \n",
    "    for k, v in [('Score A', score_a), ('Score B', score_b), ('Difference', global_difference), ('Confidence', confidence)]:\n",
    "        print('\\t{:20} {:9.4f}'.format(k, v))\n",
    "    print()\n",
    "    confidences['dataset'].append(dataset)\n",
    "    confidences['confidence'].append(confidence)\n",
    "    best.loc[dataset, 'confidence'] = confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics about benchmark graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(list)\n",
    "for dataset in log_progress_nb(graph_helper.get_all_graph_benchmark_dataset_names()):\n",
    "    X, Y = graph_helper.get_mutag_enzyme_graphs(dataset)\n",
    "    assert len(X) == len(Y)\n",
    "    nodes = list(chain.from_iterable([labels for _, labels in X]))\n",
    "    unique_nodes = set(nodes)\n",
    "    num_edges = sum([adj.nonzero()[0].shape[0] for adj, labels in X])\n",
    "    data['dataset'].append(dataset)\n",
    "    data['num_nodes'].append(len(nodes))\n",
    "    data['num_unique_nodes'].append(len(unique_nodes))\n",
    "    data['num_edges'].append(num_edges)\n",
    "    data['num_graphs'].append(len(X))\n",
    "    data['num_classes'].append(len(set(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_statistics = pd.DataFrame(data)\n",
    "df_graph_statistics[df_graph_statistics.num_graphs > 400]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
