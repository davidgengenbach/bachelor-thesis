{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: _Node weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'experiment_node_weights'\n",
    "df = results_helper.get_experiments_by_names([EXPERIMENT_NAME], fetch_predictions=True)\n",
    "df_ = results_helper.get_results(filter_out_non_complete_datasets=False, fetch_predictions=True, include_filter='__graph__')\n",
    "df = df.append(df_).reset_index()\n",
    "for x in ['graph__fast_wl__node_weight_iteration_weight_function', 'graph__fast_wl__node_weight_iteration_weight_function']:\n",
    "    df[x + '_'] = df[x].apply(lambda x: x.__name__ if callable(x) else '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.groupby(['dataset', 'graph__fast_wl__node_weight_function']).prediction_score_f1_macro.max().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['dataset', 'normalizer']).mean_test_f1_macro.max().to_frame().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = 'mean_test_f1_macro'\n",
    "attr = 'prediction_score_f1_macro'\n",
    "df_ = df.groupby(['dataset', 'graph__fast_wl__node_weight_function'])[attr].max().to_frame().unstack()\n",
    "df_['difference_non_vs_weight'] = (df_[attr, 'none'] - df_[attr, 'nxgraph_degrees_metric'])\n",
    "#df_['mean_test_f1_macro', 'none']\n",
    "#df_.columns = df_.columns.droplevel()\n",
    "#df_.columns = ['plain', 'degrees_node_weights', 'difference']\n",
    "\n",
    "#df__ = df_[['plain', 'degrees_node_weights']]\n",
    "#print(df__.to_latex(float_format=\"%.3f\"))\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other graph datasets\n",
    "\n",
    "_Mutag_, _Enzymes_, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_helper.get_experiments_by_names(['experiment_node_weights_extra', 'experiment_node_weights_extra_normal'], fetch_predictions=True)\n",
    "\n",
    "\n",
    "df['graph__fast_wl__node_weight_function'] = df.graph__fast_wl__node_weight_function.apply(lambda x: x.__name__ if callable(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attr = 'prediction_score_f1_macro'\n",
    "#attr = 'mean_test_f1_macro'\n",
    "attr = 'prediction_score_accuracy'\n",
    "best = df.groupby(['dataset', 'graph__fast_wl__node_weight_function'])[attr].max().to_frame().unstack()\n",
    "best.columns = best.columns.droplevel()\n",
    "best.rename(columns={'-': 'plain'}, inplace=True)\n",
    "\n",
    "best = best[(best.plain != '-') & (best.adj_degrees_metric_max != '-')]\n",
    "best = best[best.adj_degrees_metric_max != None]\n",
    "best['difference'] = best.plain - best.adj_degrees_metric_max\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import significance_test_utils\n",
    "attr = 'graph__fast_wl__node_weight_function'\n",
    "confidences = collections.defaultdict(list)\n",
    "for dataset, df_ in df[df.prediction_score_f1_macro !='-'].groupby('dataset'):\n",
    "    assert attr in df_.columns\n",
    "    df_['prediction_score_f1_macro'] = df_.prediction_score_f1_macro\n",
    "    best_ = df_.loc[df_.groupby(attr).prediction_score_f1_macro.apply(lambda x: x.astype(np.float64)).idxmax()]\n",
    "    if len(best_) != 2:\n",
    "        print('\\tNot enough data. Skipping')\n",
    "        continue\n",
    "    prediction_filenames = [best_.loc[best_[attr] == name].iloc[0].prediction_file for name in ['-', 'adj_degrees_metric_max']]\n",
    "    diffs, score_a, score_b, global_difference, confidence = results_helper.calculate_significance(prediction_filenames[0], prediction_filenames[1])\n",
    "    \n",
    "    for k, v in [('Score A', score_a), ('Score B', score_b), ('Difference', global_difference), ('Confidence', confidence)]:\n",
    "        print('\\t{:20} {:9.4f}'.format(k, v))\n",
    "    print()\n",
    "    confidences['dataset'].append(dataset)\n",
    "    confidences['confidence'].append(confidence)\n",
    "    display(df.loc[best_,'mean_test_f1_macro'] )\n",
    "    df.loc[best_,'confidence'] = [confidence] * 2\n",
    "df_confidences = pd.DataFrame(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    best.merge(df_confidences, left_index=True, right_index=True).drop(columns='difference').to_latex(float_format=lambda x: '%.3f' % x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(list)\n",
    "for dataset in graph_helper.get_all_graph_benchmark_dataset_names():\n",
    "    X, Y = graph_helper.get_mutag_enzyme_graphs(dataset)\n",
    "    assert len(X) == len(Y)\n",
    "    nodes = list(chain.from_iterable([labels for _, labels in X]))\n",
    "    unique_nodes = set(nodes)\n",
    "    num_edges = sum([adj.nonzero()[0].shape[0] for adj, labels in X])\n",
    "    data['dataset'].append(dataset)\n",
    "    data['num_nodes'].append(len(nodes))\n",
    "    data['num_unique_nodes'].append(len(unique_nodes))\n",
    "    data['num_edges'].append(num_edges)\n",
    "    data['num_graphs'].append(len(X))\n",
    "    data['num_classes'].append(len(set(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_statistics = pd.DataFrame(data)\n",
    "df_graph_statistics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
