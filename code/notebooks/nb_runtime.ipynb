{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download logs from server\n",
    "!rsync pe:logs/* tmp/logs -avP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download condor job history\n",
    "!ssh pe 'condor_history -forwards | grep -v \" X \" | grep \"script_serial\"' > tmp/condor_history.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condor_history_file = get_text_file_lines('tmp/condor_history.txt')\n",
    "\n",
    "def f(x):\n",
    "    return np.all([\n",
    "        ' X ' not in x,\n",
    "        'start_script_serial' in x\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_parsed_line(x):\n",
    "    out = [y.strip() for y in x.split('  ')]\n",
    "    assert len(out) == 9\n",
    "    parts = out[7].split(' ')\n",
    "    out[7] = parts[0]\n",
    "    #out.insert(7, parts[1])\n",
    "    out = out[:-1] + out[-1].split(' ', 2)\n",
    "    return out\n",
    "\n",
    "headers = ['job_id', 'user', '_', '__', '___', '____', 'commit_time', 'run_time', 'end_date', 'end_time', 'cmd']\n",
    "condor_history_file = list(filter(f, condor_history_file))\n",
    "\n",
    "df = pd.DataFrame(list(map(get_parsed_line, condor_history_file)), columns=headers)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.cmd.str.contains('experiment_config') & (df.cmd.str.contains('--n_jobs 16'))]\n",
    "df['experiment_file'] = df.cmd.str.split('experiment_config ').str.get(1).str.split('/').str.get(-1)\n",
    "df['nested'] = df.cmd.str.contains('use_nest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['experiment_file', 'run_time', 'nested']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "LOG_DIR = 'tmp/logs'\n",
    "logs = sorted(glob('{}/*.log'.format(LOG_DIR)))\n",
    "log = logs[-5]\n",
    "\n",
    "def get_log(log):\n",
    "    with open(log) as f:\n",
    "        return [x for x in f.read().split('\\n')]\n",
    "    \n",
    "def get_finish_times(log_lines):\n",
    "    FINISHED_REGEXP = r'\\d: (.+?) +- (.+?) +- Finished \\(time=(.+?)\\)'\n",
    "    finished_lines = [x.strip() for x in log_lines if x.count('(time=') == 1]\n",
    "    finish_times = re.findall(FINISHED_REGEXP, '\\n'.join(finished_lines))\n",
    "    return finish_times\n",
    "\n",
    "\n",
    "def time_str_2_seconds(x):\n",
    "    parts = x.split(':')\n",
    "    seconds = 0\n",
    "    for i, part in enumerate(reversed(parts)):\n",
    "        seconds += (np.power(60, i)) * int(part)\n",
    "    return seconds\n",
    "\n",
    "def get_finish_times_from_log(log_file):\n",
    "    df = pd.DataFrame(get_finish_times(get_log(log)), columns = ['type', 'name', 'time'])\n",
    "    df['dataset'] = df.name.apply(filename_utils.get_dataset_from_filename)\n",
    "    df['time_as_seconds'] = df.time.apply(time_str_2_seconds)\n",
    "    df['time_as_minutes'] = df.time_as_seconds.apply(lambda x: int(x / 60))\n",
    "    return df\n",
    "\n",
    "for log in sorted(logs):\n",
    "    df = get_finish_times_from_log(log)\n",
    "    #df = df[df.dataset == 'ng20']\n",
    "    if not len(df): continue\n",
    "    print(log.split('/')[-1])\n",
    "    display(df.groupby(['dataset', 'type']).time_as_minutes.max().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WL run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from memory_profiler import memory_usage\n",
    "import tempfile\n",
    "\n",
    "def get_object_size(obj):\n",
    "    file = tempfile.mktemp()\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    return os.path.getsize(file)\n",
    "\n",
    "H=4\n",
    "\n",
    "trans = transformers.FastWLGraphKernelTransformer(h=H, use_early_stopping=False, same_label=False)\n",
    "clf = sklearn.svm.LinearSVC(random_state=42)\n",
    "phi_picker = transformers.PhiPickerTransformer(use_zeroth=True)\n",
    "text_trans = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "data = {'text': collections.defaultdict(list), 'graph': collections.defaultdict(list)}\n",
    "for dataset in log_progress_nb(dataset_helper.get_dataset_names_with_concept_map()):\n",
    "    print(dataset)\n",
    "    def cleanup():\n",
    "        global clf, phi_picker, trans, text_trans, X, Y\n",
    "        clf = sklearn.base.clone(clf)\n",
    "        phi_picker = sklearn.base.clone(phi_picker)\n",
    "        trans = sklearn.base.clone(trans)\n",
    "        text_trans = sklearn.base.clone(text_trans)\n",
    "        gc.collect()\n",
    "        del X, Y\n",
    "    \n",
    "    def get_mem_usage():\n",
    "        return np.mean(memory_usage(-1, interval=.2, timeout=1, include_children=True))\n",
    "    \n",
    "    def measure(name, type_, fn):\n",
    "        print('\\t\\t', name)\n",
    "        #data['mem_start_' + name].append(get_mem_usage())\n",
    "        data[type_]['start_' + name].append(time())\n",
    "        out = fn()\n",
    "        data[type_]['end_' + name].append(time())\n",
    "        #data['mem_end_' + name].append(get_mem_usage())\n",
    "        return out\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # Text\n",
    "    print('\\tText')\n",
    "    data['text']['type'].append('text')\n",
    "    data['text']['start'].append(time())\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    data['text']['dataset'].append(dataset)\n",
    "    data['text']['num_classes'].append(len(set(Y)))\n",
    "    data['text']['num_els'].append(len(X))\n",
    "    phi = measure('tfidf', 'text', lambda: text_trans.fit_transform(X))\n",
    "    _ = measure('clf_fit', 'text', lambda: clf.fit(phi, Y))\n",
    "    _ = measure('clf_predict', 'text', lambda: clf.predict(phi))\n",
    "    data['text']['end'].append(time())\n",
    "    data['text']['num_features'].append(clf.coef_.shape[1])\n",
    "    data['text']['estimator_size'].append(get_object_size(clf) + get_object_size(text_trans))\n",
    "    \n",
    "    cleanup()\n",
    "    # Graph\n",
    "    print('\\tGraph')\n",
    "    data['graph']['type'].append('graph')\n",
    "    data['graph']['start'].append(time())\n",
    "    X, Y = dataset_helper.get_concept_map_for_dataset(dataset)\n",
    "    X = graph_helper.get_graphs_only(X)\n",
    "    all_nodes = graph_helper.get_all_node_labels(X)\n",
    "    data['graph']['dataset'].append(dataset)\n",
    "    data['graph']['num_els'].append(len(all_nodes))\n",
    "    data['graph']['num_classes'].append(len(set(Y)))\n",
    "    data['graph']['iterations'].append(trans.h)\n",
    "    _ = measure('wl', 'graph', lambda: trans.fit(X))\n",
    "    phi = measure('phi_picker', 'graph', lambda: phi_picker.transform(trans.phi_list))\n",
    "    _ = measure('clf_fit', 'graph', lambda: clf.fit(phi, Y))\n",
    "    _ = measure('clf_predict', 'graph', lambda: clf.predict(phi))\n",
    "    data['graph']['end'].append(time())\n",
    "    data['graph']['num_features'].append(clf.coef_.shape[1])\n",
    "    #\n",
    "    #data['graph']['estimator_size'].append(get_object_size(clf) + get_object_size(trans))\n",
    "    data['graph']['estimator_size'].append(get_object_size(clf) + get_object_size(trans.label_lookups))\n",
    "    \n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import time_utils\n",
    "df = pd.DataFrame(data['text']).append(pd.DataFrame(data['graph'])).reset_index()\n",
    "df['runtime_total'] = df.end - df.start\n",
    "df = df[[c for c in df.columns if c != 'index']]\n",
    "\n",
    "for x, (start_attr, end_attr) in [('graph', ('start_wl', 'end_phi_picker')), ('text', ('start_tfidf', 'end_tfidf'))]:\n",
    "    df.loc[df.type == x, 'runtime_feature_extraction'] = df[df.type == x][end_attr] - df[df.type == x][start_attr]\n",
    "    \n",
    "df[['dataset', 'type', 'num_features', 'runtime_total', 'runtime_feature_extraction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mem = (df.pivot(index='dataset', columns='type', values='estimator_size') / 1024 / 1024).rename(columns=dict(graph='graph_estimator_size', text='text_estimator_size'))\n",
    "df_runtime = df.pivot(index='dataset', columns='type', values='runtime_total')\n",
    "\n",
    "for x in ['graph', 'text']:\n",
    "    df_mem['{}_runtime'.format(x)] = df_runtime[x]\n",
    "\n",
    "for x in ['graph', 'text']:\n",
    "    df_mem['{}_runtime_feature_extraction'.format(x)] = df[df.type==x].runtime_feature_extraction.values\n",
    "\n",
    "for x in ['graph', 'text']:\n",
    "    df_mem['{}_num_features'.format(x)] = df[df.type==x].num_features.values / 1000\n",
    "    \n",
    "print(df_mem.to_latex(float_format='%.0f'))\n",
    "df_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['runtime_clf'] = df.end_clf_predict - df.start_clf_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='num_features', y='runtime_clf')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
