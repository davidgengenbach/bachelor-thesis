{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_prelude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters \n",
    "categories = reuters.categories()\n",
    "c = collections.Counter()\n",
    "multiple_indices= []\n",
    "for idx, fileid in enumerate(reuters.fileids()):\n",
    "    categories = reuters.categories(fileid)\n",
    "    multiple = len(categories) > 1\n",
    "    c['multiple' if multiple else 'one'] += 1\n",
    "    if multiple:\n",
    "        multiple_indices.append((idx, fileid))\n",
    "#multiple_indices\n",
    "sum(c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "dataset = 'r8'\n",
    "data = collections.defaultdict(lambda: [])\n",
    "X, Y = dataset_helper.get_dataset(dataset)\n",
    "# Add general dataset statistics\n",
    "data['dataset'].append(dataset)\n",
    "data['num_documents'].append(len(X))\n",
    "data['num_classes'].append(len(set(Y)))\n",
    "\n",
    "# Add text statistics\n",
    "count_vec = CountVectorizer()\n",
    "doc_vecs = count_vec.fit_transform(X)\n",
    "\n",
    "all_words = set(count_vec.vocabulary_.keys())\n",
    "data['num_words'].append(doc_vecs.sum())\n",
    "data['num_unique_words'].append(len(all_words))\n",
    "data['median_doc_length'].append(np.median([len(x) for x in X]))\n",
    "data['median_words_per_doc'].append(np.median(np.squeeze(np.asarray(doc_vecs.sum(axis = 1)))))\n",
    "pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
