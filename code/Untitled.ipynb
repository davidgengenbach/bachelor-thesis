{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "EXPORT_DPI = 100\n",
    "EXPORT_FIG_SIZE = (8, 4)\n",
    "EXPORT_FIG_SIZE_BIG = (10, 7)\n",
    "EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT = EXPORT_FIG_SIZE\n",
    "EXPORT_FIG_WIDTH_BIG, EXPORT_FIG_HEIGHT_BIG = EXPORT_FIG_SIZE_BIG\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 80\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set('notebook', 'whitegrid', palette = 'deep')\n",
    "plt.rcParams['figure.figsize'] = EXPORT_FIG_SIZE_BIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import graph_helper, dataset_helper\n",
    "import networkx as nx\n",
    "from transformers import fast_wl_pipeline, text_pipeline\n",
    "from transformers.tuple_selector import TupleSelector\n",
    "import sklearn\n",
    "from sklearn import pipeline\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "def get_classifier():\n",
    "    graph_fast_wl_grid_params = {\n",
    "        'fast_wl__h': [5],\n",
    "        'fast_wl__phi_dim': [None],\n",
    "        'fast_wl__round_to_decimals': [10],\n",
    "        'phi_picker__return_iteration': ['stacked'],\n",
    "        'phi_picker__use_zeroth': [False]\n",
    "    }\n",
    "\n",
    "    grid_params_combined = dict({\n",
    "        'classifier': []\n",
    "    }, **dict({'features__fast_wl_pipeline__feature_extraction__' + k: val for k, val in\n",
    "               graph_fast_wl_grid_params.items()}, **dict(\n",
    "        features__fast_wl_pipeline__feature_extraction__fast_wl__phi_dim=[]\n",
    "    )))\n",
    "\n",
    "    combined_features = sklearn.pipeline.FeatureUnion([\n",
    "        ('tfidf', sklearn.pipeline.Pipeline([\n",
    "            ('selector', TupleSelector(tuple_index=1)),\n",
    "            ('tfidf', text_pipeline.get_pipeline()),\n",
    "        ])),\n",
    "        ('fast_wl_pipeline', sklearn.pipeline.Pipeline([\n",
    "            ('selector', TupleSelector(tuple_index=0, v_stack=False)),\n",
    "            ('feature_extraction', fast_wl_pipeline.get_pipeline())\n",
    "        ]))\n",
    "    ], transformer_weights = dict(\n",
    "        tfidf=1,\n",
    "        fast_wl_pipeline=1\n",
    "    ))\n",
    "\n",
    "    pipeline = sklearn.pipeline.Pipeline([\n",
    "        ('features', combined_features),\n",
    "        ('scaler', sklearn.preprocessing.MaxAbsScaler()),\n",
    "        ('classifier', None)\n",
    "    ])\n",
    "    \n",
    "    return pipeline, grid_params_combined\n",
    "\n",
    "cv = sklearn.model_selection.StratifiedKFold(\n",
    "    n_splits=3,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "Result = collections.namedtuple('Result', ['y_true', 'y_preds'])\n",
    "\n",
    "dataset = 'ling-spam'\n",
    "dataset = 'ng20'\n",
    "#dataset = None\n",
    "for graph_cache_file in dataset_helper.get_all_cached_graph_datasets(dataset):\n",
    "    if 'concept' not in graph_cache_file or 'v2' not in graph_cache_file: continue\n",
    "    #if 'cooccurrence' not in graph_cache_file: continue\n",
    "    print(graph_cache_file)\n",
    "    X_combined, Y_combined = graph_helper.get_filtered_text_graph_dataset(graph_cache_file)\n",
    "\n",
    "    graphs = [g for (g, _, _) in X_combined]\n",
    "    empty_graphs = len([1 for g in graphs if nx.number_of_nodes(g) == 0 or nx.number_of_edges(g) == 0])\n",
    "    num_vertices = sum([nx.number_of_nodes(g) for g in graphs]) + empty_graphs\n",
    "    fast_wl_pipeline.convert_graphs_to_tuples(graphs)\n",
    "    X_combined = [(graph, text) for (graph, text, _) in X_combined]\n",
    "    \n",
    "    clfs, params = get_classifier()\n",
    "    clf = sklearn.linear_model.PassiveAggressiveClassifier(class_weight = 'balanced', max_iter = 10000, verbose = 0, tol = 1e-5)\n",
    "    #clf = sklearn.svm.LinearSVC(class_weight = 'balanced', max_iter = 10000, verbose = 1, tol = 1e-6)\n",
    "    params['classifier'] = [clf]\n",
    "    #params['features__transformer_weights'] = [{'tfidf': 1, 'fast_wl_pipeline': 1}, {'tfidf': 1, 'fast_wl_pipeline': 0}]\n",
    "    #\n",
    "    #params['features__fast_wl_pipeline__feature_extraction__fast_wl__h'] = [1, 10]\n",
    "    params['features__fast_wl_pipeline__feature_extraction__phi_picker__use_zeroth'] = [True, False]\n",
    "    params['features__fast_wl_pipeline__feature_extraction__fast_wl__phi_dim'] = [num_vertices]\n",
    "    \n",
    "    grid = sklearn.model_selection.ParameterGrid(params)\n",
    "    \n",
    "    assert len(grid) == 2\n",
    "    \n",
    "    results = []\n",
    "    for train, test in cv.split(X_combined, Y_combined):\n",
    "        X_train, Y_train, X_test, Y_test = np.array(X_combined)[train], np.array(Y_combined)[train], np.array(X_combined)[test], np.array(Y_combined)[test]\n",
    "        result = Result(Y_test, [])\n",
    "        for params_ in grid:\n",
    "            print('set_params', params_)\n",
    "            clfs.set_params(**params_)\n",
    "            clfs.fit(X_train, Y_train)\n",
    "            Y_pred = clfs.predict(X_test)\n",
    "            result.y_preds.append(Y_pred)\n",
    "        results.append(result)\n",
    "        break\n",
    "    break\n",
    "        \n",
    "        \n",
    "#f1 = sklearn.metrics.f1_score(y_true=Y_test, y_pred=Y_pred, average='macro')\n",
    "#clf_ = clfs.named_steps['classifier']\n",
    "#num_text_features = len(clfs.named_steps['features'].transformer_list[0][1].named_steps['tfidf'].named_steps['TfidfTransformer'].vocabulary_.keys())\n",
    "#num_graph_features = coefs.shape[1] - num_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    y_true, (y_pred_a, y_pred_b) = result.y_true, result.y_preds\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred_a, average='macro')\n",
    "    f1_ = sklearn.metrics.f1_score(y_true, y_pred_b, average='macro')\n",
    "    print(f1, f1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_transformed_results(result):\n",
    "    y_true = result.y_true\n",
    "    y_pred_a, y_pred_b = result.y_preds\n",
    "    trans_enc = sklearn.preprocessing.LabelEncoder()\n",
    "    y_true = trans_enc.fit_transform(y_true)\n",
    "    y_pred_a, y_pred_b = trans_enc.transform(y_pred_a), trans_enc.transform(y_pred_b)\n",
    "    return np.array(y_true), np.array(y_pred_a), np.array(y_pred_b)\n",
    "\n",
    "def randomization_test(y_true, y_pred_a, y_pred_b, metric = sklearn.metrics.f1_score, num_trails = 1000):\n",
    "    metrics = np.empty(num_trails, dtype = np.float64)\n",
    "    \n",
    "    def get_metric(indices):\n",
    "        y_shuffled = np.empty(len(y_true), dtype = np.uint16)\n",
    "        y_shuffled[indices == 0] = y_pred_a[indices == 0]\n",
    "        y_shuffled[indices == 1] = y_pred_b[indices == 1]\n",
    "        return y_shuffled\n",
    "    \n",
    "    for i in range(num_trails):\n",
    "        indices = np.random.randint(0, 2, size = len(y_true))\n",
    "        y_shuffled_a, y_shuffled_b = get_metric(indices), get_metric(np.abs(indices - 1))\n",
    "        metric_a, metric_b = metric(y_true, y_shuffled_a), metric(y_true, y_shuffled_b)\n",
    "        metrics[i] = metric_a - metric_b\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    return sklearn.metrics.f1_score(y_true = y_true, y_pred=y_pred, average='macro')\n",
    "\n",
    "metric = f1_metric\n",
    "for result in results:\n",
    "    y_true, y_pred_a, y_pred_b = get_transformed_results(result)\n",
    "    metric_a, metric_b = metric(y_true, y_pred_a), metric(y_true, y_pred_b)\n",
    "    diff = metric_a - metric_b\n",
    "    #abs_diff = np.fabs(diff)\n",
    "    res = randomization_test(y_true, y_pred_a, y_pred_b, metric = metric, num_trails = 2000)\n",
    "    df = pd.DataFrame({'f1': res})\n",
    "    fig, ax = plt.subplots()\n",
    "    df.f1.plot(kind = 'hist', bins = 100, ax = ax)\n",
    "    ax.axvline(diff, color = 'red')\n",
    "    ax.axvline(-diff, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_coefficients(classifier, feature_names, top_features=20):\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "\n",
    "    sorted_coefs_graphs = np.argsort(coef[num_text_features:])\n",
    "    top_positive_graph_coefs = sorted_coefs_graphs[-top_features:]\n",
    "    top_negative_graph_coefs = sorted_coefs_graphs[:top_features]\n",
    "    top_graph_coefs = np.hstack([top_negative_graph_coefs, top_positive_graph_coefs])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    colors = ['r' if c < 0 else 'b' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_graph_coefs + num_text_features], color=colors)\n",
    "    plt.title('Graph features')\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    plt.title('All features')\n",
    "    #plt.xticks(top_coefficients, rotation=60, ha='right')\n",
    "    if feature_names is not None:\n",
    "        feature_names = np.array(feature_names)\n",
    "        plt.xticks(np.arange(0, 2 * top_features), [feature_names[x] if x < len(feature_names) else '..' for x in top_coefficients], rotation=60, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('#text features: {}, #graph features: {}'.format(num_text_features, num_graph_features))\n",
    "    \n",
    "coefs = clf_.coef_.ravel()\n",
    "text_features = coefs[:num_text_features]\n",
    "graph_features = coefs[num_text_features:]\n",
    "tfidf_transformer = clfs.named_steps['features'].transformer_list[0][1].named_steps['tfidf'].named_steps['TfidfTransformer']\n",
    "fast_wl_transformer = clfs.named_steps['features'].transformer_list[0]\n",
    "text_feature_names = tfidf_transformer.get_feature_names()\n",
    "\n",
    "plot_coefficients(clf_, text_feature_names, top_features=50)\n",
    "#clfs.named_steps['features'].transformer_list[1][1].named_steps['feature_extraction'].named_steps['phi_picker'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'coefs': coefs})\n",
    "df_graphs = df[df.index >= num_text_features]\n",
    "df_texts = df[df.index < num_text_features]\n",
    "df['type'] = df.index.map(lambda x: 'text' if x < num_text_features else 'graph')\n",
    "fig, ax = plt.subplots()\n",
    "for type_, df_ in df.groupby('type'):\n",
    "    df_.coefs.plot(kind = 'hist', logy = True, bins = 100, ax = ax, label = type_, alpha = 0.8)\n",
    "ax.legend()\n",
    "#df_graphs.plot(kind = 'hist', logy = True, bins = 100)\n",
    "#df_texts.plot(kind = 'hist', logy = True, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_, df_ in df.groupby('type'):\n",
    "    pos_sum = df_[df_.coefs > 0].coefs.sum() / len(df_[df_.coefs > 0])\n",
    "    neg_sum = df_[df_.coefs <= 0].coefs.sum() / len(df_[df_.coefs <= 0])\n",
    "    print(type_,'\\t', pos_sum, neg_sum)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
