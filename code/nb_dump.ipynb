{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "EXPORT_DPI = 100\n",
    "EXPORT_FIG_SIZE = (8, 4)\n",
    "EXPORT_FIG_SIZE_BIG = (10, 7)\n",
    "EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT = EXPORT_FIG_SIZE\n",
    "EXPORT_FIG_WIDTH_BIG, EXPORT_FIG_HEIGHT_BIG = EXPORT_FIG_SIZE_BIG\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 80\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set('notebook', 'whitegrid', palette = 'deep')\n",
    "plt.rcParams['figure.figsize'] = EXPORT_FIG_SIZE_BIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "def get_pickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "\n",
    "for result_file in glob('data/results/*.npy'):\n",
    "    filename = result_file.rsplit('/', 1)[1]\n",
    "    \n",
    "    r = get_pickle(result_file)\n",
    "    p = get_pickle('data/results/predictions/' + filename)\n",
    "    #print(r['meta_data'], p['meta_data'])\n",
    "    print(p['results']['Y_pred'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import sys\n",
    "from utils import dataset_helper\n",
    "\n",
    "for f in glob('data/graphs/*'):\n",
    "    if not os.path.isdir(f): continue\n",
    "    is_graph_folder = os.path.isdir('{}/all'.format(f)) or len(glob('{}/*0.gml'.format(f))) != 0\n",
    "    if not is_graph_folder: continue\n",
    "    print(f)\n",
    "    X, Y = dataset_helper.get_gml_graph_dataset(f, use_cached=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dataset_helper\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "\n",
    "tuple_trans = NxGraphToTupleTransformer()\n",
    "\n",
    "graph_sets = {}\n",
    "for cache_file in dataset_helper.get_all_cached_graph_datasets('ling-spam'):\n",
    "    graph_type = re.findall(r'dataset_graph_(.+?)_', cache_file)[0]\n",
    "    if graph_type == 'gml': graph_type = 'concept-map'\n",
    "    if graph_type in graph_sets: continue\n",
    "    X, Y = dataset_helper.get_dataset_cached(cache_file)\n",
    "    graph_sets[graph_type] = (cache_file, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for graph_type, (cache_file, X, Y) in graph_sets.items():\n",
    "    df = df.append(pd.DataFrame({\n",
    "        'graph_type': graph_type,\n",
    "        'num_nodes': [nx.number_of_nodes(x) for x in X],\n",
    "        'num_edges': [nx.number_of_edges(x) for x in X]\n",
    "    }))\n",
    "\n",
    "hist_kwargs = dict(kind = 'hist', alpha = 0.6, bins = 40, logy = False, legend = True, xlim = (0, 1000))\n",
    "\n",
    "plt.figure()\n",
    "df.groupby('graph_type').num_edges.plot(title = '# edges per graph type',**hist_kwargs)\n",
    "plt.figure()\n",
    "df.groupby('graph_type').num_nodes.plot(title = '# nodes per graph type', **hist_kwargs)\n",
    "axes = df.groupby('graph_type').plot(**hist_kwargs)\n",
    "\n",
    "for name, ax in axes.items():\n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('graph_type').num_nodes.sum() / df.groupby('graph_type').num_edges.sum()\n",
    "df.groupby('graph_type').num_nodes.sum(), df.groupby('graph_type').num_edges.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = dataset_helper.get_dataset('ng20')\n",
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjs = [adj for adj, _ in X]\n",
    "\n",
    "adjs_1 = np.copy(adjs)\n",
    "adjs_2 = np.copy(adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for adj in adjs_1:\n",
    "    adj[adj.nonzero()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for adj in adjs_2:\n",
    "    adj = adj.tocsr()\n",
    "    adj.data = np.where(adj.data > 1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a1, a2 in zip(adjs_1, adjs_2):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_all = np.array([[1, 2, 3, 4, 1], [5, 6, 7, 8, 5]])\n",
    "colors_all = [hash(tuple(row)) for row in colors_all.T]\n",
    "_, colors_all = np.unique(colors_all, return_inverse=True)\n",
    "max_all = int(np.amax(colors_all)) + 1\n",
    "colors_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar graphs by WL feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Retrieve graph datasets (both cmap and coo)\n",
    "\n",
    "Retrieve phi feature maps for coo and cmap, then get number of non-zero elements per graph (= per row of the feature map)\n",
    "\n",
    "And calculate the gram matrix, then find the most similar graphs per graph (= per row of the gram matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "dataset_name = 'ng20'\n",
    "dataset_name = 'ling-spam'\n",
    "\n",
    "results = collections.defaultdict(lambda: {})\n",
    "for graph_phi_file in dataset_helper.get_all_cached_graph_phi_datasets(dataset_name=dataset_name):\n",
    "    if 'gml' not in graph_phi_file: continue\n",
    "    print('Processing: {}'.format(graph_phi_file.split('/')[-1]))\n",
    "    phi, Y = dataset_helper.get_dataset_cached(graph_phi_file, check_validity=False)\n",
    "    \n",
    "    for h, phi_used in enumerate(phi):\n",
    "        print('\\th={}'.format(h))\n",
    "        # Generate kernel matrix\n",
    "        gram_matrix = phi_used.dot(phi_used.T).toarray()\n",
    "        results[graph_phi_file][h] = {}\n",
    "        results[graph_phi_file][h]['phi_used'] = phi_used\n",
    "        # Vector with the number of non-zero elements per row\n",
    "        # ie. non_zero_elements[idx] = len(phi_used[idx].nonzero())\n",
    "        results[graph_phi_file][h]['non_zero_elements'] = np.squeeze(np.asarray(np.sum(phi_used, axis = 1).T))\n",
    "        results[graph_phi_file][h]['num_elements'] = phi_used.shape[0]\n",
    "        results[graph_phi_file][h]['found_counter'] = collections.Counter()\n",
    "        results[graph_phi_file][h]['most_similar_scores'] = []\n",
    "        results[graph_phi_file][h]['similarity_pairs'] = []\n",
    "\n",
    "        for idx, row in enumerate(gram_matrix):\n",
    "            indices = np.argsort(row)[-10:]\n",
    "            # Search for index of this graph in the similar graph indices,\n",
    "            # it should be the most similar graph (because it's the same graph!)\n",
    "            results[graph_phi_file][h]['found_counter']['found' if idx in indices else 'not_found'] += 1\n",
    "            results[graph_phi_file][h]['most_similar_scores'].append(row[indices].tolist())\n",
    "            results[graph_phi_file][h]['similarity_pairs'].append(indices)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sparsity of feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for graph_cache_file, iterations in results.items():\n",
    "    fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE_BIG)\n",
    "    if 'gml' not in graph_cache_file and 'cooccurrence_1_all_ling' not in graph_cache_file: continue\n",
    "    for iteration, metrics in iterations.items():\n",
    "        if iteration != 0: continue\n",
    "        df = pd.DataFrame(metrics['non_zero_elements'], columns=['non_zero_elements'])\n",
    "        df.non_zero_elements.plot(kind='hist', bins = 40, alpha = 0.8, ax = ax, title = 'Histogram of non-zero entries in feature map (per row/element)')\n",
    "    \n",
    "    ax.set_xlim((0, 1000))\n",
    "    ax.set_xlabel('# of non-zero elements per row')\n",
    "    ax.legend(['Co-occurence graphs', 'Concept maps'])\n",
    "    plt.show()\n",
    "    fig.savefig('tmp/feature-map-sparsity-{}.png'.format(dataset_name), dpi = EXPORT_DPI)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get graph dataset for the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(filter(lambda x: 'gml' in x, results.keys())))\n",
    "graph_phi_file = 'data/CACHE/dataset_graph_gml_ling-spam-single.phi.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = graph_phi_file.split('/')[-1].split('.phi')[0]\n",
    "print(filename)\n",
    "candidates = [x for x in dataset_helper.get_all_cached_graph_datasets() if filename in x]\n",
    "assert len(candidates)\n",
    "X, Y = dataset_helper.get_dataset_cached(candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots()\n",
    "    choice = np.random.choice(len(X))\n",
    "    graph = nx.Graph(X[choice])\n",
    "    res = [(node, val) for node, val in nx.pagerank(graph).items()]\n",
    "    nodes = [node for node, val in res]\n",
    "    node_vals = np.array([val for node, val in res])\n",
    "    node_sizes = np.exp(node_vals * 20) * 20 / len(nodes) * 3\n",
    "    node_sizes = [0 for x in nodes]\n",
    "    nx.draw_networkx(graph, nodelist = nodes, with_labels=False, node_size = node_sizes, node_color='#000000')\n",
    "    ax.set_title('Graph#={}, connected_components={}'.format(choice, nx.number_connected_components(graph)))\n",
    "    ax.grid('off')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('#FFFFFF')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text, _ = dataset_helper.get_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot similar graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "similarity_pairs = results[graph_phi_file]['similarity_pairs']\n",
    "similarity_scores = np.array(results[graph_phi_file]['most_similar_scores'])\n",
    "phi_used = results[graph_phi_file]['phi_used']\n",
    "\n",
    "# Check that the similarity score can not be greater than the number of nodes!\n",
    "for idx, graph, most_similar, most_similar_score in zip(range(len(X)), X, similarity_pairs, similarity_scores):\n",
    "    assert max(most_similar_score) <= len(graph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_matrix = phi_used.dot(phi_used.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "similarity_pairs = results[graph_phi_file]['similarity_pairs']\n",
    "similarity_scores = np.array(results[graph_phi_file]['most_similar_scores'])\n",
    "\n",
    "def get_similarity(graph1_idx, graph2_idx):\n",
    "    return gram_matrix[graph1_idx, graph2_idx]\n",
    "\n",
    "def get_non_zero_phi_elements(idx):\n",
    "    return phi_used[idx].nonzero()[1]\n",
    "\n",
    "def plot_similar_graphs(graph_idx, num_to_plot = 2):\n",
    "    most_similar = np.argsort(gram_matrix[graph_idx])\n",
    "    filtered = [idx for idx in most_similar if nx.number_of_nodes(X[idx]) > 0 and idx != graph_idx and get_similarity(graph_idx, idx) != 0]\n",
    "    similar_graph_idxs = np.array(filtered[-num_to_plot:])\n",
    "    graph_idxs = [graph_idx] + similar_graph_idxs.tolist()\n",
    "    similarities = [get_similarity(graph_idx, idx) for idx in graph_idxs]\n",
    "    similar_labels = [set(X[idx].nodes()) & set(X[graph_idx].nodes()) for idx in graph_idxs]\n",
    "    reference_graph = X[graph_idx]\n",
    "    \n",
    "    print(phi_used.shape[0], gram_matrix.shape[0])\n",
    "    print('Graph={}'.format(graph_idx))\n",
    "    print('NonZeroPhi={}'.format(get_non_zero_phi_elements(graph_idx)))\n",
    "    print('SimilarOwn={}'.format(get_similarity(graph_idx, graph_idx)))\n",
    "    print('SimilarIdxs={}'.format(similar_graph_idxs))\n",
    "    print('Similarities={}'.format(similarities))\n",
    "    print('SimilarLabels={}'.format(similar_labels))\n",
    "    \n",
    "    for similar_graph_idx, graph, text in [(graph_idx, X[graph_idx], X_text[graph_idx]) for graph_idx in graph_idxs]:\n",
    "        graph = graph.copy()\n",
    "        graph.remove_nodes_from(set(graph.nodes()) - set(reference_graph.nodes()))\n",
    "        print(Y[graph_idx], Y[similar_graph_idx])\n",
    "        fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE)\n",
    "        nx.draw_circular(graph, ax = ax, node_size = 14, with_labels = True, node_color = '#000000')\n",
    "        ax.text(0, 0, str(similar_graph_idx))\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"?\")\n",
    "    random_choice = 1000\n",
    "    while nx.number_of_nodes(X[random_choice]) > 10:\n",
    "        random_choice = np.random.randint(0, len(X))\n",
    "    #random_choice = 1\n",
    "    plot_similar_graphs(random_choice)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Augment edges\n",
    "\n",
    "Add an edge from node1 to node2 if they are connected by a path of length N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import collections\n",
    "import networkx as nx\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "WALK_LENGTH = 2\n",
    "dataset_name = 'ng20'\n",
    "\n",
    "for graph_cache_file in dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset_name):\n",
    "    if 'coo' not in graph_cache_file or 'all' in graph_cache_file: continue\n",
    "    print(graph_cache_file)\n",
    "    X_old, Y_old = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "    \n",
    "    # TODO\n",
    "    X_old, Y_old = X_old[:10], Y_old[:10]\n",
    "    \n",
    "    X, Y = copy.deepcopy(X_old), copy.deepcopy(Y_old)\n",
    "    for idx, graph in enumerate(X):\n",
    "        if idx % 100 == 0: sys.stdout.write('\\r{:3.0f}%'.format(idx / len(X) * 100))\n",
    "        if graph.number_of_edges() == 0 or graph.number_of_nodes() == 0: continue\n",
    "        shortest_paths = nx.all_pairs_shortest_path(graph, cutoff=WALK_LENGTH)\n",
    "        for source, target_dict in shortest_paths.items():\n",
    "            for target, path in target_dict.items():\n",
    "                graph.add_edge(source, target, attr_dict = {'weight': 1 / len(path)})\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of classes per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import collections\n",
    "df = pd.DataFrame(columns = ['dataset', 'label', 'counts'])\n",
    "df['counts'] = df['counts'].astype(np.uint64)\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    if 'ana' in dataset: continue\n",
    "    #if dataset not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    label_counter = collections.Counter(Y)\n",
    "    df = df.append(pd.DataFrame([(dataset, label, count) for label, count in label_counter.items()], columns = df.columns), )\n",
    "\n",
    "for dataset, items in df.groupby('dataset'):\n",
    "    fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE_BIG)\n",
    "    num_els = items.counts.sum()\n",
    "    stdd = (items.counts / num_els).std()\n",
    "    items = items.set_index('label')\n",
    "    items.sort_values('counts').counts.plot(kind = 'barh', title = 'Dataset: {}, stdd/#docs: {:.2f}'.format(dataset, stdd))\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves concept-maps and coo-graphs graph datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import numpy as np\n",
    "import functools\n",
    "import collections\n",
    "\n",
    "# Tuples of: (dataset_name, graph_type, (X, Y))\n",
    "# For cooccurrence graphs, it will hold a (random) choice for each window size\n",
    "graph_datasets = []\n",
    "for dataset in ['ling-spam']:\n",
    "    print('{:30} start'.format(dataset))\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset)\n",
    "    gml_graph_cache = [x for x in graph_cache_files if 'gml' in x][0]\n",
    "    coo_graph_caches = [x for x in graph_cache_files if 'cooc' in x]\n",
    "    \n",
    "    def get_window_size(graph_cache_file):\n",
    "        return graph_cache_file.split('cooccurrence_')[1].split('_')[0]\n",
    "    \n",
    "    coo_graphs_by_window_size = collections.defaultdict(lambda: [])\n",
    "    for cache_file in coo_graph_caches:\n",
    "        coo_graphs_by_window_size[get_window_size(cache_file)].append(cache_file)\n",
    "\n",
    "    graph_datasets.append((dataset, 'concept-graph', dataset_helper.get_dataset_cached(gml_graph_cache)))\n",
    "    for window_size, cached_files in sorted(coo_graphs_by_window_size.items(), key=lambda x: x[0]):\n",
    "        # Take random element from the co-occurence graph datasets\n",
    "        coo_graph_cache = np.random.choice(cached_files)\n",
    "        print('\\tRetrieving co-occurence graphs for window_size={} ({})'.format(window_size, coo_graph_cache))\n",
    "        graph_datasets.append((dataset, 'coo-{}'.format(window_size), dataset_helper.get_dataset_cached(coo_graph_cache)))\n",
    "    print('{:30} finished'.format(dataset))\n",
    "    \n",
    "df_graph_datasets = pd.DataFrame(graph_datasets, columns = ['dataset_name', 'graph_type', 'dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = []\n",
    "for dataset, graph_type, (X, Y) in graph_datasets:\n",
    "    data += [(dataset, graph_type, nx.number_connected_components(graph)) for graph in X]\n",
    "\n",
    "df_connected_components = pd.DataFrame(data, columns = ['dataset_name', 'graph_type', 'connected_components'])\n",
    "\n",
    "for dataset, df_dataset in df_connected_components.groupby('dataset_name'):\n",
    "    for graph_type, df in df_dataset.groupby('graph_type'):\n",
    "        if graph_type != 'concept-graph': continue\n",
    "        fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE)\n",
    "        df.connected_components.plot(kind = 'hist', bins = 20, title = 'Dataset: {}, graph type: {}'.format(dataset, graph_type))\n",
    "        ax.set_xlabel('number of connected components')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig('tmp/hist-connected-components-{}-{}.png'.format(dataset, graph_type), dpi = EXPORT_DPI)\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density, #nodes, #edges histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figsize = (16, 4)\n",
    "NUM_BINS = 60\n",
    "alpha = 0.6\n",
    "\n",
    "graph_metrics = [\n",
    "    ('density', lambda graph: nx.density(graph) if graph.number_of_nodes() > 0 else 0.0),\n",
    "    ('number of nodes', lambda graph: graph.number_of_nodes()),\n",
    "    ('number of edges', lambda graph: graph.number_of_edges()),\n",
    "    ('connected components', lambda graph: nx.number_connected_components(graph)),\n",
    "    ('num_nodes_div_num_edges', lambda graph:  graph.number_of_nodes() / graph.number_of_edges() if graph.number_of_edges() > 0 else -99),\n",
    "    ('num_edges_div_num_nodes', lambda graph:  graph.number_of_edges() / graph.number_of_nodes() if graph.number_of_nodes() > 0 else -99)\n",
    "]\n",
    "\n",
    "for metric_name, metric in graph_metrics:\n",
    "    graph_metrics = []\n",
    "    for dataset, graph_type, (X, Y) in graph_datasets:\n",
    "        graph_metrics += [(dataset, graph_type, metric(graph)) for graph in X]\n",
    "\n",
    "    df = pd.DataFrame(graph_metrics, columns = ['dataset', 'graph_type', 'graph_metric'])\n",
    "    df = df[df.graph_metric > -10]\n",
    "    fig, ax = plt.subplots(figsize = (EXPORT_FIG_WIDTH + 2, EXPORT_FIG_HEIGHT + 1))\n",
    "    metrics_ = df.graph_metric.tolist()\n",
    "    binwidth = (max(metrics_) - min(metrics_)) / NUM_BINS\n",
    "    bins = np.arange(min(metrics_), max(metrics_) + binwidth, binwidth)\n",
    "    a = df.groupby('graph_type').graph_metric.plot(kind = 'hist',bins = bins, alpha = alpha, ax = ax, title = 'Histogram of {}'.format(metric_name), logy = True, legend = True)\n",
    "    medians = df.groupby('graph_type').graph_metric.median()\n",
    "    for median in medians:\n",
    "        ax.axvline(median, linewidth=1, alpha = alpha, color='b', linestyle='dashed')\n",
    "    ax.set_xlabel(metric_name)\n",
    "    plt.show()\n",
    "    fig.savefig('tmp/graph-statistics/hist-{}.png'.format(metric_name), dpi = EXPORT_DPI)\n",
    "    plt.close(fig)\n",
    "    #ax = sns.violinplot(x = 'graph_type', y = 'graph_densities', data=df, cut = 0, inner = 'quartile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot examples of graph types\n",
    "concept map and co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(graph_datasets, columns = ['dataset', 'graph_type', 'graph_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_GRAPHS_PER_TYPE = 4\n",
    "\n",
    "for dataset, data in df.groupby('dataset'):\n",
    "    fig, axes = plt.subplots(ncols=data.graph_type.value_counts().size, nrows=NUM_GRAPHS_PER_TYPE, figsize = EXPORT_FIG_SIZE_BIG)\n",
    "\n",
    "    for idx, row_ax in enumerate(axes):\n",
    "        print('Row: {}/{}'.format(idx, len(axes) - 1))\n",
    "        for (_, item), ax in zip(data.iterrows(), row_ax):\n",
    "            if idx == 0:\n",
    "                ax.set_title(item.graph_type)\n",
    "            X, Y = item.graph_dataset\n",
    "\n",
    "            random_graph = None\n",
    "            while not random_graph or nx.number_of_nodes(random_graph) not in range(5, 10):\n",
    "                random_graph = np.random.choice(X)\n",
    "                \n",
    "            #nx.draw_spring(random_graph, ax = ax, node_size = 20)#, style = 'dotted')\n",
    "            nx.draw_networkx(random_graph, ax = ax, node_size = 14, with_labels = False, node_color = '#000000')#, style = 'dotted')\n",
    "            \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.grid('off')\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_color('#FFFFFF')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig('tmp/graph-examples.png', dpi = EXPORT_DPI)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot unique word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import sympy\n",
    "import pandas as pd\n",
    "\n",
    "word_counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    text = []\n",
    "    for t in X:\n",
    "        text.append(t)\n",
    "    text = ' '.join(text)\n",
    "    text = text.lower().replace('\\n', ' ')\n",
    "    words = [x.strip() for x in text.split() if x.strip() != '']\n",
    "    unique_words = set(words)\n",
    "    word_counts.append((dataset_name, len(unique_words), len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocessing\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if 'ana' in dataset_name: continue\n",
    "    print(dataset_name)\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    X_pp = preprocessing.preprocess_text_spacy(X, concat = False, only_nouns = False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(word_counts, columns = ['dataset', 'unique_words', 'words']).set_index('dataset').sort_values('unique_words')\n",
    "df['unique_words_ratio'] = df.unique_words / df.words\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "df[['unique_words', 'words']].plot(kind = 'barh', logx = True, title = 'Unique word count', ax = ax)\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "df.unique_words_ratio.plot(kind = 'barh', title = '#Unique words/#words', ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed gml classification (single document, merged document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "from kernels import fast_wl\n",
    "import networkx as nx\n",
    "import graph_helper\n",
    "import numpy as np\n",
    "\n",
    "#'reuters-21578',\n",
    "for dataset in ['ng20']:\n",
    "    X, Y = dataset_helper.get_gml_graph_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_single, Y_single = dataset_helper.get_dataset_cached([x for x in dataset_helper.get_all_cached_graph_datasets(dataset_name='ng20') if 'gml' in x and 'single' in x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_classes(x, y, classes_to_keep = ()):\n",
    "    indices = [y_ in classes_to_keep for y_ in y]\n",
    "    return np.array(x, dtype = object)[indices].tolist(), np.array(y, dtype = object)[indices].tolist()\n",
    "\n",
    "X_single_filtered, Y_single_filtered = filter_classes(X_single, Y_single, set(Y))\n",
    "\n",
    "# Compute phi\n",
    "graph_helper.convert_graphs_to_adjs_tuples(X_single_filtered)\n",
    "graph_helper.convert_graphs_to_adjs_tuples(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merged = np.concatenate([X, X_single_filtered])\n",
    "Y_merged = np.concatenate([Y, Y_single_filtered])\n",
    "phi_lists, new_label_lookups, new_label_counters = fast_wl.fast_wl_compute(X_merged.tolist(), h = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "svm = sklearn.svm.LinearSVC()\n",
    "results = []\n",
    "for idx, phi in enumerate(phi_lists):\n",
    "    svm.fit(phi.T[:len(X),:], Y)\n",
    "    Y_pred = svm.predict(phi.T[len(X):,:])\n",
    "    results.append((idx, sklearn.metrics.f1_score(y_true = Y_single_filtered, y_pred=Y_pred, average = 'micro')))\n",
    "    #print(sklearn.metrics.classification_report(y_true = Y_single_filtered, y_pred = Y_pred)) #, average='macro')\n",
    "pd.DataFrame(results, columns = ['phi', 'f1_macro']).set_index('phi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepwalk\n",
    "from deepwalk import graph\n",
    "from deepwalk import walks as serialized_walks\n",
    "from gensim.models import Word2Vec\n",
    "from deepwalk.skipgram import Skipgram\n",
    "import dataset_helper\n",
    "import graph_helper\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from misc import tsne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_memory_data_size = 1000000000\n",
    "number_walks = 1000\n",
    "representation_size = 64\n",
    "seed = 0\n",
    "undirected = True\n",
    "vertex_freq_degree = False\n",
    "walk_length = 60\n",
    "window_size = 10\n",
    "workers = 1\n",
    "output = 'data/DUMP'\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}.npy'.format(dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=graph_helper.convert_dataset_to_co_occurence_graph_dataset, cache_file=cache_file)\n",
    "    break\n",
    "    \n",
    "models = []\n",
    "for idx, g in enumerate(X):\n",
    "    if idx == 3: break\n",
    "    print('Graph: {:>4}'.format(idx))\n",
    "    G = graph.from_networkx(g)\n",
    "\n",
    "    print(\"Number of nodes: {}\".format(len(G.nodes())))\n",
    "    if len(G.nodes()) == 0:\n",
    "        continue\n",
    "\n",
    "    num_walks = len(G.nodes()) * number_walks\n",
    "\n",
    "    print(\"Number of walks: {}\".format(num_walks))\n",
    "\n",
    "    data_size = num_walks * walk_length\n",
    "\n",
    "    print(\"Data size (walks*length): {}\".format(data_size))\n",
    "\n",
    "    print(\"Walking...\")\n",
    "    walks = graph.build_deepwalk_corpus(G, num_paths=number_walks, path_length=walk_length, alpha=0, rand=random.Random(seed))\n",
    "    print(\"Training...\")\n",
    "    model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, workers=workers)\n",
    "\n",
    "    #model.wv.save_word2vec_format(output)\n",
    "    models.append(model)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print('Next')\n",
    "    vectors = tsne.get_tsne_embedding(model)\n",
    "    tsne.plot_embedding(model, vectors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test WL phi computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge node labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "for (n, treshold), lookup in results.items():\n",
    "    cliques = coreference.get_cliques_from_lookup(lookup)\n",
    "    similarity_counter = {'similar': len(lookup.keys()), 'unsimilar': num_labels - len(lookup.keys())}\n",
    "    clique_lenghts = [len(x) for x in list(cliques.values())]\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n",
    "    fig.suptitle('Treshold: {}, N={}'.format(treshold, n), fontsize = 16)\n",
    "\n",
    "    pd.DataFrame(clique_lenghts).plot(ax = axes[0], kind = 'hist', logy = True, legend = False, title = \"Histogram of clique lengths\".format(treshold))\n",
    "    pd.DataFrame(list(similarity_counter.items()), columns = ['name', 'count']).set_index('name').plot(ax = axes[1], kind = 'bar', legend = False, title = '# of labels that have been merged vs. not merged')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    fig.savefig('tmp/{:.5f}.{}.png'.format(treshold, n), dpi = 120)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set('notebook', 'white')\n",
    "def plot_by(df, by, bins = 15, title = '', figsize = (12, 5), fontsize = 16):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for n, vals in df.groupby(by):\n",
    "        labels.append(n)\n",
    "        data.append(vals.clique_length)\n",
    "    ax.hist(data, bins = bins, alpha=0.7, label=labels, log = True)\n",
    "    fig.suptitle(title, fontsize = fontsize)\n",
    "    ax.legend(loc='upper right', fontsize = fontsize)\n",
    "    ax.set_xlabel('clique sizes')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    return fig, ax\n",
    "fig, ax = plot_by(df, 'n', title = 'Clique size histogram by n (all thresholds together)')\n",
    "fig.savefig('tmp/clique_size_by_n_all_thresholds.png', dpi = 120)\n",
    "fig, ax = plot_by(df, 'threshold', title = 'Clique size histogram by threshold (all n together)')\n",
    "fig.savefig('tmp/clique_size_by_threshold_all_n.png', dpi = 120)\n",
    "fig, ax = plot_by(df[df.threshold == 0.6], 'n', title = 'Clique size histogram by n (threshold=0.6)')\n",
    "fig.savefig('tmp/clique_size_by_n_threshold_0.6.png', dpi = 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coreference\n",
    "for lookup_file in glob('data/embeddings/graph-embeddings/*.threshold-*.*.label-lookup.npy'):\n",
    "    threshold, topn = get_treshold_and_topn_from_lookupfilename(lookup_file)\n",
    "    with open(lookup_file, 'rb') as f:\n",
    "        lookup = pickle.load(f)\n",
    "    for key in lookup.values():\n",
    "        if not isinstance(key, (str, int)):\n",
    "            print(\"?\")\n",
    "            break\n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup=lookup, title = 'threshold={}, topn={}'.format(threshold, topn))\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn\n",
    "graph_cache_file = 'dataset_graph_gml_ng20-single.npy'\n",
    "X, Y = dataset_helper.get_dataset_cached('data/CACHE/{}'.format(graph_cache_file))\n",
    "X, Y = np.array(X, dtype=object), np.array(Y, dtype=object)\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(n_splits = 40, random_state=42)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    X_test, Y_test = X[test_index], Y[test_index]\n",
    "    break\n",
    "with open('data/CACHE/dataset_graph_gml_small-single.npy', 'wb') as f:\n",
    "    pickle.dump((X_test.tolist(), Y_test.tolist()), f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dataset_helper\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set('notebook', 'white')\n",
    "limit_dataset = ['ng20', 'ling-spam', 'reuters-21578', 'webkb']\n",
    "limit_dataset = ['ling-spam']\n",
    "all_stats = {}\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in limit_dataset: continue\n",
    "    print(dataset_name)\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    graphs = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    number_of_nodes = []\n",
    "    \n",
    "    coo_graph = [x for x in graphs if 'cooccurrence' in x][0]\n",
    "    gml_graph = [x for x in graphs if 'gml' in x][0]\n",
    "    \n",
    "    def get_num_nodes(graph_file):\n",
    "        X_graph, _ = dataset_helper.get_dataset_cached(graph_file)\n",
    "        return [nx.number_of_nodes(x) for x in X_graph]\n",
    "    \n",
    "    stats = {\n",
    "        'cooccurrence': get_num_nodes(coo_graph),\n",
    "        'concept-graphs': get_num_nodes(gml_graph)\n",
    "    }\n",
    "    \n",
    "    min_len = min([len(v) for k, v in stats.items()])\n",
    "    graph_stats = {k: v[:min(min_len, len(v))] for k, v in stats.items()}\n",
    "    text_stats = {'doc_lengths': [len(x) for x in X]}\n",
    "    \n",
    "    all_stats[dataset_name] = {\n",
    "        'graphs': graph_stats,\n",
    "        'text': text_stats['doc_lengths'],\n",
    "        'num_docs': len(X),\n",
    "        'num_classes': len(set(Y))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 20\n",
    "\n",
    "for dataset_name, stats in all_stats.items():\n",
    "    graph_stats = stats['graphs']\n",
    "    text_stats = stats['text']\n",
    "    \n",
    "    df = pd.DataFrame(graph_stats)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols=2)\n",
    "    df.plot(kind = 'hist', bins=bins, alpha = 0.7, logy = True, ax=ax[0])\n",
    "    ax[0].set_xlabel('# nodes per graph')\n",
    "    \n",
    "    df = pd.DataFrame(text_stats)\n",
    "    df.plot(kind = 'hist', bins=bins, logy = True, legend = False, ax=ax[1])\n",
    "    ax[1].set_xlabel('# characters per document')\n",
    "    fig.savefig('tmp/other/stats-{}.png'.format(dataset_name), dpi = 150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "out = []\n",
    "for dataset, stats in all_stats.items():\n",
    "    out.append((dataset, np.mean(stats['text']), np.mean(stats['graphs']['cooccurrence']), np.mean(stats['graphs']['concept-graphs']), stats['num_docs'], stats['num_classes']))\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize = (8, 6))\n",
    "\n",
    "df = pd.DataFrame(out, columns = ['dataset', 'avg_doc_length', 'avg_coo_node_num', 'avg_cp_node_num', 'num_docs', 'num_classes'])\n",
    "df = df.set_index('dataset')\n",
    "#, ('# classes', 'num_classes')\n",
    "for idx, (name, x) in enumerate([('Average document length', 'avg_doc_length'), ('# documents', 'num_docs'), ('Average of number of concept-graph nodes', 'avg_cp_node_num')]):\n",
    "    ax = axes[idx]\n",
    "    df[x].plot(kind = 'barh', logx = True, title = name, ax = ax)\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/other/stats-datasets.png', dpi = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label count histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import graph_helper\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        #if 'gml' not in graph_cache_file: continue\n",
    "        if 'gml' in graph_cache_file: continue\n",
    "        print('Loading dataset: {}'.format(graph_cache_file))\n",
    "        X_old, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        label_counter = collections.Counter()\n",
    "        for graph in X_old:\n",
    "            labels = [str(x).strip() for x in graph.nodes()]\n",
    "            label_counter.update(labels)\n",
    "        counts_ = list(label_counter.values())\n",
    "        counts += list(zip([dataset_name] * len(counts_), counts_))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(counts, columns = ['dataset', 'counts'])\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "axes = df.hist(log = True, bins = 60, by = 'dataset', ax = axes)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlim((0, 10000))\n",
    "    pass\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/label-distribution-per-dataset.png', dpi = EXPORT_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from utils import helper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for pred_file in glob('data/results/predictions/*.npy'):\n",
    "    if 'gml' not in pred_file or 'ng20' not in pred_file: continue\n",
    "    with open(pred_file, 'rb') as f:\n",
    "        predictions = pickle.load(f)\n",
    "    Y_true, Y_pred = predictions['Y_real'], predictions['Y_pred']\n",
    "    assert len(Y_true) == len(Y_pred)\n",
    "    cmat = sklearn.metrics.confusion_matrix(y_true = Y_true, y_pred = Y_pred)\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        f1 = sklearn.metrics.f1_score(y_true = Y_true, y_pred = Y_pred, average = 'macro')\n",
    "        helper.plot_confusion_matrix(cmat, classes=set(Y_true), normalize = True, show_non_horizontal_percent=False, title='{} ({})'.format(f1, pred_file.split('/')[-1]))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    del predictions, Y_true, Y_pred\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {
    "height": "427px",
    "width": "443px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
