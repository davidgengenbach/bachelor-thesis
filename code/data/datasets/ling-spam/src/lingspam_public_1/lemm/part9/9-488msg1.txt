Subject: review of boguraev and pustejovsky

branimir boguraev and jame pustejovsky . 1996 . corpus process for lexical acquisition . mit press : cambridge , massachusett . 245 page . $ 32 . 50 . the term " acquisition " in the title of this book refer to automatic learn - - - acquisition not by human child , but by natural language system . the papers in this book deal with the topic of build and refine lexica for natural language system automatically - - i . e . by computer , with little or no human intervention - - from large corpus . build lexica for natural language system by hand be difficult , expensive , and labor-intensive , and the result may be out of date before it be complete . furthermore , by the standard of earlier system , lexica have become enormous . continuous speech dictation system ship with active vocabulary in the range of 30 , 0 lexical item . lexica in production by one company be expect to have 200 , 0 entry for american english and 700 , 0 entry for german . so , from an industrial point of view , work on the automatic acquisition of lexical knowledge be very welcome . this be not to say that automatic lexical acquisition should be of interest only to apply linguist . lexical information be also necessary in psycholinguistic research , and some of the work in this volume show such application . furthermore , the sort of datum that researcher in this field be attempt to acquire be just the sort of datum that be need for large-scale application of formalism like head - drive phrase structure grammar . so , the work describe in this book should be of interest to academic , as well as industrial , linguist . this book be the result of a workshop , and as such , it have the usual scatter of topic see in proceedings . this should be see as a feature , not a bug : the result be that there be something here for everyone . various papers come from the field of corpus linguistics , statistical analysis of language , psycholinguistic , rule acquisition , semantics , and lexical acquisition . the papers be divide into five broad category : ( 1 ) unknown word , ( 2 ) build representation , ( 3 ) categorization , ( 4 ) lexical semantics , and ( 5 ) evaluation . in addition , a paper by the editor lay out the reason for , and challenge of , automatic acquisition of lexical information . ( 1 ) introduction issue in text-base lexicon acquisition , branimir boguraev and jame pustejovsky . this paper present an in-depth answer to the question with which lexicon builder be perenially plague by anyone to whom they try to explain their work : why not just use an on-line dictionary ? the short answer be that such dictionary be static and do not evolve at the same pace as the language that they be attempt to describe . the long answer be that natural language system require information that be not reflect in traditional dictionaries-semantic feature geometry , subcategorization frame , and so on . so : " the fundamental problem of lexical acquisition . . . be how to provide , fully and adequately , the system with the lexical knowledge they need to operate with the proper degree of efficiency . the answer . . . to which the community be converge today . . . be to extract the lexicon from the text themselve " ( 3 ) . automatic lexical acquisition can trivially solve the short-answer problem by allow update as frequently as new datum can be acquire . more importantly , it allow the linguist to define the question that they would like the lexicon to answer , rather than have those question choose for them by the dictionary maker . ( 2 ) deal with unknown word consider a spell-check program that encounter the ( unknown ) word " horowitz . " the spell checker would like to know the best action to take with this word : be it a mis-spel that should be replace with something else , or be it a precious datum that should be add to its lexicon ? the spell-checker ask its user ; the papers in this section discuss attempt to answer these question automatically . linguist tend not to pay much attention to proper noun . as mcdonald put it in an epigram to his paper in this volume , " proper name be the rodney dangerfield of linguistics . they do n't get no respect " ( 21 ) . thus , it may surprise the reader to find that all three of the papers in this section deal with name . the identification and classification of name be , in fact , of considerable interest in natural language system . for relatively uninflect language like english , name may constitute the majority of unknown word encounter in a corpus . name raise special issue for classification , include the fact that they may have multiple form ; multiple form may have the same referent in a single text , raise problem for reference and coindexation ; and , on a less theoretically interest but no less morally and legally compel level , they may require special treatment in the corpus . for instance , proper name be routinely remove from medical datum , and may need to be remove from sociolinguistic datum , as well . internal and external evidence in the identification and semantic categorization of proper name . david d . mcdonald . this paper be write in the language of artificial intelligence . it describe the proper name facility of the sparser system . it describe the use of context-sensitive rewrite rule to analyze " external evidence " for proper name , e . g . their combinatorial property . a surprise and impressive aspect of the system describe here be that it do not use store list of proper noun . identify unknown proper name in newswire text . inderjeet manus , t . richard macmillan . this paper describe a method of use contextual clue such as appositive ( " < name > , the daughter of a prominent local physician " or " a niloticist of great repute , < name > " ) and felicity condition for identify name . the contextual clue themselve be then tap for datum about the referent of the name . categorize and standardize proper noun for efficient information retrieval . woojin paik , elizabeth d . liddy , edmund yu , and mary mckenna . this paper deal with discover and encode relationship between group and their member . paik et al . state the problem as follow : " proper noun be . . . important source of information for detect relevant document in information retrieval . . . . group proper noun ( e . g . , " middle east " ) and group common noun ( e . g . , " third world " ) will not match on their constituent unless the group entity be mention in the document " ( 61 ) . the problem , then , be to allow a search on " health care third world " to find a document on " health care in nicaragua . " the paper include a short but useful discussion of the problem that can arise with respect to preposition when noun phrase contain proper noun be parse as common noun phrase . ( the author solve this problem by change the order of two bracket routine . ) ( 3 ) build representation customize a lexicon to better suit a computational task . martus a . hearst , hinrich schuetze . as mention above , lexicon build be expensive ; this paper describe a method for reduce development cost by customize a pre-exist lexicon , rather than build a new one . the project describe here use as its pre-exist lexicon wordnet , an on-line lexicon that contain information about semantic relationship such as hypernymy , hyponymy , etc . this be customize by reduce the resolution of the semantic hierarchy to simple category , and by combine category from " distant part of the hierarchy . . . . . we be interest in find group of term that contribute to a frame or schema-like representation . . . this can be achieve by find associational lexical relation among the exist taxonymic relation " ( 79 ) . crucially , these relation should be derive from a particular corpus . the paper include a nice description of the algorithm use for collapse semantic category . toward build contextual representation of word senses use statistical model . claudium leacock , geoffrey towell , and ellen m . voorhee . this paper describe a method for differentiate amongst the multiple senses of a polysemous word . the author discuss use " topical context , " or content word occur in the vicinity , and " local context , " which include not just content word but function morpheme , word order , and syntactic structure . they test three method of acquire topical context : bayesian , context vector , and a neural network . they also give the result of a psycholinguistic experiment compare human performance with machine performance , give the topical context create by the three type of " classifier . " local context acquisition be base on acquire " template , " or specific sequence of word . this paper give a particularly nice description of its algorithm , and be so clearly write as to be suitable for presentation in course on statistics or psycholinguistic . ( 4 ) categorization a context drive conceptual cluster method for verb classification . roberto basilus , maria - teresa pazienza , paolum velardus . this paper describe a method of categorize verb with respect to thematic role , draw on the cobweb and ariosto _ lex system . it aim be to do categorization without relie on " define feature , " and to categorize with respect to the domain of discourse . the author describe their algorithm , and the paper have a nice literature review , cover both psycholinguistic and computational perspective on classification . distinguish usage . scott a . waterman . this paper tackle the syntax / semantics interface . the author attempt to give a linguistic ground to system that map text to some knowledge base by means of pattern match : " by relate lexical pattern-base approach to a lexical semantic framework , such as the generative lexicon theory [ pustejovsky , 1991 ] , my aim be to provide a basis through which pattern-base understand system can be understand in more conventional linguistic term . . . . . my main contention be that such a framework can be develop by view the lexical pattern as structural mapping from text to denotation in a compositional lexical semantics . . . obviate the need for separate syntactic and semantic analysis " ( 144 ) . this paper feature an excellent presentation of background idea and explication of the issue that it discuss . ( 5 ) lexical semantics detect dependency between semantic verb subclass and subcategorization frame in text corpus . victor poznanskus , antonio sanfilippo . this paper describe " a suite of program . . . . which elicit dependency between semantic verb class and their . . . subcategorization frame use machine readable thesaurus to assist in semantic tag of text " ( 176 ) . the system use a commercially available thesaurus-like online lexicon to do semantic tag . a " subcategorization frame " be then automatically extract , and the subcategorization frame be analyze and classify . acquire predicate-argument map information from multilingual text . chinatsu aone , dougla mckee . the author hold predicate-argument map to be equivalent to conceptual representation ; as such , it be clearly important to language understand . this be the only paper in the volume that deal with bilingual corpus . ( 6 ) evaluate acquisition evaluation technique for automatic semantic extraction : compare syntactic and window base approach . gregory grefenstette . this paper propose technique for compare " knowledge-poor " approach to determine the degree of semantic similarity between two word . a syntax-base method be compare to a window technique . the syntax-base method be show to perform better for high-frequency word , while the window method be the better performer for low-frequency word . conclusion this be by no means an introductory text on automatic lexical acquisition . nonetheless , this volume contain papers that will appeal to worker in a variety of linguistic discipline . the reviewer k . bretonnel cohen be a linguist at voice input technology in dublin , ohio , where his responsibility include the construction of tool for lexicon build and analysis .
