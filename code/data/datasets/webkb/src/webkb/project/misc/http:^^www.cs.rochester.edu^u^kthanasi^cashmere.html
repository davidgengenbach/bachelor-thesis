Date: Wednesday, 20-Nov-96 20:04:38 GMT
Server: NCSA/1.3
MIME-version: 1.0
Content-type: text/html
Last-modified: Thursday, 07-Nov-96 19:41:15 GMT
Content-length: 7148

<TITLE>CASHMERe Home Page</TITLE>

<CENTER>
<!WA0><IMG ALIGN=TOP SRC="http://www.cs.rochester.edu/u/scott/cashmere/cashmere.gif">
<P>
<H2> Coherence Algorithms for SHared MEmory aRchitectures </H2>
</CENTER>
<HR>

<H2>The CASHMERe Project</H2> 

<UL>
<LI> <!WA1><A HREF="#OVERVIEW"> Overview</A>
<LI> <!WA2><A HREF="#PEOPLE"> People</A>
<LI> <!WA3><A HREF="#PAPERS"> Papers </A>
</UL>
<HR>

<H2> <A NAME="OVERVIEW"> Overview </A> </H2>

CASHMERe stands for "Coherence Algorithms for SHared MEmory
aRchitectures" and is an ongoing effort to provide efficient,
scalable, shared memory with minimal hardware support.  It is well
accepted today that commercial workstations offer the best
price/performance ratio and that shared memory provides the most
desirable programming paradigm for parallel computing.  Unfortunately
shared memory emulations on networks of workstations provide
acceptable performance for only a limited class of applications.
CASHMERe attempts to bridge the performance gap between shared memory
emulations on networks of workstations and tightly-coupled
cache-coherent multiprocessors while using minimal hardware support.

<P>

In the context of CASHMERe we have discovered that NCC-NUMA (Non Cache
Coherent Non Uniform Memory Access) machines can greatly improve the
performance of <!WA4><A HREF="http://www.cs.rochester.edu/u/scott/cashmere/DSM_NCC.gif"> DSM systems</A>, and
approach that of <!WA5><A HREF="http://www.cs.rochester.edu/u/scott/cashmere/CC_NCC.gif"> fully hardware
coherent multiprocessors</A>.  The basic property of NCC-NUMA systems
is the ability to access remote memory directly; such a capability is
offered by a variety of network interfaces including DEC's Memory
Channel, HP's Hamlyn, and the Princeton <!WA6><A
HREF="http://www.cs.princeton.edu/shrimp/"> Shrimp</A>. Given current
technology the additional hardware cost of NCC-NUMA systems over pure
message passing systems is minimal.  Based on this fact and our
performance results we believe that NCC-NUMA machines lie near the knee
of the <!WA7><A HREF=http://www.cs.rochester.edu/u/scott/cashmere/Cost_Perf.gif> price-performance curve</A>.

<P> The department of <!WA8><A HREF="http://www.cs.rochester.edu"> Computer
Science</A> at the <!WA9><A HREF="http://www.rochester.edu"> University of
Rochester</A> is building a 32 processor <!WA10><A HREF="http://www.cs.rochester.edu/u/scott/cashmere/HW.gif">
Cashmere prototype</A>. Significant part of the funding comes in the
form of an equipment grant from <!WA11><A HREF="http://www.dec.com">Digital
Equipment Corporation</A>.  The prototype consists of eight
4-processor <!WA12><A
HREF="ftp://ftp.digital.com/pub/Digital/info/infosheet/EC-F4452-10.txt">DEC
2100</A> 4/233 multiprocessors on a <!WA13><A
HREF="http://www.digital.com:80/info/hpc/interconnects.html#MC_Overview">Memory
Channel</A> network.  The Memory Channel plugs into any PCI bus.  It
provides a memory-mapped network interface with which processors can
read and write remote locations without kernel intervention or
inter-processor interrupts.  End-to-end bandwidth is currently about
40MB/sec; remote write latency is about 3.5us.  The next hardware
generation is expected to increase bandwidth by approximately one
order of magnitude, and cut latency by half.  Cashmere augments the
functionality of the Memory Channel by providing cache coherence in
software.

<H2> <!WA14><A HREF="http://www.cs.rochester.edu/u/kthanasi/SSMM_96/talk.html">Implementation of Cashmere</A></H2>
Slides from the
<!WA15><A HREF="ftp://hopscotch.dolphinics.com/pub/asplos/ssm-workshop.html">
Workshop on Scalable Shared Memory Multiprocessors</A>,
Boston, MA, October 1996.

<H2> <A NAME="PEOPLE"> CASHMERe People </A> </H2>

The people behind CASHMERe are
<!WA16><A HREF="http://www.cs.rochester.edu/u/scott/"> Michael L. Scott</A>,
<!WA17><A HREF="http://www.cs.rochester.edu/u/wei/"> Wei Li</A>,
<!WA18><A HREF="http://www.cs.rochester.edu/u/sandhya/"> Sandhya Dwarkadas</A>,
<!WA19><A HREF="http://www.cs.rochester.edu/u/kthanasi/"> Leonidas Kontothanassis</A>,
<!WA20><A HREF="http://www.cs.rochester.edu/u/gchunt"> Galen Hunt</A>,
<!WA21><A HREF="http://www.cs.rochester.edu/u/michael"> Maged Michael</A>,
<!WA22><A HREF="http://www.cs.rochester.edu/u/stets"> Robert Stets</A>.
<!WA23><A HREF="http://www.cs.rochester.edu/u/nikolaos/"> Nikolaos Hardavellas</A>,
<!WA24><A HREF="http://www.cs.rochester.edu/u/si/"> Sotirios Ioannidis</A>,
<!WA25><A HREF="http://www.cs.rochester.edu/u/miera/"> Wagner Meira</A>,
<!WA26><A HREF="http://www.cs.rochester.edu/u/poulos/"> Alexandros Poulos</A>,
<!WA27><A HREF="http://www.cs.rochester.edu/u/cierniak/"> Michal Cierniak</A>,
<!WA28><A HREF="http://www.cs.rochester.edu/u/srini/"> Srinivasan Parthasarathy</A>,
and
<!WA29><A HREF="http://www.cs.rochester.edu/u/zaki/"> Mohammed Zaki</A>.

<H2> <A NAME="PAPERS"> CASHMERe papers </A> </H2>

<UL> 

<LI>
G. C. Hunt and M. L. Scott. <!WA30><A
HREF="ftp://ftp.cs.rochester.edu/pub/papers/systems/96.tr626.Using_peer_support_to_reduce_fault-tolerant_overhead.ps.gz">
``Using Peer Support to Reduce Fault-Tolerant Overhead in
Distributed Shared Memories''</A>.
TR 626, Computer Science Department, University of Rochester, June 1996.

<LI> L. I. Kontothanassis and M. L. Scott. <!WA31><A
HREF="ftp://ftp.cs.rochester.edu/pub/u/kthanasi/95.CAN.Efficient_Shared_Memory_Minimal_HW_Support.ps.gz">
``Efficient Shared Memory with Minimal Hardware Support''</A>. In
Computer Architecture News, September 1995.

<LI> L. I. Kontothanassis and M. L. Scott. <!WA32><A
HREF="ftp://ftp.cs.rochester.edu/pub/papers/systems/95.tr578.Distributed_shared_memory_for_new_generation_networks.ps.gz">
``Using Memory-Mapped Network Interfaces to Improve the Performance of
Distributed Shared Memory''</A>. In Proc., 2nd HPCA, San Jose, CA,
February 1996.

<LI> L. I. Kontothanassis, M. L. Scott, and R. Bianchini. <!WA33><A
HREF="http://www.cs.rochester.edu/u/kthanasi/SC95/sc95.html"> ``Lazy Release Consistency for
Hardware-Coherent Multiprocessors''</A>. In Proc., SUPERCOMPUTING '95,
San Diego, CA, December 1995.

<LI> L. I. Kontothanassis and M. L. Scott.  <!WA34><A
HREF="ftp://ftp.cs.rochester.edu/pub/u/kthanasi/95.JPDC.SW-Coherence.ps.gz">
``Software Cache Coherence for Current and Future
Architectures''</A>. In Special JPDC Issue on Scalable Shared Memory,
November 1995, V29, N2, pp 179-195.

<LI> L. I. Kontothanassis and M. L. Scott.  <!WA35><A
HREF="ftp://ftp.cs.rochester.edu/pub/u/kthanasi/94.HPCA.SW_coherence_Large_Scale_Multi.ps.Z">
``Software Cache Coherence for Large Scale Multiprocessors''</A>.  In
Proc., 1st HPCA, Raleigh, NC, January 1995.

<LI> M. Marchetti, L. I. Kontothanassis, R. Bianchini, and
M. L. Scott.  <!WA36><A
HREF="ftp://ftp.cs.rochester.edu/pub/papers/systems/94.tr535.Using_simple_page_placement_policies.ps.Z">
``Using Simple Page Placement Policies to Reduce the Cost of Cache
Fills in Coherent Shared-Memory Systems''</A>. In Proc., IPPS '95,
Santa Barbara, CA, April 1995.

<LI> M. Cierniak and Wei Li. <!WA37><A
HREF="ftp://ftp.cs.rochester.edu/pub/papers/systems/tr542.Unifying_data_and_control_transformations.ps.Z">
``Unifying Data and Control Transformations for Distributed
Shared-Memory Machines''</A>. In Proc., SIGPLAN '95 PLDI, La Jolla,
CA, June 1995. Also available as TR 542.

</UL>

For comments and/or requests send mail to
<!WA38><A HREF="mailto:kthanasi@crl.dec.com"> kthanasi@crl.dec.com </A>
or
<!WA39><A HREF="mailto:scott@cs.rochester.edu">scott@cs.rochester.edu</A>.

<HR>

<!WA40><A HREF="http://www.cs.rochester.edu/urcs.html">
<!WA41><IMG ALIGN=MIDDLE BORDER=NONE SRC="http://www.cs.rochester.edu/images/urcslogo.gif">
 URCS Home Page</A><P>

<HR>
