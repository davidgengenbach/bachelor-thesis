MIME-Version: 1.0
Server: CERN/3.0
Date: Monday, 06-Jan-97 19:42:10 GMT
Content-Type: text/html
Content-Length: 5397
Last-Modified: Monday, 25-Nov-96 01:52:52 GMT

<title>Michael Bogomolny</title>

<h1>Michael Bogomolny's CogSci Advert</h1>

<!WA0><!WA0><!WA0><!WA0><!WA0><!WA0><IMG SRC="http://www.cs.utexas.edu/users/bogo/finalback"></a> <br>
<!WA1><!WA1><!WA1><!WA1><!WA1><!WA1><IMG SRC="http://www.cs.utexas.edu/users/bogo/big1.jpg"></a> <br>
(although this is not a picture of me, sometimes it is how I feel 
after reading our articles)

<ul>
<li> currently in first semester of Computer Science Ph.D. program, 
University of Texas at Austin
<li> B.A. in Physics and Computer Science, Amherst College, 1994
</ul>

<br>
<h2>Research Interests</h2>
Note: I intend to work with <!WA2><!WA2><!WA2><!WA2><!WA2><!WA2><a href="http://net.cs.utexas.edu/users/risto/cs395t-cs/ads/husman">Jenefer Husman</a> on risk averse decisions for my final project.
<ul>
<p>
<li><large>Risk averse decisions </large> <br> <br>
If I were to bet you a quarter on the outcome of a coin toss, 
you would probably accept the bet. 
If I were to bet you $1000 on the same coin toss, 
you would probably reject the bet. Why do people reject fair bets? 
Well, there's a sound economic theory involving maximizing utility 
and the law of diminishing returns that explains this one. 
However, this does not explain why people, 
when asked the same question formulated in both a risk taking 
and a risk preventing manner, will respond differently. <br> <br>
Here's an example from our beloved Tversky and Kahneman, taken almost verbatim
from "The Framing of Decisions and the Psychology of Choice" (Science, 1981): <br> <br>
Imagine that the US is preparing for the outbreak of an unusual disease which 
is expected to kill 600 people. Two alternative programs to combat the disease have been
proposed. Assume that the exact scientific estimate of the consequences of the programs
are as follows: <br> <br>
Problem 1: [N=152] <br>
If program A is adopted, 200 people will be saved. [72%] <br>
If program B is adopted, there is 1/3 probability that 600 people will be
saved, and 2/3 probability that no people will be saved. [28%] <br> <br>
Problem 2: [N=155] <br>
If program A is adopted 400 people will die. [22%] <br>
If program B is adopted there is a 1/3 probability that nobody will die, and 2/3 
probability that 600 people will die. [78%] <br> <br>
Which one of the two programs would you favor? <br>

<p>
<li> Analog vs digital<br>
If humans' brains are made of neurons and neurons fire or not 
depending on the level of electrochemical charge built up at 
their axons does that make a brain analog? (My biological 
foundation here is shaky, by the way, so go ahead and scream 
at me if my hypothesis is wrong.) If computers transfer 
information over a bus and a particular wire has either high 
or low voltage that is then interpreted as 0 or 1, does that 
make computers analog as well?
<p>
<li> Accurate processing with inaccurate outcomes <br>
Sometimes we simply come up with the "wrong" answer. We make 
mistakes in subtraction while balancing our checkbook, or "remember" 
an invalid telephone number. (Or maybe she gave me a number that 
wasn't her real telephone number? But that's another story...) 
Nevertheless, we would be hard pressed to point to a misfiring of a 
neuron to account for this error. Can correct processing lead to 
incorrect results? (Is human cognitive processing 'sound' or even 
'complete'?) (REMINDER: Make those quotes italicized.) 
</dl>

<br>
<h3>Classes</h3>
<small>(append "Introduction to" to all these courses)</small><br><br>
<!WA3><!WA3><!WA3><!WA3><!WA3><!WA3><a href="http://www.cs.utexas.edu/users/rdb/cs195T">
Graduate Computer Science Research</a><br>
<!WA4><!WA4><!WA4><!WA4><!WA4><!WA4><a href="http://www.cs.utexas.edu/users/risto/cs395t-cs">
Cognitive Science</a><br>
<!WA5><!WA5><!WA5><!WA5><!WA5><!WA5><a href="http://www.cs.utexas.edu/users/users/novak/cwcs381k.html">
Aritificial Intelligence</a><br>
<!WA6><!WA6><!WA6><!WA6><!WA6><!WA6><a href="http://www.cs.utexas.edu/users/vl/teaching/description388L.html">
Mathematical Logic</a><br>
Topology <!WA7><!WA7><!WA7><!WA7><!WA7><!WA7><a href="http://www.ma.utexas.edu/users/ghrist">(Ghrist)</a><br>
<!WA8><!WA8><!WA8><!WA8><!WA8><!WA8><a href="http://www.cs.utexas.edu/users/lorenzo/corsi/cs372/96F">
Operating Systems</a><br>
<br>
(Add my 6 cogsci papers so far and the AI symbolic differentiator, 8-puzzle, 
and theorem prover.)

<br>
<br>
<br>
<h4>Contact Information</h4>
<dl>
<dt> Email: <!WA9><!WA9><!WA9><!WA9><!WA9><!WA9><a href="mailto:bogo@cs.utexas.edu" OnMouseOver="window.status='tell it like it is!';return true"> bogo@cs.utexas.edu</a> or, better yet, send me a <!WA10><!WA10><!WA10><!WA10><!WA10><!WA10><a href="http://postcards.www.media.mit.edu/Postcards" OnMouseOver="window.status='I just love these things!';return true"> postcard</a>
<dt> Phone: (512) 453-4955
<dt> Postal address:
<dd>
4200 Wilshire Parkway <br>
Austin, TX  78722 <br>
</dl>

<br>
<h5>Update Information</h5>
This page was written on 10/30/96 using the text editor "vi". It was last updated on 11/22/96.
(Insert empty promises about "under construction" and "will be updated soon" here.)

<br>
<h5>For this ad, we were supposed to list some 
<!WA11><!WA11><!WA11><!WA11><!WA11><!WA11><a href="http://www.cs.utexas.edu/users/bogo/hidden.html" 
OnMouseOver="window.status='Not so very hidden, actually.';return true">hidden</a> 
talents</h5>

<h6>definition of <!WA12><!WA12><!WA12><!WA12><!WA12><!WA12><a href="http://huis.huis.hiroshima-u.ac.jp/Computer/Jargon/LexiconEntries/Quantum_bogodynamics.html">quantum bogodynamics</a><br>
definition of <!WA13><!WA13><!WA13><!WA13><!WA13><!WA13><a href="http://huis.huis.hiroshima-u.ac.jp/Computer/Jargon/LexiconEntries/Bogo-sort.html">bogo-sort</a><br>
While you're there, feel free to look up bogosity, bogometer, bogus, bogon, bogon filter, bogon flux, bogue out, bogotify, autobogotiphobia, blinkenlights, and Lasherism. I am pleased to say that I or my work are in not way connected with these stupidities.</h6>
