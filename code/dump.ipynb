{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from transformers.wl_graph_kernel_transformer import WLGraphKernelTransformer\n",
    "from transformers.preprocessing_transformer import PreProcessingTransformer\n",
    "from transformers.d2v_transformer import Doc2VecTransformer\n",
    "import graph_helper\n",
    "import dataset_helper\n",
    "import wl\n",
    "import os\n",
    "\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name != 'r8': continue\n",
    "        \n",
    "    X, Y = dataset_helper.get_dataset(dataset_name, use_cached= True)\n",
    "    \n",
    "    p = Pipeline([\n",
    "        ('preprocessing', PreProcessingTransformer(only_nouns = True)),\n",
    "        ('d2v', Doc2VecTransformer()),\n",
    "        ('clf', sklearn.linear_model.PassiveAggressiveClassifier())\n",
    "    ])\n",
    "    \n",
    "    param_grid = dict(\n",
    "        d2v__embedding_size = [500],\n",
    "        d2v__iterations = [10],\n",
    "        d2v__infer_steps = [10],\n",
    "        clf__n_iter = [100],\n",
    "        clf__class_weight = ['balanced']\n",
    "    )\n",
    "\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits = 3, random_state= 42, shuffle= True)\n",
    "    gscv = GridSearchCV(estimator = p, param_grid=param_grid, cv=cv, scoring = 'f1_macro', n_jobs=1, verbose = 11)\n",
    "    gscv_result = gscv.fit(X, Y)\n",
    "    print(gscv_result.best_estimator_, gscv_result.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from wl_graph_kernel_transformer import WLGraphKernelTransformer\n",
    "import graph_helper\n",
    "import dataset_helper\n",
    "import wl\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "DATASET = 'ling-spam'\n",
    "GRAPH_TYPE = 'cooccurrence'\n",
    "SUFFIX = '-single'\n",
    "SUFFIX = ''\n",
    "cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_{}_{}{}.npy'.format(GRAPH_TYPE, DATASET, SUFFIX)\n",
    "\n",
    "if not os.path.exists(cache_file):\n",
    "    print('Could not find cachefile: \"{}\". Aborting.'.format(cache_file))\n",
    "else:\n",
    "    X, Y = dataset_helper.get_dataset(DATASET, use_cached = True, cache_file = cache_file)\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "p = Pipeline([\n",
    "    ('wl_transformer', WLGraphKernelTransformer()),\n",
    "    ('clf', None)\n",
    "])\n",
    "\n",
    "param_grid = dict(\n",
    "    wl_transformer__H= [3],\n",
    "    wl_transformer__n_jobs= [1],\n",
    "    clf = [sklearn.linear_model.PassiveAggressiveClassifier()],\n",
    "    clf__n_iter=[500],\n",
    "    clf__class_weight = ['balanced']\n",
    ")\n",
    "\n",
    "cv = GridSearchCV(estimator = p, param_grid=param_grid, cv=3, scoring = 'f1_macro', n_jobs=1, verbose = 11)\n",
    "gscv_result = cv.fit(X, Y)\n",
    "\n",
    "gscv_result.best_estimator_, gscv_result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dataset_helper\n",
    "import preprocessing\n",
    "import graph_helper\n",
    "import cooccurrence\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "def process(X, Y):\n",
    "    return graph_helper.convert_dataset_to_co_occurence_graph_dataset(X, Y, min_length = 2, window_size = window_size)\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    print(\"Creating co-occurence graphs for: {}\".format(dataset))\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}_{}.npy'.format(window_size, dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=process, cache_file=cache_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepwalk\n",
    "from deepwalk import graph\n",
    "from deepwalk import walks as serialized_walks\n",
    "from gensim.models import Word2Vec\n",
    "from deepwalk.skipgram import Skipgram\n",
    "import dataset_helper\n",
    "import graph_helper\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import tsne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_memory_data_size = 1000000000\n",
    "number_walks = 1000\n",
    "representation_size = 64\n",
    "seed = 0\n",
    "undirected = True\n",
    "vertex_freq_degree = False\n",
    "walk_length = 60\n",
    "window_size = 10\n",
    "workers = 1\n",
    "output = 'data/DUMP'\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}.npy'.format(dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=graph_helper.convert_dataset_to_co_occurence_graph_dataset, cache_file=cache_file)\n",
    "    break\n",
    "    \n",
    "models = []\n",
    "for idx, g in enumerate(X):\n",
    "    if idx == 3: break\n",
    "    print('Graph: {:>4}'.format(idx))\n",
    "    G = graph.from_networkx(g)\n",
    "\n",
    "    print(\"Number of nodes: {}\".format(len(G.nodes())))\n",
    "    if len(G.nodes()) == 0:\n",
    "        continue\n",
    "\n",
    "    num_walks = len(G.nodes()) * number_walks\n",
    "\n",
    "    print(\"Number of walks: {}\".format(num_walks))\n",
    "\n",
    "    data_size = num_walks * walk_length\n",
    "\n",
    "    print(\"Data size (walks*length): {}\".format(data_size))\n",
    "\n",
    "    print(\"Walking...\")\n",
    "    walks = graph.build_deepwalk_corpus(G, num_paths=number_walks, path_length=walk_length, alpha=0, rand=random.Random(seed))\n",
    "    print(\"Training...\")\n",
    "    model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, workers=workers)\n",
    "\n",
    "    #model.wv.save_word2vec_format(output)\n",
    "    models.append(model)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print('Next')\n",
    "    vectors = tsne.get_tsne_embedding(model)\n",
    "    tsne.plot_embedding(model, vectors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "from time import time\n",
    "phi_list_train_last = phi_list_train[-1]\n",
    "test_graphs = train[:100]\n",
    "\n",
    "def test_graph(idx, a):\n",
    "    topic, graph = a\n",
    "    #if idx % 1 == 0: print('{:>8}/{}'.format(idx, len(test_graphs)))\n",
    "    phi_train = wl.compute_phi(graph, phi_list_train_last.shape, label_lookup_train, label_counters_train, h = 1)\n",
    "    for i, (real, new) in enumerate(zip(phi_list_train, phi_train)):\n",
    "        real = real[:,idx]\n",
    "        new = lil_matrix(new.reshape(-1,1))\n",
    "        if not np.array_equiv(real.nonzero()[0], new.nonzero()[0]):\n",
    "            print('Phi not equal', i, 'Real', real, '\\nNew\\n', new)\n",
    "    print('Finished: {}'.format(idx))\n",
    "    \n",
    "mats = Parallel(n_jobs=2)(delayed(test_graph)(*d) for d in list(enumerate(test_graphs)))\n",
    "\n",
    "for idx, (topic, graph) in enumerate(test_graphs):\n",
    "    break\n",
    "    if idx % 1 == 0: print('{:>8}/{}'.format(idx, len(test_graphs)))\n",
    "    phi_train = wl.compute_phi(graph, phi_list_train_last.shape, label_lookup_train, label_counters_train, h = 1)\n",
    "    for i, (real, new) in enumerate(zip(real_, phi_train)):\n",
    "        real = real[:,idx]\n",
    "        new = lil_matrix(new.reshape(-1,1))\n",
    "        if not np.array_equiv(real.nonzero()[0], new.nonzero()[0]):\n",
    "            print('Phi not equal', i, 'Real', real, '\\nNew\\n', new)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import wl\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix, csr_matrix, vstack\n",
    "\n",
    "def get_all_nodes(gs):\n",
    "    return functools.reduce(lambda acc, x: acc | set(x), gs, set())\n",
    "\n",
    "def get_wl_args(graphs):\n",
    "    adjs = [nx.adjacency_matrix(g).toarray() for g in graphs]\n",
    "    nodes = [g.nodes() for g in graphs]\n",
    "    return adjs, nodes\n",
    "\n",
    "\n",
    "g1 = nx.DiGraph()\n",
    "g1.add_edge('A', 'B')\n",
    "g1.add_edge('B', 'C')\n",
    "\n",
    "g2 = nx.DiGraph()\n",
    "g2.add_edge('A', 'B')\n",
    "g2.add_edge('B', 'C')\n",
    "g2.add_edge('B', 'D')\n",
    "\n",
    "g3 = nx.DiGraph()\n",
    "g3.add_edge('E', 'F')\n",
    "g3.add_node('G')\n",
    "all_graphs = (g1, g2, g3)\n",
    "\n",
    "DEBUG = False\n",
    "H = 10\n",
    "\n",
    "all_nodes = get_all_nodes((g1, g2, g3))\n",
    "\n",
    "adjs, nodes = get_wl_args((g1, g2))\n",
    "K_1_2, phi_1_2, label_lookups_1_2, label_counters_1_2 = wl.WL_compute(ad_list=adjs, node_label=nodes, all_nodes=all_nodes, h = H, DEBUG=DEBUG)\n",
    "adjs, nodes = get_wl_args((g1, g2, g3))\n",
    "K_1_2_3, phi_1_2_3, label_lookups_1_2_3, label_counters_1_2_3 = wl.WL_compute(ad_list=adjs, node_label=nodes, all_nodes=all_nodes, h = H, DEBUG=DEBUG)\n",
    "\n",
    "TARGET_GRAPH = g3\n",
    "K_1_2_3_test, phi_1_2_3_test = wl.WL_compute_new(\n",
    "    ad_list=[nx.adjacency_matrix(TARGET_GRAPH).toarray()],\n",
    "    node_label=[TARGET_GRAPH.nodes()],\n",
    "    label_counters_prev = label_counters_1_2,\n",
    "    all_nodes= all_nodes,\n",
    "    h = H,\n",
    "    k_prev = np.copy(K_1_2),\n",
    "    phi_prev = np.copy(phi_1_2),\n",
    "    label_lookups_prev = np.copy(label_lookups_1_2)\n",
    ")\n",
    "\n",
    "phi_3_test = wl.compute_phi(g3, phi_1_2_3[0].shape, label_lookups_1_2_3, label_counters_1_2_3, h = H)\n",
    "for idx, (real, new) in enumerate(zip(phi_1_2_3, phi_3_test)):\n",
    "    real = real[:,2]\n",
    "    new = lil_matrix(new.reshape(-1,1))\n",
    "    if not np.array_equiv(real.todense(), new.todense()):\n",
    "        print('Phi not equal', idx)\n",
    "\n",
    "if 0 == 1:\n",
    "    for i, (a, b) in enumerate(zip(phi_1_2_3_test, phi_1_2_3)):\n",
    "        if not np.array_equiv(a - b.todense(), np.zeros(b.shape, dtype = np.int32)):\n",
    "            print(\"\\tPhi different! {}\".format(i))\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "\n",
    "    for i, (a, b) in enumerate(zip(K_1_2_3_test, K_1_2_3)):\n",
    "        if not np.array_equal(a, b):\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "            print(\"\\tK different! {}\".format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "for file in glob('data/results/*.npy'):\n",
    "    with open(file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    if 'ling-spam' not in file: continue\n",
    "    print('#### {}:\\n\\t{}'.format(file, results['mean_test_score'][0]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
