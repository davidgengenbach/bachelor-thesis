{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2893\n"
     ]
    }
   ],
   "source": [
    "import dataset_helper\n",
    "\n",
    "for graph_dataset_cache_file in dataset_helper.get_all_cached_graph_datasets():\n",
    "    if 'ling-spam' not in graph_dataset_cache_file: continue\n",
    "    X, Y = dataset_helper.get_dataset_cached(graph_dataset_cache_file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_helper\n",
    "all_node_labels = graph_helper.get_all_node_labels(X, as_sorted_list = False)\n",
    "with open('tmp/ling-spam-labels.txt', 'w') as f:\n",
    "    f.write('\\n'.join(all_node_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import preprocessing\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if 'ling-spam' not in dataset_name: continue\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name=dataset_name)\n",
    "    X = preprocessing.preprocess_text_spacy(X, concat = False, only_nouns = False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_as_words = [[y.text for y in x] for x in X]\n",
    "model = gensim.models.Word2Vec(X_as_words, size=50, window=5, min_count=1, workers=4)\n",
    "vocab = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsne\n",
    "import sklearn\n",
    "from sklearn import manifold\n",
    "tsne = sklearn.manifold.TSNE(n_components=2)\n",
    "tsne_vectors = tsne.fit_transform(vocab.syn0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "indexed_vocab = {v.index: k for k, v in vocab.vocab.items()}\n",
    "\n",
    "figsize = (12, 12)\n",
    "def plot_embedding_plt(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(indexed_vocab[i]),\n",
    "                 fontdict={'size': 14})\n",
    "\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "plot_embedding_plt(tsne_vectors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from transformers.wl_graph_kernel_transformer import WLGraphKernelTransformer\n",
    "from transformers.preprocessing_transformer import PreProcessingTransformer\n",
    "from transformers.d2v_transformer import Doc2VecTransformer\n",
    "import graph_helper\n",
    "import dataset_helper\n",
    "import wl\n",
    "import os\n",
    "\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name != 'r8': continue\n",
    "        \n",
    "    X, Y = dataset_helper.get_dataset(dataset_name, use_cached= True)\n",
    "    \n",
    "    p = Pipeline([\n",
    "        ('preprocessing', PreProcessingTransformer(only_nouns = True)),\n",
    "        ('d2v', Doc2VecTransformer()),\n",
    "        ('clf', sklearn.linear_model.PassiveAggressiveClassifier())\n",
    "    ])\n",
    "    \n",
    "    param_grid = dict(\n",
    "        d2v__embedding_size = [500],\n",
    "        d2v__iterations = [10],\n",
    "        d2v__infer_steps = [10],\n",
    "        clf__n_iter = [100],\n",
    "        clf__class_weight = ['balanced']\n",
    "    )\n",
    "\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits = 3, random_state= 42, shuffle= True)\n",
    "    gscv = GridSearchCV(estimator = p, param_grid=param_grid, cv=cv, scoring = 'f1_macro', n_jobs=1, verbose = 11)\n",
    "    gscv_result = gscv.fit(X, Y)\n",
    "    print(gscv_result.best_estimator_, gscv_result.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import w2v_d2v\n",
    "model = w2v_d2v.init_w2v_google()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create co-occurrence graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import dataset_helper\n",
    "import preprocessing\n",
    "import graph_helper\n",
    "import cooccurrence\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "\n",
    "window_size = 1\n",
    "min_length = 2\n",
    "\n",
    "def process(X, Y):\n",
    "    return graph_helper.convert_dataset_to_co_occurence_graph_dataset(X, Y, min_length = min_length, window_size = window_size, n_jobs = 1)\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    print(\"Creating co-occurence graphs for: {}\".format(dataset))\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}_with-nouns_{}.npy'.format(window_size, dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=process, cache_file=cache_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import deepwalk\n",
    "from deepwalk import graph\n",
    "from deepwalk import walks as serialized_walks\n",
    "from gensim.models import Word2Vec\n",
    "from deepwalk.skipgram import Skipgram\n",
    "import dataset_helper\n",
    "import graph_helper\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import tsne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_memory_data_size = 1000000000\n",
    "number_walks = 1000\n",
    "representation_size = 64\n",
    "seed = 0\n",
    "undirected = True\n",
    "vertex_freq_degree = False\n",
    "walk_length = 60\n",
    "window_size = 10\n",
    "workers = 1\n",
    "output = 'data/DUMP'\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}.npy'.format(dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=graph_helper.convert_dataset_to_co_occurence_graph_dataset, cache_file=cache_file)\n",
    "    break\n",
    "    \n",
    "models = []\n",
    "for idx, g in enumerate(X):\n",
    "    if idx == 3: break\n",
    "    print('Graph: {:>4}'.format(idx))\n",
    "    G = graph.from_networkx(g)\n",
    "\n",
    "    print(\"Number of nodes: {}\".format(len(G.nodes())))\n",
    "    if len(G.nodes()) == 0:\n",
    "        continue\n",
    "\n",
    "    num_walks = len(G.nodes()) * number_walks\n",
    "\n",
    "    print(\"Number of walks: {}\".format(num_walks))\n",
    "\n",
    "    data_size = num_walks * walk_length\n",
    "\n",
    "    print(\"Data size (walks*length): {}\".format(data_size))\n",
    "\n",
    "    print(\"Walking...\")\n",
    "    walks = graph.build_deepwalk_corpus(G, num_paths=number_walks, path_length=walk_length, alpha=0, rand=random.Random(seed))\n",
    "    print(\"Training...\")\n",
    "    model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, workers=workers)\n",
    "\n",
    "    #model.wv.save_word2vec_format(output)\n",
    "    models.append(model)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print('Next')\n",
    "    vectors = tsne.get_tsne_embedding(model)\n",
    "    tsne.plot_embedding(model, vectors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test WL phi computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "from time import time\n",
    "phi_list_train_last = phi_list_train[-1]\n",
    "test_graphs = train[:100]\n",
    "\n",
    "def test_graph(idx, a):\n",
    "    topic, graph = a\n",
    "    #if idx % 1 == 0: print('{:>8}/{}'.format(idx, len(test_graphs)))\n",
    "    phi_train = wl.compute_phi(graph, phi_list_train_last.shape, label_lookup_train, label_counters_train, h = 1)\n",
    "    for i, (real, new) in enumerate(zip(phi_list_train, phi_train)):\n",
    "        real = real[:,idx]\n",
    "        new = lil_matrix(new.reshape(-1,1))\n",
    "        if not np.array_equiv(real.nonzero()[0], new.nonzero()[0]):\n",
    "            print('Phi not equal', i, 'Real', real, '\\nNew\\n', new)\n",
    "    print('Finished: {}'.format(idx))\n",
    "    \n",
    "mats = Parallel(n_jobs=2)(delayed(test_graph)(*d) for d in list(enumerate(test_graphs)))\n",
    "\n",
    "for idx, (topic, graph) in enumerate(test_graphs):\n",
    "    break\n",
    "    if idx % 1 == 0: print('{:>8}/{}'.format(idx, len(test_graphs)))\n",
    "    phi_train = wl.compute_phi(graph, phi_list_train_last.shape, label_lookup_train, label_counters_train, h = 1)\n",
    "    for i, (real, new) in enumerate(zip(real_, phi_train)):\n",
    "        real = real[:,idx]\n",
    "        new = lil_matrix(new.reshape(-1,1))\n",
    "        if not np.array_equiv(real.nonzero()[0], new.nonzero()[0]):\n",
    "            print('Phi not equal', i, 'Real', real, '\\nNew\\n', new)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import wl\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix, csr_matrix, vstack\n",
    "\n",
    "def get_all_nodes(gs):\n",
    "    return functools.reduce(lambda acc, x: acc | set(x), gs, set())\n",
    "\n",
    "def get_wl_args(graphs):\n",
    "    adjs = [nx.adjacency_matrix(g).toarray() for g in graphs]\n",
    "    nodes = [g.nodes() for g in graphs]\n",
    "    return adjs, nodes\n",
    "\n",
    "\n",
    "g1 = nx.DiGraph()\n",
    "g1.add_edge('A', 'B')\n",
    "g1.add_edge('B', 'C')\n",
    "\n",
    "g2 = nx.DiGraph()\n",
    "g2.add_edge('A', 'B')\n",
    "g2.add_edge('B', 'C')\n",
    "g2.add_edge('B', 'D')\n",
    "\n",
    "g3 = nx.DiGraph()\n",
    "g3.add_edge('E', 'F')\n",
    "g3.add_node('G')\n",
    "all_graphs = (g1, g2, g3)\n",
    "\n",
    "DEBUG = False\n",
    "H = 10\n",
    "\n",
    "all_nodes = get_all_nodes((g1, g2, g3))\n",
    "\n",
    "adjs, nodes = get_wl_args((g1, g2))\n",
    "K_1_2, phi_1_2, label_lookups_1_2, label_counters_1_2 = wl.WL_compute(ad_list=adjs, node_label=nodes, all_nodes=all_nodes, h = H, DEBUG=DEBUG)\n",
    "adjs, nodes = get_wl_args((g1, g2, g3))\n",
    "K_1_2_3, phi_1_2_3, label_lookups_1_2_3, label_counters_1_2_3 = wl.WL_compute(ad_list=adjs, node_label=nodes, all_nodes=all_nodes, h = H, DEBUG=DEBUG)\n",
    "\n",
    "TARGET_GRAPH = g3\n",
    "K_1_2_3_test, phi_1_2_3_test = wl.WL_compute_new(\n",
    "    ad_list=[nx.adjacency_matrix(TARGET_GRAPH).toarray()],\n",
    "    node_label=[TARGET_GRAPH.nodes()],\n",
    "    label_counters_prev = label_counters_1_2,\n",
    "    all_nodes= all_nodes,\n",
    "    h = H,\n",
    "    k_prev = np.copy(K_1_2),\n",
    "    phi_prev = np.copy(phi_1_2),\n",
    "    label_lookups_prev = np.copy(label_lookups_1_2)\n",
    ")\n",
    "\n",
    "phi_3_test = wl.compute_phi(g3, phi_1_2_3[0].shape, label_lookups_1_2_3, label_counters_1_2_3, h = H)\n",
    "for idx, (real, new) in enumerate(zip(phi_1_2_3, phi_3_test)):\n",
    "    real = real[:,2]\n",
    "    new = lil_matrix(new.reshape(-1,1))\n",
    "    if not np.array_equiv(real.todense(), new.todense()):\n",
    "        print('Phi not equal', idx)\n",
    "\n",
    "if 0 == 1:\n",
    "    for i, (a, b) in enumerate(zip(phi_1_2_3_test, phi_1_2_3)):\n",
    "        if not np.array_equiv(a - b.todense(), np.zeros(b.shape, dtype = np.int32)):\n",
    "            print(\"\\tPhi different! {}\".format(i))\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "\n",
    "    for i, (a, b) in enumerate(zip(K_1_2_3_test, K_1_2_3)):\n",
    "        if not np.array_equal(a, b):\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "            print(\"\\tK different! {}\".format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "for file in glob('data/results/*.npy'):\n",
    "    with open(file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    if 'ling-spam' not in file: continue\n",
    "    print('#### {}:\\n\\t{}'.format(file.split('/')[-1], results['mean_test_score'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched phi calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import graph_helper\n",
    "import os\n",
    "import wl\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def get_size(file):\n",
    "    return os.path.getsize(file)\n",
    "\n",
    "for graph_cache_file in sorted(dataset_helper.get_all_cached_graph_datasets(), key = lambda x: get_size(x)):\n",
    "    X, Y = dataset_helper.get_dataset_cached(cache_file=graph_cache_file)\n",
    "    \n",
    "    graphs = [g for g in X[:2000] if nx.number_of_nodes(g) > 0 and nx.number_of_edges(g) > 0]\n",
    "    adjs, nodes = graph_helper.get_wl_args(graphs)\n",
    "    all_node_labels = graph_helper.get_all_node_labels(X)\n",
    "    print(\"finished\")\n",
    "    K, phi_list, label_lookups, label_counters = wl.WL_compute(adjs, nodes, 1, all_node_labels, compute_k=False )\n",
    "\n",
    "    #K_new, phi_list_new, label_lookups_new, label_counters_new = wl.WL_compute([adjs[-1]], [nodes[-1]], 1, all_node_labels, compute_k=False, initial_label_counters= label_counters, initial_label_lookups= label_lookups)\n",
    "    #print(np.array_equiv(phi_list[-1][:,-1].nonzero()[0], phi_list_new[-1][:,-1].nonzero()[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "h = 3\n",
    "start_1 = time()\n",
    "phi_lists, current_label_lookups, current_label_counters = wl.WL_compute_batched(adjs=adjs, node_label=nodes, batch_size=1000, all_nodes = all_node_labels, compute_k = False, h = h, DEBUG = True)\n",
    "start_2 = time()\n",
    "K, phi_list, label_lookups, label_counters = wl.WL_compute(adjs, nodes, all_nodes = all_node_labels, h = h, compute_k=False , DEBUG = True)\n",
    "end = time()\n",
    "print('First:\\t{}\\nSecond:\\t{}'.format(start_2 - start_1, end - start_2))\n",
    "print(phi_lists[-1].shape, len(adjs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(phi_lists[-1].nonzero(), phi_list[-1].nonzero())\n",
    "#print(np.array_equiv(phi_lists[-1].nonzero(), phi_list[-1].nonzero()))\n",
    "already_checked = set()\n",
    "for idx in zip(*phi_lists[-1].nonzero()):\n",
    "    val_a = phi_lists[-1][idx]\n",
    "    val_b = phi_list[-1][idx]\n",
    "    already_checked |= set(idx)\n",
    "    if val_a != val_b:\n",
    "        print(\"?\")\n",
    "        \n",
    "for idx in zip(*phi_list[-1].nonzero()):\n",
    "    val_a = phi_lists[-1][idx]\n",
    "    val_b = phi_list[-1][idx]\n",
    "    if val_a != val_b:\n",
    "        print(\"?\")\n",
    "\n",
    "#    print(row, col)\n",
    "#np.any(phi_lists[-1].nonzero()phi_list[-1].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_cade-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_ling-spam.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_mini20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_ng20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_ng20.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_r52-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_r8-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_r8.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_reuters-21578.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_webkb-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_no-nouns_webkb.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_cade-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_ling-spam.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_mini20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_ng20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_ng20.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_r52-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_r8-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_r8.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_reuters-21578.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_webkb-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_1_with-nouns_webkb.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_cade-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_ling-spam.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_mini20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_ng20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_ng20.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_r52-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_r8-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_r8.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_reuters-21578.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_webkb-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_2_no-nouns_webkb.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_cade-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_ling-spam.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_mini20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_ng20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_ng20.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_r52-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_r8-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_r8.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_reuters-21578.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_webkb-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_3_no-nouns_webkb.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_cade-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_ling-spam.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_mini20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_ng20-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_ng20.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_r52-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_r8-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_r8.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_reuters-21578.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_webkb-ana.npy\n",
      "data/CACHE/dataset_graph_cooccurrence_4_no-nouns_webkb.npy\n",
      "data/CACHE/dataset_graph_gml_ling-spam-single.npy\n",
      "data/CACHE/dataset_graph_gml_ng20-single.npy\n",
      "Starting: <function wl_fast at 0x1646e3bf8>\n",
      "Time needed for 14765 elements and 4 iterations: 67.08s\n",
      "data/CACHE/dataset_graph_gml_reuters-21578-single.npy\n",
      "data/CACHE/dataset_graph_gml_webkb-single.npy\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import fast_wl\n",
    "import dataset_helper\n",
    "from time import time\n",
    "import wl\n",
    "import graph_helper\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "H = 4\n",
    "\n",
    "def wl_old(X, h = H):\n",
    "    node_label = [sorted(g.nodes()) for g in X]\n",
    "    ad_list = [nx.adjacency_matrix(g, nodelist=label) for g, label in zip(X, node_label)]\n",
    "    all_nodes = graph_helper.get_all_node_labels(X)\n",
    "    K, phi_list, label_lookups, label_counters = wl.WL_compute(ad_list, node_label, h, all_nodes = all_nodes, compute_k = False, keep_phi_history = True, DEBUG = False)\n",
    "    return phi_list\n",
    "    \n",
    "def wl_fast(X, h = H):\n",
    "    phi_lists, new_label_lookups, new_label_counters = fast_wl.fast_wl_compute(X, h = H)\n",
    "    return phi_lists\n",
    "\n",
    "for dataset_cache_file in dataset_helper.get_all_cached_graph_datasets():\n",
    "    print(dataset_cache_file)\n",
    "    if False or 'gml' not in dataset_cache_file or 'ng20' not in dataset_cache_file: continue\n",
    "    X, Y = dataset_helper.get_dataset_cached(dataset_cache_file)\n",
    "    X, Y = zip(*[(x, y) for x, y in zip(X, Y) if nx.number_of_nodes(x) > 0 and nx.number_of_edges(x) > 0])\n",
    "    for x in [wl_fast]:\n",
    "    #for x in [wl_old, wl_fast]:\n",
    "        print('Starting: {}'.format(x))\n",
    "        t = time()\n",
    "        phi_lists = x(X)\n",
    "        print('Time needed for {} elements and {} iterations: {:.2f}s'.format(len(X), H, time() - t))\n",
    "\n",
    "if False:\n",
    "    g1 = nx.Graph()\n",
    "    g1.add_edge('A', 'B')\n",
    "    g1.add_edge('A', 'C')\n",
    "\n",
    "    g2 = g1.copy()\n",
    "\n",
    "    g3 = nx.Graph()\n",
    "    g3.add_edge('A', 'D')\n",
    "\n",
    "    graphs = [g1, g2, g3]\n",
    "    phi_lists, new_label_lookups, new_label_counters = fast_wl.fast_wl_compute(graphs, h = 1)\n",
    "    print(phi_lists[-1].todense())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
