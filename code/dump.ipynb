{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "EXPORT_DPI = 100\n",
    "EXPORT_FIG_SIZE = (8, 4)\n",
    "EXPORT_FIG_SIZE_BIG = (10, 7)\n",
    "EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT = EXPORT_FIG_SIZE\n",
    "EXPORT_FIG_WIDTH_BIG, EXPORT_FIG_HEIGHT_BIG = EXPORT_FIG_SIZE_BIG\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', 'whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar graphs by WL feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Retrieve graph datasets (both cmap and coo)\n",
    "\n",
    "Retrieve phi feature maps for coo and cmap, then get number of non-zero elements per graph (= per row of the feature map)\n",
    "\n",
    "And calculate the gram matrix, then find the most similar graphs per graph (= per row of the gram matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "dataset_name = 'ng20'\n",
    "dataset_name = 'ling-spam'\n",
    "\n",
    "results = collections.defaultdict(lambda: {})\n",
    "for graph_phi_file in dataset_helper.get_all_cached_graph_phi_datasets(dataset_name=dataset_name):\n",
    "    if 'gml' not in graph_phi_file: continue\n",
    "    print('Processing: {}'.format(graph_phi_file.split('/')[-1]))\n",
    "    phi, Y = dataset_helper.get_dataset_cached(graph_phi_file, check_validity=False)\n",
    "    \n",
    "    for h, phi_used in enumerate(phi):\n",
    "        print('\\th={}'.format(h))\n",
    "        # Generate kernel matrix\n",
    "        gram_matrix = phi_used.dot(phi_used.T).toarray()\n",
    "        results[graph_phi_file][h] = {}\n",
    "        results[graph_phi_file][h]['phi_used'] = phi_used\n",
    "        # Vector with the number of non-zero elements per row\n",
    "        # ie. non_zero_elements[idx] = len(phi_used[idx].nonzero())\n",
    "        results[graph_phi_file][h]['non_zero_elements'] = np.squeeze(np.asarray(np.sum(phi_used, axis = 1).T))\n",
    "        results[graph_phi_file][h]['num_elements'] = phi_used.shape[0]\n",
    "        results[graph_phi_file][h]['found_counter'] = collections.Counter()\n",
    "        results[graph_phi_file][h]['most_similar_scores'] = []\n",
    "        results[graph_phi_file][h]['similarity_pairs'] = []\n",
    "\n",
    "        for idx, row in enumerate(gram_matrix):\n",
    "            indices = np.argsort(row)[-10:]\n",
    "            # Search for index of this graph in the similar graph indices,\n",
    "            # it should be the most similar graph (because it's the same graph!)\n",
    "            results[graph_phi_file][h]['found_counter']['found' if idx in indices else 'not_found'] += 1\n",
    "            results[graph_phi_file][h]['most_similar_scores'].append(row[indices].tolist())\n",
    "            results[graph_phi_file][h]['similarity_pairs'].append(indices)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sparsity of feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for graph_cache_file, iterations in results.items():\n",
    "    fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE_BIG)\n",
    "    if 'gml' not in graph_cache_file and 'cooccurrence_1_all_ling' not in graph_cache_file: continue\n",
    "    for iteration, metrics in iterations.items():\n",
    "        if iteration != 0: continue\n",
    "        df = pd.DataFrame(metrics['non_zero_elements'], columns=['non_zero_elements'])\n",
    "        df.non_zero_elements.plot(kind='hist', bins = 40, alpha = 0.8, ax = ax, title = 'Histogram of non-zero entries in feature map (per row/element)')\n",
    "    \n",
    "    ax.set_xlim((0, 1000))\n",
    "    ax.set_xlabel('# of non-zero elements per row')\n",
    "    ax.legend(['Co-occurence graphs', 'Concept maps'])\n",
    "    plt.show()\n",
    "    fig.savefig('tmp/feature-map-sparsity-{}.png'.format(dataset_name), dpi = EXPORT_DPI)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get graph dataset for the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(filter(lambda x: 'gml' in x, results.keys())))\n",
    "graph_phi_file = 'data/CACHE/dataset_graph_gml_ling-spam-single.phi.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = graph_phi_file.split('/')[-1].split('.phi')[0]\n",
    "print(filename)\n",
    "candidates = [x for x in dataset_helper.get_all_cached_graph_datasets() if filename in x]\n",
    "assert len(candidates)\n",
    "X, Y = dataset_helper.get_dataset_cached(candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots()\n",
    "    choice = np.random.choice(len(X))\n",
    "    graph = nx.Graph(X[choice])\n",
    "    res = [(node, val) for node, val in nx.pagerank(graph).items()]\n",
    "    nodes = [node for node, val in res]\n",
    "    node_vals = np.array([val for node, val in res])\n",
    "    node_sizes = np.exp(node_vals * 20) * 20 / len(nodes) * 3\n",
    "    node_sizes = [0 for x in nodes]\n",
    "    nx.draw_networkx(graph, nodelist = nodes, with_labels=False, node_size = node_sizes, node_color='#000000')\n",
    "    ax.set_title('Graph#={}, connected_components={}'.format(choice, nx.number_connected_components(graph)))\n",
    "    ax.grid('off')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('#FFFFFF')\n",
    "    plt.show()\n",
    "#nx.draw_circular(graph, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(X[1576], with_labels=False, node_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text, _ = dataset_helper.get_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot similar graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "similarity_pairs = results[graph_phi_file]['similarity_pairs']\n",
    "similarity_scores = np.array(results[graph_phi_file]['most_similar_scores'])\n",
    "phi_used = results[graph_phi_file]['phi_used']\n",
    "\n",
    "#choice = np.random.choice(100)\n",
    "#graph, most_similar, most_similar_score = X[choice], similarity_pairs[choice], similarity_scores[choice]\n",
    "\n",
    "for idx, graph, most_similar, most_similar_score in zip(range(len(X)), X, similarity_pairs, similarity_scores):\n",
    "    if max(most_similar_score) > len(graph.nodes()) and max(most_similar_score) != 0:\n",
    "        #print(\"?\", idx, max(most_similar_score), len(graph.nodes()))\n",
    "        phi_similar = phi_used[idx].toarray()[0]\n",
    "        print(idx, phi_similar.nonzero(), graph.nodes())\n",
    "        continue\n",
    "    assert max(most_similar_score) <= len(graph.nodes())\n",
    "\n",
    "#choice, most_similar, len(graph.nodes()), max(most_similar_score), len(phi_used[choice].toarray()[0].nonzero()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_matrix = phi_used.dot(phi_used.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "similarity_pairs = results[graph_phi_file]['similarity_pairs']\n",
    "similarity_scores = np.array(results[graph_phi_file]['most_similar_scores'])\n",
    "\n",
    "def get_similarity(graph1_idx, graph2_idx):\n",
    "    return gram_matrix[graph1_idx, graph2_idx]\n",
    "\n",
    "def get_non_zero_phi_elements(idx):\n",
    "    return phi_used[idx].nonzero()[1]\n",
    "\n",
    "def plot_similar_graphs(graph_idx, num_to_plot = 2):\n",
    "    most_similar = np.argsort(gram_matrix[graph_idx])\n",
    "    filtered = [idx for idx in most_similar if nx.number_of_nodes(X[idx]) > 0 and idx != graph_idx and get_similarity(graph_idx, idx) != 0]\n",
    "    similar_graph_idxs = np.array(filtered[-num_to_plot:])\n",
    "    graph_idxs = [graph_idx] + similar_graph_idxs.tolist()\n",
    "    similarities = [get_similarity(graph_idx, idx) for idx in graph_idxs]\n",
    "    similar_labels = [set(X[idx].nodes()) & set(X[graph_idx].nodes()) for idx in graph_idxs]\n",
    "    reference_graph = X[graph_idx]\n",
    "    \n",
    "    print(phi_used.shape[0], gram_matrix.shape[0])\n",
    "    print('Graph={}'.format(graph_idx))\n",
    "    print('NonZeroPhi={}'.format(get_non_zero_phi_elements(graph_idx)))\n",
    "    print('SimilarOwn={}'.format(get_similarity(graph_idx, graph_idx)))\n",
    "    print('SimilarIdxs={}'.format(similar_graph_idxs))\n",
    "    print('Similarities={}'.format(similarities))\n",
    "    print('SimilarLabels={}'.format(similar_labels))\n",
    "    \n",
    "    for similar_graph_idx, graph, text in [(graph_idx, X[graph_idx], X_text[graph_idx]) for graph_idx in graph_idxs]:\n",
    "        graph = graph.copy()\n",
    "        graph.remove_nodes_from(set(graph.nodes()) - set(reference_graph.nodes()))\n",
    "        print(Y[graph_idx], Y[similar_graph_idx])\n",
    "        fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE)\n",
    "        nx.draw_circular(graph, ax = ax, node_size = 14, with_labels = True, node_color = '#000000')\n",
    "        ax.text(0, 0, str(similar_graph_idx))\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"?\")\n",
    "    random_choice = 1000\n",
    "    while nx.number_of_nodes(X[random_choice]) > 10:\n",
    "        random_choice = np.random.randint(0, len(X))\n",
    "    #random_choice = 1\n",
    "    plot_similar_graphs(random_choice)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Augment edges\n",
    "\n",
    "Add an edge from node1 to node2 if they are connected by a path of length N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import collections\n",
    "import networkx as nx\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "WALK_LENGTH = 2\n",
    "dataset_name = 'ng20'\n",
    "\n",
    "for graph_cache_file in dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset_name):\n",
    "    if 'coo' not in graph_cache_file or 'all' in graph_cache_file: continue\n",
    "    print(graph_cache_file)\n",
    "    X_old, Y_old = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "    \n",
    "    # TODO\n",
    "    X_old, Y_old = X_old[:10], Y_old[:10]\n",
    "    \n",
    "    X, Y = copy.deepcopy(X_old), copy.deepcopy(Y_old)\n",
    "    for idx, graph in enumerate(X):\n",
    "        if idx % 100 == 0: sys.stdout.write('\\r{:3.0f}%'.format(idx / len(X) * 100))\n",
    "        if graph.number_of_edges() == 0 or graph.number_of_nodes() == 0: continue\n",
    "        shortest_paths = nx.all_pairs_shortest_path(graph, cutoff=WALK_LENGTH)\n",
    "        for source, target_dict in shortest_paths.items():\n",
    "            for target, path in target_dict.items():\n",
    "                graph.add_edge(source, target, attr_dict = {'weight': 1 / len(path)})\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of classes per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import collections\n",
    "df = pd.DataFrame(columns = ['dataset', 'label', 'counts'])\n",
    "df['counts'] = df['counts'].astype(np.uint64)\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    if 'ana' in dataset: continue\n",
    "    #if dataset not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    X, Y = dataset_helper.get_dataset(dataset)\n",
    "    label_counter = collections.Counter(Y)\n",
    "    df = df.append(pd.DataFrame([(dataset, label, count) for label, count in label_counter.items()], columns = df.columns), )\n",
    "\n",
    "for dataset, items in df.groupby('dataset'):\n",
    "    fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE_BIG)\n",
    "    num_els = items.counts.sum()\n",
    "    stdd = (items.counts / num_els).std()\n",
    "    items = items.set_index('label')\n",
    "    items.sort_values('counts').counts.plot(kind = 'barh', title = 'Dataset: {}, stdd/#docs: {:.2f}'.format(dataset, stdd))\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves concept-maps and coo-graphs graph datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import numpy as np\n",
    "import functools\n",
    "import collections\n",
    "\n",
    "# Tuples of: (dataset_name, graph_type, (X, Y))\n",
    "# For cooccurrence graphs, it will hold a (random) choice for each window siz\n",
    "graph_datasets = []\n",
    "for dataset in ['ling-spam']:\n",
    "    print('{:30} start'.format(dataset))\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset)\n",
    "    gml_graph_cache = [x for x in graph_cache_files if 'gml' in x][0]\n",
    "    coo_graph_caches = [x for x in graph_cache_files if 'cooc' in x]\n",
    "    \n",
    "    def get_window_size(graph_cache_file):\n",
    "        return graph_cache_file.split('cooccurrence_')[1].split('_')[0]\n",
    "    \n",
    "    coo_graphs_by_window_size = collections.defaultdict(lambda: [])\n",
    "    for cache_file in coo_graph_caches:\n",
    "        coo_graphs_by_window_size[get_window_size(cache_file)].append(cache_file)\n",
    "\n",
    "    graph_datasets.append((dataset, 'concept-graph', dataset_helper.get_dataset_cached(gml_graph_cache)))\n",
    "    for window_size, cached_files in sorted(coo_graphs_by_window_size.items(), key=lambda x: x[0]):\n",
    "        # Take random element from the co-occurence graph datasets\n",
    "        coo_graph_cache = np.random.choice(cached_files)\n",
    "        print('\\tRetrieving co-occurence graphs for window_size={} ({})'.format(window_size, coo_graph_cache))\n",
    "        graph_datasets.append((dataset, 'coo-{}'.format(window_size), dataset_helper.get_dataset_cached(coo_graph_cache)))\n",
    "    print('{:30} finished'.format(dataset))\n",
    "    \n",
    "df_graph_datasets = pd.DataFrame(graph_datasets, columns = ['dataset_name', 'graph_type', 'dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = []\n",
    "for dataset, graph_type, (X, Y) in graph_datasets:\n",
    "    data += [(dataset, graph_type, nx.number_connected_components(graph)) for graph in X]\n",
    "\n",
    "df_connected_components = pd.DataFrame(data, columns = ['dataset_name', 'graph_type', 'connected_components'])\n",
    "\n",
    "for dataset, df_dataset in df_connected_components.groupby('dataset_name'):\n",
    "    for graph_type, df in df_dataset.groupby('graph_type'):\n",
    "        if graph_type != 'concept-graph': continue\n",
    "        fig, ax = plt.subplots(figsize = EXPORT_FIG_SIZE)\n",
    "        df.connected_components.plot(kind = 'hist', bins = 20, title = 'Dataset: {}, graph type: {}'.format(dataset, graph_type))\n",
    "        ax.set_xlabel('number of connected components')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig('tmp/hist-connected-components-{}-{}.png'.format(dataset, graph_type), dpi = EXPORT_DPI)\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density, #nodes, #edges histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figsize = (16, 4)\n",
    "NUM_BINS = 60\n",
    "alpha = 0.6\n",
    "\n",
    "graph_metrics = [\n",
    "    ('density', lambda graph: nx.density(graph) if graph.number_of_nodes() > 0 else 0.0),\n",
    "    ('number of nodes', lambda graph: graph.number_of_nodes()),\n",
    "    ('number of edges', lambda graph: graph.number_of_edges()),\n",
    "    ('connected components', lambda graph: nx.number_connected_components(graph)),\n",
    "    ('num_nodes_div_num_edges', lambda graph:  graph.number_of_nodes() / graph.number_of_edges() if graph.number_of_edges() > 0 else -99),\n",
    "    ('num_edges_div_num_nodes', lambda graph:  graph.number_of_edges() / graph.number_of_nodes() if graph.number_of_nodes() > 0 else -99)\n",
    "]\n",
    "\n",
    "for metric_name, metric in graph_metrics:\n",
    "    graph_metrics = []\n",
    "    for dataset, graph_type, (X, Y) in graph_datasets:\n",
    "        graph_metrics += [(dataset, graph_type, metric(graph)) for graph in X]\n",
    "\n",
    "    df = pd.DataFrame(graph_metrics, columns = ['dataset', 'graph_type', 'graph_metric'])\n",
    "    df = df[df.graph_metric > -10]\n",
    "    fig, ax = plt.subplots(figsize = (EXPORT_FIG_WIDTH + 2, EXPORT_FIG_HEIGHT + 1))\n",
    "    metrics_ = df.graph_metric.tolist()\n",
    "    binwidth = (max(metrics_) - min(metrics_)) / NUM_BINS\n",
    "    bins = np.arange(min(metrics_), max(metrics_) + binwidth, binwidth)\n",
    "    a = df.groupby('graph_type').graph_metric.plot(kind = 'hist',bins = bins, alpha = alpha, ax = ax, title = 'Histogram of {}'.format(metric_name), logy = True, legend = True)\n",
    "    medians = df.groupby('graph_type').graph_metric.median()\n",
    "    for median in medians:\n",
    "        ax.axvline(median, linewidth=1, alpha = alpha, color='b', linestyle='dashed')\n",
    "    ax.set_xlabel(metric_name)\n",
    "    plt.show()\n",
    "    fig.savefig('tmp/graph-statistics/hist-{}.png'.format(metric_name), dpi = EXPORT_DPI)\n",
    "    plt.close(fig)\n",
    "    #ax = sns.violinplot(x = 'graph_type', y = 'graph_densities', data=df, cut = 0, inner = 'quartile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot examples of graph types\n",
    "concept map and co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(graph_datasets, columns = ['dataset', 'graph_type', 'graph_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_GRAPHS_PER_TYPE = 4\n",
    "\n",
    "for dataset, data in df.groupby('dataset'):\n",
    "    fig, axes = plt.subplots(ncols=data.graph_type.value_counts().size, nrows=NUM_GRAPHS_PER_TYPE, figsize = EXPORT_FIG_SIZE_BIG)\n",
    "\n",
    "    for idx, row_ax in enumerate(axes):\n",
    "        print('Row: {}/{}'.format(idx, len(axes) - 1))\n",
    "        for (_, item), ax in zip(data.iterrows(), row_ax):\n",
    "            if idx == 0:\n",
    "                ax.set_title(item.graph_type)\n",
    "            X, Y = item.graph_dataset\n",
    "\n",
    "            random_graph = None\n",
    "            while not random_graph or nx.number_of_nodes(random_graph) not in range(5, 10):\n",
    "                random_graph = np.random.choice(X)\n",
    "                \n",
    "            #nx.draw_spring(random_graph, ax = ax, node_size = 20)#, style = 'dotted')\n",
    "            nx.draw_networkx(random_graph, ax = ax, node_size = 14, with_labels = False, node_color = '#000000')#, style = 'dotted')\n",
    "            \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.grid('off')\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_color('#FFFFFF')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig('tmp/graph-examples.png', dpi = EXPORT_DPI)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot unique word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import sympy\n",
    "import pandas as pd\n",
    "\n",
    "word_counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    text = []\n",
    "    for t in X:\n",
    "        text.append(t)\n",
    "    text = ' '.join(text)\n",
    "    text = text.lower().replace('\\n', ' ')\n",
    "    words = [x.strip() for x in text.split() if x.strip() != '']\n",
    "    unique_words = set(words)\n",
    "    word_counts.append((dataset_name, len(unique_words), len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if 'ana' in dataset_name: continue\n",
    "    print(dataset_name)\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    X_pp = preprocessing.preprocess_text_spacy(X, concat = False, only_nouns = False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(word_counts, columns = ['dataset', 'unique_words', 'words']).set_index('dataset').sort_values('unique_words')\n",
    "df['unique_words_ratio'] = df.unique_words / df.words\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "df[['unique_words', 'words']].plot(kind = 'barh', logx = True, title = 'Unique word count', ax = ax)\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "df.unique_words_ratio.plot(kind = 'barh', title = '#Unique words/#words', ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed gml classification (single document, merged document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "from kernels import fast_wl\n",
    "import networkx as nx\n",
    "import graph_helper\n",
    "import numpy as np\n",
    "\n",
    "#'reuters-21578',\n",
    "for dataset in ['ng20']:\n",
    "    X, Y = dataset_helper.get_gml_graph_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_single, Y_single = dataset_helper.get_dataset_cached([x for x in dataset_helper.get_all_cached_graph_datasets(dataset_name='ng20') if 'gml' in x and 'single' in x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_classes(x, y, classes_to_keep = ()):\n",
    "    indices = [y_ in classes_to_keep for y_ in y]\n",
    "    return np.array(x, dtype = object)[indices].tolist(), np.array(y, dtype = object)[indices].tolist()\n",
    "\n",
    "X_single_filtered, Y_single_filtered = filter_classes(X_single, Y_single, set(Y))\n",
    "\n",
    "# Compute phi\n",
    "graph_helper.convert_graphs_to_adjs_tuples(X_single_filtered)\n",
    "graph_helper.convert_graphs_to_adjs_tuples(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merged = np.concatenate([X, X_single_filtered])\n",
    "Y_merged = np.concatenate([Y, Y_single_filtered])\n",
    "phi_lists, new_label_lookups, new_label_counters = fast_wl.fast_wl_compute(X_merged.tolist(), h = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "svm = sklearn.svm.LinearSVC()\n",
    "results = []\n",
    "for idx, phi in enumerate(phi_lists):\n",
    "    svm.fit(phi.T[:len(X),:], Y)\n",
    "    Y_pred = svm.predict(phi.T[len(X):,:])\n",
    "    results.append((idx, sklearn.metrics.f1_score(y_true = Y_single_filtered, y_pred=Y_pred, average = 'micro')))\n",
    "    #print(sklearn.metrics.classification_report(y_true = Y_single_filtered, y_pred = Y_pred)) #, average='macro')\n",
    "pd.DataFrame(results, columns = ['phi', 'f1_macro']).set_index('phi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPGK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernels import spgk\n",
    "import dataset_helper\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "graph_dataset_caches = dataset_helper.get_all_cached_graph_datasets()\n",
    "for graph_cache in graph_dataset_caches:\n",
    "    if 'ling-spam' not in graph_cache: continue\n",
    "    print(graph_cache)\n",
    "    X, Y = dataset_helper.get_dataset_cached(graph_cache)\n",
    "    #X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, train_size = 0.7, stratify = Y)\n",
    "    X_train, y_train = X, Y\n",
    "    K = spgk.build_kernel_matrix(X_train, depth = 1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "NUM_ELEMENTS=int(K.shape[0] * 0.8)\n",
    "svm = sklearn.svm.SVC(kernel = 'precomputed', class_weight='balanced')\n",
    "#sklearn.model_selection.cross_val_score(svm, K, y_train)\n",
    "svm.fit(K[:NUM_ELEMENTS,:NUM_ELEMENTS], y_train[:NUM_ELEMENTS])\n",
    "#Y_pred = svm.predict(K[NUM_ELEMENTS:,:NUM_ELEMENTS])\n",
    "#print(collections.Counter(y_train[:NUM_ELEMENTS]))\n",
    "#print(sklearn.metrics.classification_report(y_true = y_train[NUM_ELEMENTS:], y_pred=Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepwalk\n",
    "from deepwalk import graph\n",
    "from deepwalk import walks as serialized_walks\n",
    "from gensim.models import Word2Vec\n",
    "from deepwalk.skipgram import Skipgram\n",
    "import dataset_helper\n",
    "import graph_helper\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import tsne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_memory_data_size = 1000000000\n",
    "number_walks = 1000\n",
    "representation_size = 64\n",
    "seed = 0\n",
    "undirected = True\n",
    "vertex_freq_degree = False\n",
    "walk_length = 60\n",
    "window_size = 10\n",
    "workers = 1\n",
    "output = 'data/DUMP'\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}.npy'.format(dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=graph_helper.convert_dataset_to_co_occurence_graph_dataset, cache_file=cache_file)\n",
    "    break\n",
    "    \n",
    "models = []\n",
    "for idx, g in enumerate(X):\n",
    "    if idx == 3: break\n",
    "    print('Graph: {:>4}'.format(idx))\n",
    "    G = graph.from_networkx(g)\n",
    "\n",
    "    print(\"Number of nodes: {}\".format(len(G.nodes())))\n",
    "    if len(G.nodes()) == 0:\n",
    "        continue\n",
    "\n",
    "    num_walks = len(G.nodes()) * number_walks\n",
    "\n",
    "    print(\"Number of walks: {}\".format(num_walks))\n",
    "\n",
    "    data_size = num_walks * walk_length\n",
    "\n",
    "    print(\"Data size (walks*length): {}\".format(data_size))\n",
    "\n",
    "    print(\"Walking...\")\n",
    "    walks = graph.build_deepwalk_corpus(G, num_paths=number_walks, path_length=walk_length, alpha=0, rand=random.Random(seed))\n",
    "    print(\"Training...\")\n",
    "    model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, workers=workers)\n",
    "\n",
    "    #model.wv.save_word2vec_format(output)\n",
    "    models.append(model)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print('Next')\n",
    "    vectors = tsne.get_tsne_embedding(model)\n",
    "    tsne.plot_embedding(model, vectors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test WL phi computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    for i, (a, b) in enumerate(zip(phi_1_2_3_test, phi_1_2_3)):\n",
    "        if not np.array_equiv(a - b.todense(), np.zeros(b.shape, dtype = np.int32)):\n",
    "            print(\"\\tPhi different! {}\".format(i))\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "\n",
    "    for i, (a, b) in enumerate(zip(K_1_2_3_test, K_1_2_3)):\n",
    "        if not np.array_equal(a, b):\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "            print(\"\\tK different! {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge node labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "for (n, treshold), lookup in results.items():\n",
    "    cliques = coreference.get_cliques_from_lookup(lookup)\n",
    "    similarity_counter = {'similar': len(lookup.keys()), 'unsimilar': num_labels - len(lookup.keys())}\n",
    "    clique_lenghts = [len(x) for x in list(cliques.values())]\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n",
    "    fig.suptitle('Treshold: {}, N={}'.format(treshold, n), fontsize = 16)\n",
    "\n",
    "    pd.DataFrame(clique_lenghts).plot(ax = axes[0], kind = 'hist', logy = True, legend = False, title = \"Histogram of clique lengths\".format(treshold))\n",
    "    pd.DataFrame(list(similarity_counter.items()), columns = ['name', 'count']).set_index('name').plot(ax = axes[1], kind = 'bar', legend = False, title = '# of labels that have been merged vs. not merged')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    fig.savefig('tmp/{:.5f}.{}.png'.format(treshold, n), dpi = 120)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set('notebook', 'white')\n",
    "def plot_by(df, by, bins = 15, title = '', figsize = (12, 5), fontsize = 16):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for n, vals in df.groupby(by):\n",
    "        labels.append(n)\n",
    "        data.append(vals.clique_length)\n",
    "    ax.hist(data, bins = bins, alpha=0.7, label=labels, log = True)\n",
    "    fig.suptitle(title, fontsize = fontsize)\n",
    "    ax.legend(loc='upper right', fontsize = fontsize)\n",
    "    ax.set_xlabel('clique sizes')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    return fig, ax\n",
    "fig, ax = plot_by(df, 'n', title = 'Clique size histogram by n (all thresholds together)')\n",
    "fig.savefig('tmp/clique_size_by_n_all_thresholds.png', dpi = 120)\n",
    "fig, ax = plot_by(df, 'threshold', title = 'Clique size histogram by threshold (all n together)')\n",
    "fig.savefig('tmp/clique_size_by_threshold_all_n.png', dpi = 120)\n",
    "fig, ax = plot_by(df[df.threshold == 0.6], 'n', title = 'Clique size histogram by n (threshold=0.6)')\n",
    "fig.savefig('tmp/clique_size_by_n_threshold_0.6.png', dpi = 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn\n",
    "graph_cache_file = 'dataset_graph_gml_ng20-single.npy'\n",
    "X, Y = dataset_helper.get_dataset_cached('data/CACHE/{}'.format(graph_cache_file))\n",
    "X, Y = np.array(X, dtype=object), np.array(Y, dtype=object)\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(n_splits = 40, random_state=42)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    X_test, Y_test = X[test_index], Y[test_index]\n",
    "    break\n",
    "with open('data/CACHE/dataset_graph_gml_small-single.npy', 'wb') as f:\n",
    "    pickle.dump((X_test.tolist(), Y_test.tolist()), f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set('notebook', 'white')\n",
    "limit_dataset = ['ng20', 'ling-spam', 'reuters-21578', 'webkb']\n",
    "#limit_dataset = ['ling-spam']\n",
    "all_stats = {}\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in limit_dataset: continue\n",
    "    print(dataset_name)\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    graphs = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    number_of_nodes = []\n",
    "    \n",
    "    coo_graph = [x for x in graphs if 'cooccurrence' in x][0]\n",
    "    gml_graph = [x for x in graphs if 'gml' in x][0]\n",
    "    \n",
    "    def get_num_nodes(graph_file):\n",
    "        X_graph, _ = dataset_helper.get_dataset_cached(graph_file)\n",
    "        return [nx.number_of_nodes(x) for x in X_graph]\n",
    "    \n",
    "    stats = {\n",
    "        'cooccurrence': get_num_nodes(coo_graph),\n",
    "        'concept-graphs': get_num_nodes(gml_graph)\n",
    "    }\n",
    "    \n",
    "    min_len = min([len(v) for k, v in stats.items()])\n",
    "    graph_stats = {k: v[:min(min_len, len(v))] for k, v in stats.items()}\n",
    "    text_stats = {'doc_lengths': [len(x) for x in X]}\n",
    "    \n",
    "    all_stats[dataset_name] = {\n",
    "        'graphs': graph_stats,\n",
    "        'text': text_stats['doc_lengths'],\n",
    "        'num_docs': len(X),\n",
    "        'num_classes': len(set(Y))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (12, 4)\n",
    "bins = 20\n",
    "\n",
    "for dataset_name, stats in all_stats.items():\n",
    "    graph_stats = stats['graphs']\n",
    "    text_stats = stats['text']\n",
    "    \n",
    "    df = pd.DataFrame(graph_stats)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols=2, figsize = figsize)\n",
    "    df.plot(kind = 'hist', bins=bins, alpha = 0.7, logy = True, ax=ax[0])\n",
    "    ax[0].set_xlabel('# nodes per graph')\n",
    "    \n",
    "    df = pd.DataFrame(text_stats)\n",
    "    df.plot(kind = 'hist', bins=bins, logy = True, legend = False, ax=ax[1])\n",
    "    ax[1].set_xlabel('# characters per document')\n",
    "    fig.savefig('tmp/other/stats-{}.png'.format(dataset_name), dpi = 150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "out = []\n",
    "for dataset, stats in all_stats.items():\n",
    "    out.append((dataset, np.mean(stats['text']), np.mean(stats['graphs']['cooccurrence']), np.mean(stats['graphs']['concept-graphs']), stats['num_docs'], stats['num_classes']))\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize = (8, 6))\n",
    "\n",
    "df = pd.DataFrame(out, columns = ['dataset', 'avg_doc_length', 'avg_coo_node_num', 'avg_cp_node_num', 'num_docs', 'num_classes'])\n",
    "df = df.set_index('dataset')\n",
    "#, ('# classes', 'num_classes')\n",
    "for idx, (name, x) in enumerate([('Average document length', 'avg_doc_length'), ('# documents', 'num_docs'), ('Average of number of concept-graph nodes', 'avg_cp_node_num')]):\n",
    "    ax = axes[idx]\n",
    "    df[x].plot(kind = 'barh', logx = True, title = name, ax = ax)\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/other/stats-datasets.png', dpi = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "\n",
    "check_graphs = False\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    \n",
    "    with open('data/embeddings/graph-embeddings/{}.label-lookup.npy'.format(dataset_name), 'rb') as f:\n",
    "        lookup = pickle.load(f)\n",
    "    \n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup, len(lookup.keys()))\n",
    "    plt.show()\n",
    "        \n",
    "    counter = collections.Counter()\n",
    "    all_labels = set()\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        X, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        X = trans.transform(X)\n",
    "        \n",
    "        for adj, labels in X:\n",
    "            all_labels |= set(labels)\n",
    "            for label in labels:\n",
    "                counter['found' if label in lookup and str(lookup[label]).strip() != str(label).strip() else 'not_found'] += 1\n",
    "    print(counter)\n",
    "    print(len(all_labels))\n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup, len(all_labels))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "import networkx as nx\n",
    "\n",
    "def merge_graphs(graphs):\n",
    "    return nx.compose_all(graphs)\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    \n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        if 'gml' not in graph_cache_file: continue\n",
    "        \n",
    "        X, Y = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        #X = trans.transform(X)\n",
    "        \n",
    "        num_labels = 0\n",
    "        all_labels = set()\n",
    "        all_labels_stripped = set()\n",
    "        \n",
    "        #for adj, labels in X:\n",
    "        for g in X:\n",
    "            labels = g.nodes()\n",
    "            all_labels |= set(labels)\n",
    "            all_labels_stripped |= set([str(label).strip() for label in labels])\n",
    "            num_labels += len(labels)\n",
    "    \n",
    "    num_uniq_labels = len(all_labels)\n",
    "    num_uniq_labels_stripped = len(all_labels_stripped)\n",
    "    print('#labels:\\t\\t{}'.format(num_labels))\n",
    "    print('#uniq. labels:\\t\\t{}'.format(num_uniq_labels))\n",
    "    print('#uniq. labels stripped:\\t{}'.format(num_uniq_labels_stripped))\n",
    "    print('#non-uniq. labels: \\t{}'.format(num_labels - num_uniq_labels))\n",
    "    \n",
    "    d = dataset_helper.get_dataset_dict(X, Y)\n",
    "    merged = {label: merge_graphs(graphs) for label, graphs in d.items()}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "from transformers.relabel_graphs_transformer import RelabelGraphsTransformer\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import coreference \n",
    "sns.set_style('white')\n",
    "\n",
    "def get_treshold_and_topn_from_lookupfilename(filename):\n",
    "    topn = filename.split('topn-')[1].split('.label')[0]\n",
    "    threshold = filename.split('threshold-')[1].split('.topn')[0]\n",
    "    return threshold, topn\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    #if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    if dataset_name not in ['ling-spam', 'ng20']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        print('Loading dataset: Start ({})'.format(graph_cache_file))\n",
    "        X_old, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        print('TupleTransform: Start')\n",
    "        X_old = trans.transform(X_old)\n",
    "        X_labels = [labels for _, labels in X_old]\n",
    "        \n",
    "        lookups = glob('data/embeddings/graph-embeddings/{}.*.*.label-lookup.npy'.format(dataset_name))\n",
    "        for lookup_file in lookups:\n",
    "            # Load lookup\n",
    "            threshold, topn = get_treshold_and_topn_from_lookupfilename(lookup_file)\n",
    "            with open(lookup_file, 'rb') as f:\n",
    "                lookup = pickle.load(f)\n",
    "            \n",
    "            # Relabel\n",
    "            relabel_trans = RelabelGraphsTransformer(lookup)\n",
    "            X = relabel_trans.transform(X_old)\n",
    "            duplicate_labels_count = []\n",
    "            different_counts = []\n",
    "            for (_, labels), old_labels in zip(X, X_labels):\n",
    "                labels_set = set(labels)\n",
    "                different_counts.append(collections.Counter([str(l1).lower().strip() != str(l2).lower().strip() for l1, l2 in zip(labels, old_labels)])[True])\n",
    "                duplicate_labels_count.append(len(labels) - len(labels_set))\n",
    "            label_counters = [len(labels) for _, labels in X]\n",
    "    \n",
    "            df = pd.DataFrame(list(zip(different_counts, duplicate_labels_count, label_counters)), columns = ['different_count', 'duplicate_labels_count', 'label_count'])\n",
    "            df['relabeled_ratio'] = df.different_count / df.label_count\n",
    "            df['duplicate_ratio'] = df.duplicate_labels_count / df.label_count\n",
    "            fig, ax = plt.subplots(figsize = (16, 4))\n",
    "            df[['relabeled_ratio', 'duplicate_ratio']].plot(kind = 'hist', bins = 100, log = True, title = 'Histogram (threshold={}, topn={})'.format(threshold, topn), ax = ax, alpha = 0.7)\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coreference\n",
    "for lookup_file in glob('data/embeddings/graph-embeddings/*.threshold-*.*.label-lookup.npy'):\n",
    "    threshold, topn = get_treshold_and_topn_from_lookupfilename(lookup_file)\n",
    "    with open(lookup_file, 'rb') as f:\n",
    "        lookup = pickle.load(f)\n",
    "    for key in lookup.values():\n",
    "        if not isinstance(key, (str, int)):\n",
    "            print(\"?\")\n",
    "            break\n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup=lookup, title = 'threshold={}, topn={}'.format(threshold, topn))\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import graph_helper\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        #if 'gml' not in graph_cache_file: continue\n",
    "        if 'gml' in graph_cache_file: continue\n",
    "        print('Loading dataset: {}'.format(graph_cache_file))\n",
    "        X_old, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        label_counter = collections.Counter()\n",
    "        for graph in X_old:\n",
    "            labels = [str(x).strip() for x in graph.nodes()]\n",
    "            label_counter.update(labels)\n",
    "        counts_ = list(label_counter.values())\n",
    "        counts += list(zip([dataset_name] * len(counts_), counts_))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(counts, columns = ['dataset', 'counts'])\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize = (7, 4))\n",
    "axes = df.hist(log = True, bins = 60, by = 'dataset', ax = axes)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlim((0, 10000))\n",
    "    pass\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/label-distribution-per-dataset.png', dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_elements = 10\n",
    "data = list(zip([\"a\"] * num_elements, range(num_elements))) + list(zip([\"b\"] * num_elements, range(num_elements)))\n",
    "df = pd.DataFrame(data, columns = [\"name\", \"counts\"])\n",
    "df.plot(kind = \"hist\", by = df.name)\n",
    "df.hist(by = df.name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    #if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    if dataset_name not in ['ling-spam']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        if 'gml' in graph_cache_file: continue\n",
    "        X, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(\"^[a-zA-Z0-9]+$\")\n",
    "all_labels = graph_helper.get_all_node_labels(X)\n",
    "new_labels = set()\n",
    "for label in all_labels:\n",
    "    new_labels.add(label.strip())\n",
    "    new_labels.add(label)\n",
    "len(all_labels), len(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "indices = [0, 1, 5, 8]\n",
    "mat = sparse.lil_matrix((10, 10))\n",
    "mat[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for pred_file in glob('data/results/predictions/*.npy'):\n",
    "    if 'gml' not in pred_file or 'ng20' not in pred_file: continue\n",
    "    with open(pred_file, 'rb') as f:\n",
    "        predictions = pickle.load(f)\n",
    "    Y_true, Y_pred = predictions['Y_real'], predictions['Y_pred']\n",
    "    assert len(Y_true) == len(Y_pred)\n",
    "    cmat = sklearn.metrics.confusion_matrix(y_true = Y_true, y_pred = Y_pred)\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        f1 = sklearn.metrics.f1_score(y_true = Y_true, y_pred = Y_pred, average = 'macro')\n",
    "        helper.plot_confusion_matrix(cmat, classes=set(Y_true), normalize = True, show_non_horizontal_percent=False, title='{} ({})'.format(f1, pred_file.split('/')[-1]))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    del predictions, Y_true, Y_pred\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import graph_helper\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    #if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    if dataset_name not in ['ng20']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    node_counts = []\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        if 'gml' in graph_cache_file: continue\n",
    "        print('Loading dataset: {}'.format(graph_cache_file))\n",
    "        X, Y = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        for x in X:\n",
    "            node_counts.append((graph_cache_file, len(x.nodes())))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(node_counts, columns = ['filename', 'node_counts'])\n",
    "df_all_words = df[df.filename.str.contains('_all_')]\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (14, 8))\n",
    "df_all_words.node_counts.plot(kind = 'hist', log = False, bins = 400, ax = ax, title = 'Node count histogramm (ng20)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('node count per graph')\n",
    "median = df_all_words.node_counts.median()\n",
    "ax.text(x = median, y = -400, s = 'Median: {:.0f}'.format(median), fontdict={'fontsize': 12}, horizontalalignment = 'center')\n",
    "#ax.set_xscale('log')\n",
    "ax.vlines(ymin = 0, ymax=6000, x = median, colors='red')\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/node-count-ng20.png', dpi = 100)\n",
    "display(df_all_words.node_counts.describe().to_frame().T)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "for cache_file in dataset_helper.get_all_cached_graph_phi_datasets(dataset_name = 'ling-spam'):\n",
    "    print(cache_file)\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        X, Y = pickle.load(f)\n",
    "    stacked = scipy.sparse.hstack(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_all_words[df_all_words.node_counts == 1].size, df_all_words.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPGK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "from kernels import spgk\n",
    "import networkx as nx\n",
    "from time import time\n",
    "\n",
    "depth = 1\n",
    "MAX_ELEMENTS = None\n",
    "    \n",
    "for graph_cache_file in dataset_helper.get_all_cached_graph_datasets(dataset_name='ling-spam' if True else None):\n",
    "    if 'all' in graph_cache_file: continue\n",
    "    print('Cache file: {}'.format(graph_cache_file.split('/')[-1]))\n",
    "    X, Y = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "    if not isinstance(X[0], nx.Graph):\n",
    "        print('\\tWrong X type, expected: nx.Graphs, got: {}'.format(type(X[0])))\n",
    "        continue\n",
    "\n",
    "    num_elements = MAX_ELEMENTS if MAX_ELEMENTS else len(X)\n",
    "    X = X[:num_elements]\n",
    "    \n",
    "    try:\n",
    "        start_time = time()\n",
    "        \n",
    "        for x in X:\n",
    "            for u,v,edata in x.edges(data = True):\n",
    "                if 'weight' not in edata: continue\n",
    "                if edata['weight'] < 1:\n",
    "                    print(\"Edge weight={:>2}, from='{}', to='{}'\".format(edata['weight'], u, v))\n",
    "                edata['weight'] = 1\n",
    "            if False:\n",
    "                self_loop_edges = x.selfloop_edges()\n",
    "                if len(self_loop_edges):\n",
    "                    x.remove_edges_from(self_loop_edges)\n",
    "\n",
    "        K = spgk.build_kernel_matrix(X, depth)\n",
    "        \n",
    "        time_needed = time() - start_time\n",
    "        \n",
    "        print('\\tNon-zero\\t\\t{:.0f}%'.format(100 * len(K.nonzero()[0]) / (K.shape[0] * K.shape[0])))\n",
    "        print('\\tTime needed')\n",
    "        print('\\t\\ttotal {}:\\t{:.2f}s'.format(num_elements, time_needed))\n",
    "        print('\\t\\tper element:\\t{:.2f}s'.format(time_needed / num_elements))\n",
    "        print('\\t\\tper 10000:\\t{:.2f}s'.format(time_needed / num_elements * 10000))\n",
    "    except Exception as e:\n",
    "        print('\\tError: {}'.format(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in X:\n",
    "    adj = nx.adjacency_matrix(graph)\n",
    "    adj[0, 0] = 2\n",
    "    adj[adj.nonzero()] = 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "graph = sparse.eye(2, 10)\n",
    "\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "class TupleSelector(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, selected_index = 0):\n",
    "        self.selected_index = selected_index\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return [x[self.selected_index] for x in X]\n",
    "\n",
    "estimators = [('TupleSelector', PCA()), ('kernel_pca', KernelPCA())]\n",
    "combined = FeatureUnion(estimators)\n",
    "\n",
    "trans = sklearn.feature_extraction.text.TfidfVectorizer(stop_words='english')\n",
    "trans.fit(['Yes scheint es ja zu schlaiken', 'Yes'])\n",
    "X_text = trans.transform(['yes', 'yes'])\n",
    "\n",
    "X_combined = sparse.hstack([graph, X_text])\n",
    "graph.shape, X_text.shape, X_combined.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "427px",
    "width": "443px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
