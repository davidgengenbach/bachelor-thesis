{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ed723bbf0995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwl_graph_kernel_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWLGraphKernelTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreProcessingTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2v_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2VecTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidgengenbach/Documents/Projekte/bachelor-thesis/code/transformers/wl_graph_kernel_transformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidgengenbach/Documents/Projekte/bachelor-thesis/code/graph_helper.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcooccurrence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidgengenbach/Documents/Projekte/bachelor-thesis/code/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidgengenbach/anaconda3/lib/python3.5/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/davidgengenbach/anaconda3/lib/python3.5/site-packages/spacy/en/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **overrides)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Special-case hack for loading the GloVe vectors, to support <1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_glove_vectors_loading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mLanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/davidgengenbach/anaconda3/lib/python3.5/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **overrides)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                          \u001b[0;32mif\u001b[0m \u001b[0;34m'vocab'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0madd_vectors\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidgengenbach/anaconda3/lib/python3.5/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_vocab\u001b[0;34m(cls, nlp)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             vocab = Vocab.load(nlp.path, lex_attr_getters=cls.lex_attr_getters,\n\u001b[0;32m---> 42\u001b[0;31m                              tag_map=cls.tag_map, lemmatizer=lemmatizer)\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtag_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorph_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0morth_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from transformers.wl_graph_kernel_transformer import WLGraphKernelTransformer\n",
    "from transformers.preprocessing_transformer import PreProcessingTransformer\n",
    "from transformers.d2v_transformer import Doc2VecTransformer\n",
    "import graph_helper\n",
    "import dataset_helper\n",
    "import wl\n",
    "import os\n",
    "\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name != 'r8': continue\n",
    "        \n",
    "    X, Y = dataset_helper.get_dataset(dataset_name, use_cached= True)\n",
    "    \n",
    "    p = Pipeline([\n",
    "        ('preprocessing', PreProcessingTransformer(only_nouns = True)),\n",
    "        ('d2v', Doc2VecTransformer()),\n",
    "        ('clf', sklearn.linear_model.PassiveAggressiveClassifier())\n",
    "    ])\n",
    "    \n",
    "    param_grid = dict(\n",
    "        d2v__embedding_size = [500],\n",
    "        d2v__iterations = [10],\n",
    "        d2v__infer_steps = [10],\n",
    "        clf__n_iter = [100],\n",
    "        clf__class_weight = ['balanced']\n",
    "    )\n",
    "\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits = 3, random_state= 42, shuffle= True)\n",
    "    gscv = GridSearchCV(estimator = p, param_grid=param_grid, cv=cv, scoring = 'f1_macro', n_jobs=1, verbose = 11)\n",
    "    gscv_result = gscv.fit(X, Y)\n",
    "    print(gscv_result.best_estimator_, gscv_result.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create co-occurrence graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import dataset_helper\n",
    "import preprocessing\n",
    "import graph_helper\n",
    "import cooccurrence\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "\n",
    "window_size = 1\n",
    "min_length = 2\n",
    "\n",
    "def process(X, Y):\n",
    "    return graph_helper.convert_dataset_to_co_occurence_graph_dataset(X, Y, min_length = min_length, window_size = window_size, n_jobs = 1)\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    print(\"Creating co-occurence graphs for: {}\".format(dataset))\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}_with-nouns_{}.npy'.format(window_size, dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=process, cache_file=cache_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import deepwalk\n",
    "from deepwalk import graph\n",
    "from deepwalk import walks as serialized_walks\n",
    "from gensim.models import Word2Vec\n",
    "from deepwalk.skipgram import Skipgram\n",
    "import dataset_helper\n",
    "import graph_helper\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import tsne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_memory_data_size = 1000000000\n",
    "number_walks = 1000\n",
    "representation_size = 64\n",
    "seed = 0\n",
    "undirected = True\n",
    "vertex_freq_degree = False\n",
    "walk_length = 60\n",
    "window_size = 10\n",
    "workers = 1\n",
    "output = 'data/DUMP'\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}.npy'.format(dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=graph_helper.convert_dataset_to_co_occurence_graph_dataset, cache_file=cache_file)\n",
    "    break\n",
    "    \n",
    "models = []\n",
    "for idx, g in enumerate(X):\n",
    "    if idx == 3: break\n",
    "    print('Graph: {:>4}'.format(idx))\n",
    "    G = graph.from_networkx(g)\n",
    "\n",
    "    print(\"Number of nodes: {}\".format(len(G.nodes())))\n",
    "    if len(G.nodes()) == 0:\n",
    "        continue\n",
    "\n",
    "    num_walks = len(G.nodes()) * number_walks\n",
    "\n",
    "    print(\"Number of walks: {}\".format(num_walks))\n",
    "\n",
    "    data_size = num_walks * walk_length\n",
    "\n",
    "    print(\"Data size (walks*length): {}\".format(data_size))\n",
    "\n",
    "    print(\"Walking...\")\n",
    "    walks = graph.build_deepwalk_corpus(G, num_paths=number_walks, path_length=walk_length, alpha=0, rand=random.Random(seed))\n",
    "    print(\"Training...\")\n",
    "    model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, workers=workers)\n",
    "\n",
    "    #model.wv.save_word2vec_format(output)\n",
    "    models.append(model)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print('Next')\n",
    "    vectors = tsne.get_tsne_embedding(model)\n",
    "    tsne.plot_embedding(model, vectors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test WL phi computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "from time import time\n",
    "phi_list_train_last = phi_list_train[-1]\n",
    "test_graphs = train[:100]\n",
    "\n",
    "def test_graph(idx, a):\n",
    "    topic, graph = a\n",
    "    #if idx % 1 == 0: print('{:>8}/{}'.format(idx, len(test_graphs)))\n",
    "    phi_train = wl.compute_phi(graph, phi_list_train_last.shape, label_lookup_train, label_counters_train, h = 1)\n",
    "    for i, (real, new) in enumerate(zip(phi_list_train, phi_train)):\n",
    "        real = real[:,idx]\n",
    "        new = lil_matrix(new.reshape(-1,1))\n",
    "        if not np.array_equiv(real.nonzero()[0], new.nonzero()[0]):\n",
    "            print('Phi not equal', i, 'Real', real, '\\nNew\\n', new)\n",
    "    print('Finished: {}'.format(idx))\n",
    "    \n",
    "mats = Parallel(n_jobs=2)(delayed(test_graph)(*d) for d in list(enumerate(test_graphs)))\n",
    "\n",
    "for idx, (topic, graph) in enumerate(test_graphs):\n",
    "    break\n",
    "    if idx % 1 == 0: print('{:>8}/{}'.format(idx, len(test_graphs)))\n",
    "    phi_train = wl.compute_phi(graph, phi_list_train_last.shape, label_lookup_train, label_counters_train, h = 1)\n",
    "    for i, (real, new) in enumerate(zip(real_, phi_train)):\n",
    "        real = real[:,idx]\n",
    "        new = lil_matrix(new.reshape(-1,1))\n",
    "        if not np.array_equiv(real.nonzero()[0], new.nonzero()[0]):\n",
    "            print('Phi not equal', i, 'Real', real, '\\nNew\\n', new)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import wl\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix, csr_matrix, vstack\n",
    "\n",
    "def get_all_nodes(gs):\n",
    "    return functools.reduce(lambda acc, x: acc | set(x), gs, set())\n",
    "\n",
    "def get_wl_args(graphs):\n",
    "    adjs = [nx.adjacency_matrix(g).toarray() for g in graphs]\n",
    "    nodes = [g.nodes() for g in graphs]\n",
    "    return adjs, nodes\n",
    "\n",
    "\n",
    "g1 = nx.DiGraph()\n",
    "g1.add_edge('A', 'B')\n",
    "g1.add_edge('B', 'C')\n",
    "\n",
    "g2 = nx.DiGraph()\n",
    "g2.add_edge('A', 'B')\n",
    "g2.add_edge('B', 'C')\n",
    "g2.add_edge('B', 'D')\n",
    "\n",
    "g3 = nx.DiGraph()\n",
    "g3.add_edge('E', 'F')\n",
    "g3.add_node('G')\n",
    "all_graphs = (g1, g2, g3)\n",
    "\n",
    "DEBUG = False\n",
    "H = 10\n",
    "\n",
    "all_nodes = get_all_nodes((g1, g2, g3))\n",
    "\n",
    "adjs, nodes = get_wl_args((g1, g2))\n",
    "K_1_2, phi_1_2, label_lookups_1_2, label_counters_1_2 = wl.WL_compute(ad_list=adjs, node_label=nodes, all_nodes=all_nodes, h = H, DEBUG=DEBUG)\n",
    "adjs, nodes = get_wl_args((g1, g2, g3))\n",
    "K_1_2_3, phi_1_2_3, label_lookups_1_2_3, label_counters_1_2_3 = wl.WL_compute(ad_list=adjs, node_label=nodes, all_nodes=all_nodes, h = H, DEBUG=DEBUG)\n",
    "\n",
    "TARGET_GRAPH = g3\n",
    "K_1_2_3_test, phi_1_2_3_test = wl.WL_compute_new(\n",
    "    ad_list=[nx.adjacency_matrix(TARGET_GRAPH).toarray()],\n",
    "    node_label=[TARGET_GRAPH.nodes()],\n",
    "    label_counters_prev = label_counters_1_2,\n",
    "    all_nodes= all_nodes,\n",
    "    h = H,\n",
    "    k_prev = np.copy(K_1_2),\n",
    "    phi_prev = np.copy(phi_1_2),\n",
    "    label_lookups_prev = np.copy(label_lookups_1_2)\n",
    ")\n",
    "\n",
    "phi_3_test = wl.compute_phi(g3, phi_1_2_3[0].shape, label_lookups_1_2_3, label_counters_1_2_3, h = H)\n",
    "for idx, (real, new) in enumerate(zip(phi_1_2_3, phi_3_test)):\n",
    "    real = real[:,2]\n",
    "    new = lil_matrix(new.reshape(-1,1))\n",
    "    if not np.array_equiv(real.todense(), new.todense()):\n",
    "        print('Phi not equal', idx)\n",
    "\n",
    "if 0 == 1:\n",
    "    for i, (a, b) in enumerate(zip(phi_1_2_3_test, phi_1_2_3)):\n",
    "        if not np.array_equiv(a - b.todense(), np.zeros(b.shape, dtype = np.int32)):\n",
    "            print(\"\\tPhi different! {}\".format(i))\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "\n",
    "    for i, (a, b) in enumerate(zip(K_1_2_3_test, K_1_2_3)):\n",
    "        if not np.array_equal(a, b):\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "            print(\"\\tK different! {}\".format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "for file in glob('data/results/*.npy'):\n",
    "    with open(file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    if 'ling-spam' not in file: continue\n",
    "    print('#### {}:\\n\\t{}'.format(file.split('/')[-1], results['mean_test_score'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched phi calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import graph_helper\n",
    "import os\n",
    "import wl\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def get_size(file):\n",
    "    return os.path.getsize(file)\n",
    "\n",
    "for graph_cache_file in sorted(dataset_helper.get_all_cached_graph_datasets(), key = lambda x: get_size(x)):\n",
    "    X, Y = dataset_helper.get_dataset_cached(cache_file=graph_cache_file)\n",
    "    \n",
    "    graphs = [g for g in X[:2000] if nx.number_of_nodes(g) > 0 and nx.number_of_edges(g) > 0]\n",
    "    adjs, nodes = graph_helper.get_wl_args(graphs)\n",
    "    all_node_labels = graph_helper.get_all_node_labels(X)\n",
    "    print(\"finished\")\n",
    "    K, phi_list, label_lookups, label_counters = wl.WL_compute(adjs, nodes, 1, all_node_labels, compute_k=False )\n",
    "\n",
    "    #K_new, phi_list_new, label_lookups_new, label_counters_new = wl.WL_compute([adjs[-1]], [nodes[-1]], 1, all_node_labels, compute_k=False, initial_label_counters= label_counters, initial_label_lookups= label_lookups)\n",
    "    #print(np.array_equiv(phi_list[-1][:,-1].nonzero()[0], phi_list_new[-1][:,-1].nonzero()[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "h = 3\n",
    "start_1 = time()\n",
    "phi_lists, current_label_lookups, current_label_counters = wl.WL_compute_batched(adjs=adjs, node_label=nodes, batch_size=1000, all_nodes = all_node_labels, compute_k = False, h = h, DEBUG = True)\n",
    "start_2 = time()\n",
    "K, phi_list, label_lookups, label_counters = wl.WL_compute(adjs, nodes, all_nodes = all_node_labels, h = h, compute_k=False , DEBUG = True)\n",
    "end = time()\n",
    "print('First:\\t{}\\nSecond:\\t{}'.format(start_2 - start_1, end - start_2))\n",
    "print(phi_lists[-1].shape, len(adjs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(phi_lists[-1].nonzero(), phi_list[-1].nonzero())\n",
    "#print(np.array_equiv(phi_lists[-1].nonzero(), phi_list[-1].nonzero()))\n",
    "already_checked = set()\n",
    "for idx in zip(*phi_lists[-1].nonzero()):\n",
    "    val_a = phi_lists[-1][idx]\n",
    "    val_b = phi_list[-1][idx]\n",
    "    already_checked |= set(idx)\n",
    "    if val_a != val_b:\n",
    "        print(\"?\")\n",
    "        \n",
    "for idx in zip(*phi_list[-1].nonzero()):\n",
    "    val_a = phi_lists[-1][idx]\n",
    "    val_b = phi_list[-1][idx]\n",
    "    if val_a != val_b:\n",
    "        print(\"?\")\n",
    "\n",
    "#    print(row, col)\n",
    "#np.any(phi_lists[-1].nonzero()phi_list[-1].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30 30  0]\n",
      " [30 30  0]\n",
      " [ 0  0 20]]\n",
      "[[1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import sympy\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, coo_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_prime_range(start, end):\n",
    "    return list(sympy.primerange(start, end + 1))\n",
    "\n",
    "primes_arguments_required = [2, 3, 7, 19, 53, 131, 311, 719, 1619, 3671, 8161, 17863, 38873, 84017, 180503, 386093, 821641, 1742537, 3681131, 7754077, 16290047]\n",
    "\n",
    "def fast_wl_compute(graphs, h = 1, label_lookup = {}, primes_arguments_required = primes_arguments_required):\n",
    "    num_labels = len(label_lookup.keys())\n",
    "    num_graphs = len(graphs)\n",
    "    \n",
    "    primes_needed = primes_arguments_required[int(np.ceil(np.log2(num_labels)) + 2)]\n",
    "    p = get_prime_range(1, primes_needed)\n",
    "    log_primes = np.log(p)\n",
    "    label_counters = []\n",
    "    label_lookups = []\n",
    "    labels = [sorted(g.nodes()) for g in graphs]\n",
    "    adjs = [nx.adjacency_matrix(graph, nodelist=labels_) for graph, labels_ in zip(graphs, labels)]\n",
    "    phi_shape = (sum(len(x) for x in labels), num_graphs)\n",
    "    \n",
    "    # Uncomment when gram/kernel matrix is wanted\n",
    "    #K = np.zeros((num_graphs, num_graphs), dtype = np.uint32)\n",
    "    phi_lists = []\n",
    "    for i in range(h):\n",
    "        label_lookup = {}\n",
    "        label_counter = 1\n",
    "        phis = []\n",
    "        phi = lil_matrix(phi_shape, dtype = np.uint8)\n",
    "        for idx, (labels_, A) in enumerate(zip(labels, adjs)):\n",
    "            signatures = np.round((labels_ + A * log_primes[np.array(labels_) - 1]), decimals=10)\n",
    "            new_labels = np.zeros(len(signatures), dtype = np.uint8)\n",
    "            for idx_, signature in enumerate(signatures):\n",
    "                if signature not in label_lookup:\n",
    "                    label_lookup[signature] = label_counter\n",
    "                    label_counter += 1\n",
    "                new_labels[idx_] = label_lookup[signature]\n",
    "            labels[idx] = new_labels\n",
    "            phis.append(new_labels)\n",
    "            phi[new_labels - 1, idx] = np.ones(phi[new_labels - 1, idx].shape, dtype = np.uint8)\n",
    "        phi_lists.append(phi)\n",
    "        # Uncomment when gram/kernel matrix is wanted\n",
    "        #K += phi.T.dot(phi)\n",
    "    return phi_lists\n",
    "\n",
    "\n",
    "def relabel_graphs(graphs, label_lookup = {}):\n",
    "    label_counter = 1\n",
    "    for graph in graphs:\n",
    "        current_counter = sorted(label_lookup.keys())\n",
    "        for label in sorted(graph.nodes()):\n",
    "            if label not in label_lookup:\n",
    "                label_lookup[label] = label_counter\n",
    "                label_counter += 1\n",
    "        nx.relabel_nodes(graph, {k: v for k, v in label_lookup.items() if k in graph}, copy = False)\n",
    "    return label_lookup\n",
    "    \n",
    "g1 = nx.Graph()\n",
    "g1.add_edge('A', 'B')\n",
    "g1.add_edge('A', 'C')\n",
    "\n",
    "g2 = g1.copy()\n",
    "\n",
    "g3 = nx.Graph()\n",
    "g3.add_edge('A', 'D')\n",
    "\n",
    "graphs = [g1, g2, g3]\n",
    "label_lookup = relabel_graphs(graphs)\n",
    "new_labels = fast_wl_compute(graphs, h = 10, label_lookup=label_lookup)\n",
    "print(new_labels[-1].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
