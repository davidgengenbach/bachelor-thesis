{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import deepwalk\n",
    "from deepwalk import graph\n",
    "from deepwalk import walks as serialized_walks\n",
    "from gensim.models import Word2Vec\n",
    "from deepwalk.skipgram import Skipgram\n",
    "import dataset_helper\n",
    "import graph_helper\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import tsne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_memory_data_size = 1000000000\n",
    "number_walks = 1000\n",
    "representation_size = 64\n",
    "seed = 0\n",
    "undirected = True\n",
    "vertex_freq_degree = False\n",
    "walk_length = 60\n",
    "window_size = 10\n",
    "workers = 1\n",
    "output = 'data/DUMP'\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}.npy'.format(dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=graph_helper.convert_dataset_to_co_occurence_graph_dataset, cache_file=cache_file)\n",
    "    break\n",
    "    \n",
    "models = []\n",
    "for idx, g in enumerate(X):\n",
    "    if idx == 3: break\n",
    "    print('Graph: {:>4}'.format(idx))\n",
    "    G = graph.from_networkx(g)\n",
    "\n",
    "    print(\"Number of nodes: {}\".format(len(G.nodes())))\n",
    "    if len(G.nodes()) == 0:\n",
    "        continue\n",
    "\n",
    "    num_walks = len(G.nodes()) * number_walks\n",
    "\n",
    "    print(\"Number of walks: {}\".format(num_walks))\n",
    "\n",
    "    data_size = num_walks * walk_length\n",
    "\n",
    "    print(\"Data size (walks*length): {}\".format(data_size))\n",
    "\n",
    "    print(\"Walking...\")\n",
    "    walks = graph.build_deepwalk_corpus(G, num_paths=number_walks, path_length=walk_length, alpha=0, rand=random.Random(seed))\n",
    "    print(\"Training...\")\n",
    "    model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, workers=workers)\n",
    "\n",
    "    #model.wv.save_word2vec_format(output)\n",
    "    models.append(model)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print('Next')\n",
    "    vectors = tsne.get_tsne_embedding(model)\n",
    "    tsne.plot_embedding(model, vectors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test WL phi computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    for i, (a, b) in enumerate(zip(phi_1_2_3_test, phi_1_2_3)):\n",
    "        if not np.array_equiv(a - b.todense(), np.zeros(b.shape, dtype = np.int32)):\n",
    "            print(\"\\tPhi different! {}\".format(i))\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "\n",
    "    for i, (a, b) in enumerate(zip(K_1_2_3_test, K_1_2_3)):\n",
    "        if not np.array_equal(a, b):\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "            print(\"\\tK different! {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge node labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "for (n, treshold), lookup in results.items():\n",
    "    cliques = coreference.get_cliques_from_lookup(lookup)\n",
    "    similarity_counter = {'similar': len(lookup.keys()), 'unsimilar': num_labels - len(lookup.keys())}\n",
    "    clique_lenghts = [len(x) for x in list(cliques.values())]\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n",
    "    fig.suptitle('Treshold: {}, N={}'.format(treshold, n), fontsize = 16)\n",
    "\n",
    "    pd.DataFrame(clique_lenghts).plot(ax = axes[0], kind = 'hist', logy = True, legend = False, title = \"Histogram of clique lengths\".format(treshold))\n",
    "    pd.DataFrame(list(similarity_counter.items()), columns = ['name', 'count']).set_index('name').plot(ax = axes[1], kind = 'bar', legend = False, title = '# of labels that have been merged vs. not merged')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    fig.savefig('tmp/{:.5f}.{}.png'.format(treshold, n), dpi = 120)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set('notebook', 'white')\n",
    "def plot_by(df, by, bins = 15, title = '', figsize = (12, 5), fontsize = 16):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for n, vals in df.groupby(by):\n",
    "        labels.append(n)\n",
    "        data.append(vals.clique_length)\n",
    "    ax.hist(data, bins = bins, alpha=0.7, label=labels, log = True)\n",
    "    fig.suptitle(title, fontsize = fontsize)\n",
    "    ax.legend(loc='upper right', fontsize = fontsize)\n",
    "    ax.set_xlabel('clique sizes')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    return fig, ax\n",
    "fig, ax = plot_by(df, 'n', title = 'Clique size histogram by n (all thresholds together)')\n",
    "fig.savefig('tmp/clique_size_by_n_all_thresholds.png', dpi = 120)\n",
    "fig, ax = plot_by(df, 'threshold', title = 'Clique size histogram by threshold (all n together)')\n",
    "fig.savefig('tmp/clique_size_by_threshold_all_n.png', dpi = 120)\n",
    "fig, ax = plot_by(df[df.threshold == 0.6], 'n', title = 'Clique size histogram by n (threshold=0.6)')\n",
    "fig.savefig('tmp/clique_size_by_n_threshold_0.6.png', dpi = 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn\n",
    "graph_cache_file = 'dataset_graph_gml_ng20-single.npy'\n",
    "X, Y = dataset_helper.get_dataset_cached('data/CACHE/{}'.format(graph_cache_file))\n",
    "X, Y = np.array(X, dtype=object), np.array(Y, dtype=object)\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(n_splits = 40, random_state=42)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    X_test, Y_test = X[test_index], Y[test_index]\n",
    "    break\n",
    "with open('data/CACHE/dataset_graph_gml_small-single.npy', 'wb') as f:\n",
    "    pickle.dump((X_test.tolist(), Y_test.tolist()), f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set('notebook', 'white')\n",
    "limit_dataset = ['ng20', 'ling-spam', 'reuters-21578', 'webkb']\n",
    "#limit_dataset = ['ling-spam']\n",
    "all_stats = {}\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in limit_dataset: continue\n",
    "    print(dataset_name)\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    graphs = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    number_of_nodes = []\n",
    "    \n",
    "    coo_graph = [x for x in graphs if 'cooccurrence' in x][0]\n",
    "    gml_graph = [x for x in graphs if 'gml' in x][0]\n",
    "    \n",
    "    def get_num_nodes(graph_file):\n",
    "        X_graph, _ = dataset_helper.get_dataset_cached(graph_file)\n",
    "        return [nx.number_of_nodes(x) for x in X_graph]\n",
    "    \n",
    "    stats = {\n",
    "        'cooccurrence': get_num_nodes(coo_graph),\n",
    "        'concept-graphs': get_num_nodes(gml_graph)\n",
    "    }\n",
    "    \n",
    "    min_len = min([len(v) for k, v in stats.items()])\n",
    "    graph_stats = {k: v[:min(min_len, len(v))] for k, v in stats.items()}\n",
    "    text_stats = {'doc_lengths': [len(x) for x in X]}\n",
    "    \n",
    "    all_stats[dataset_name] = {\n",
    "        'graphs': graph_stats,\n",
    "        'text': text_stats['doc_lengths'],\n",
    "        'num_docs': len(X),\n",
    "        'num_classes': len(set(Y))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figsize = (12, 4)\n",
    "bins = 20\n",
    "\n",
    "for dataset_name, stats in all_stats.items():\n",
    "    graph_stats = stats['graphs']\n",
    "    text_stats = stats['text']\n",
    "    \n",
    "    df = pd.DataFrame(graph_stats)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols=2, figsize = figsize)\n",
    "    df.plot(kind = 'hist', bins=bins, alpha = 0.7, logy = True, ax=ax[0])\n",
    "    ax[0].set_xlabel('# nodes per graph')\n",
    "    \n",
    "    df = pd.DataFrame(text_stats)\n",
    "    df.plot(kind = 'hist', bins=bins, logy = True, legend = False, ax=ax[1])\n",
    "    ax[1].set_xlabel('# characters per document')\n",
    "    fig.savefig('tmp/other/stats-{}.png'.format(dataset_name), dpi = 150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "out = []\n",
    "for dataset, stats in all_stats.items():\n",
    "    out.append((dataset, np.mean(stats['text']), np.mean(stats['graphs']['cooccurrence']), np.mean(stats['graphs']['concept-graphs']), stats['num_docs'], stats['num_classes']))\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize = (8, 6))\n",
    "\n",
    "df = pd.DataFrame(out, columns = ['dataset', 'avg_doc_length', 'avg_coo_node_num', 'avg_cp_node_num', 'num_docs', 'num_classes'])\n",
    "df = df.set_index('dataset')\n",
    "#, ('# classes', 'num_classes')\n",
    "for idx, (name, x) in enumerate([('Average document length', 'avg_doc_length'), ('# documents', 'num_docs'), ('Average of number of concept-graph nodes', 'avg_cp_node_num'), ('Average of number of co-occurence nodes', 'avg_coo_node_num')]):\n",
    "    ax = axes[idx]\n",
    "    df[x].plot(kind = 'barh', logx = True, title = name, ax = ax)\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/other/stats-datasets.png', dpi = 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "\n",
    "check_graphs = False\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    \n",
    "    with open('data/embeddings/graph-embeddings/{}.label-lookup.npy'.format(dataset_name), 'rb') as f:\n",
    "        lookup = pickle.load(f)\n",
    "    \n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup, len(lookup.keys()))\n",
    "    plt.show()\n",
    "        \n",
    "    counter = collections.Counter()\n",
    "    all_labels = set()\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        X, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        X = trans.transform(X)\n",
    "        \n",
    "        for adj, labels in X:\n",
    "            all_labels |= set(labels)\n",
    "            for label in labels:\n",
    "                counter['found' if label in lookup and str(lookup[label]).strip() != str(label).strip() else 'not_found'] += 1\n",
    "    print(counter)\n",
    "    print(len(all_labels))\n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup, len(all_labels))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "import networkx as nx\n",
    "\n",
    "def merge_graphs(graphs):\n",
    "    return nx.compose_all(graphs)\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    \n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        if 'gml' not in graph_cache_file: continue\n",
    "        \n",
    "        X, Y = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        #X = trans.transform(X)\n",
    "        \n",
    "        num_labels = 0\n",
    "        all_labels = set()\n",
    "        all_labels_stripped = set()\n",
    "        \n",
    "        #for adj, labels in X:\n",
    "        for g in X:\n",
    "            labels = g.nodes()\n",
    "            all_labels |= set(labels)\n",
    "            all_labels_stripped |= set([str(label).strip() for label in labels])\n",
    "            num_labels += len(labels)\n",
    "    \n",
    "    num_uniq_labels = len(all_labels)\n",
    "    num_uniq_labels_stripped = len(all_labels_stripped)\n",
    "    print('#labels:\\t\\t{}'.format(num_labels))\n",
    "    print('#uniq. labels:\\t\\t{}'.format(num_uniq_labels))\n",
    "    print('#uniq. labels stripped:\\t{}'.format(num_uniq_labels_stripped))\n",
    "    print('#non-uniq. labels: \\t{}'.format(num_labels - num_uniq_labels))\n",
    "    \n",
    "    d = dataset_helper.get_dataset_dict(X, Y)\n",
    "    merged = {label: merge_graphs(graphs) for label, graphs in d.items()}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "from transformers.relabel_graphs_transformer import RelabelGraphsTransformer\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import coreference \n",
    "sns.set_style('white')\n",
    "\n",
    "def get_treshold_and_topn_from_lookupfilename(filename):\n",
    "    topn = filename.split('topn-')[1].split('.label')[0]\n",
    "    threshold = filename.split('threshold-')[1].split('.topn')[0]\n",
    "    return threshold, topn\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    #if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    if dataset_name not in ['ling-spam', 'ng20']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        print('Loading dataset: Start ({})'.format(graph_cache_file))\n",
    "        X_old, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        print('TupleTransform: Start')\n",
    "        X_old = trans.transform(X_old)\n",
    "        X_labels = [labels for _, labels in X_old]\n",
    "        \n",
    "        lookups = glob('data/embeddings/graph-embeddings/{}.*.*.label-lookup.npy'.format(dataset_name))\n",
    "        for lookup_file in lookups:\n",
    "            # Load lookup\n",
    "            threshold, topn = get_treshold_and_topn_from_lookupfilename(lookup_file)\n",
    "            with open(lookup_file, 'rb') as f:\n",
    "                lookup = pickle.load(f)\n",
    "            \n",
    "            # Relabel\n",
    "            relabel_trans = RelabelGraphsTransformer(lookup)\n",
    "            X = relabel_trans.transform(X_old)\n",
    "            duplicate_labels_count = []\n",
    "            different_counts = []\n",
    "            for (_, labels), old_labels in zip(X, X_labels):\n",
    "                labels_set = set(labels)\n",
    "                different_counts.append(collections.Counter([str(l1).lower().strip() != str(l2).lower().strip() for l1, l2 in zip(labels, old_labels)])[True])\n",
    "                duplicate_labels_count.append(len(labels) - len(labels_set))\n",
    "            label_counters = [len(labels) for _, labels in X]\n",
    "    \n",
    "            df = pd.DataFrame(list(zip(different_counts, duplicate_labels_count, label_counters)), columns = ['different_count', 'duplicate_labels_count', 'label_count'])\n",
    "            df['relabeled_ratio'] = df.different_count / df.label_count\n",
    "            df['duplicate_ratio'] = df.duplicate_labels_count / df.label_count\n",
    "            fig, ax = plt.subplots(figsize = (16, 4))\n",
    "            df[['relabeled_ratio', 'duplicate_ratio']].plot(kind = 'hist', bins = 100, log = True, title = 'Histogram (threshold={}, topn={})'.format(threshold, topn), ax = ax, alpha = 0.7)\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import coreference\n",
    "for lookup_file in glob('data/embeddings/graph-embeddings/*.threshold-*.*.label-lookup.npy'):\n",
    "    threshold, topn = get_treshold_and_topn_from_lookupfilename(lookup_file)\n",
    "    with open(lookup_file, 'rb') as f:\n",
    "        lookup = pickle.load(f)\n",
    "    for key in lookup.values():\n",
    "        if not isinstance(key, (str, int)):\n",
    "            print(\"?\")\n",
    "            break\n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup=lookup, title = 'threshold={}, topn={}'.format(threshold, topn))\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import graph_helper\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        #if 'gml' not in graph_cache_file: continue\n",
    "        if 'gml' in graph_cache_file: continue\n",
    "        print('Loading dataset: {}'.format(graph_cache_file))\n",
    "        X_old, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        label_counter = collections.Counter()\n",
    "        for graph in X_old:\n",
    "            labels = [str(x).strip() for x in graph.nodes()]\n",
    "            label_counter.update(labels)\n",
    "        counts_ = list(label_counter.values())\n",
    "        counts += list(zip([dataset_name] * len(counts_), counts_))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(counts, columns = ['dataset', 'counts'])\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize = (7, 4))\n",
    "axes = df.hist(log = True, bins = 60, by = 'dataset', ax = axes)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlim((0, 10000))\n",
    "    pass\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/label-distribution-per-dataset.png', dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_elements = 10\n",
    "data = list(zip([\"a\"] * num_elements, range(num_elements))) + list(zip([\"b\"] * num_elements, range(num_elements)))\n",
    "df = pd.DataFrame(data, columns = [\"name\", \"counts\"])\n",
    "df.plot(kind = \"hist\", by = df.name)\n",
    "df.hist(by = df.name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    #if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    if dataset_name not in ['ling-spam']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        if 'gml' in graph_cache_file: continue\n",
    "        X, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60981, 60981)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(\"^[a-zA-Z0-9]+$\")\n",
    "all_labels = graph_helper.get_all_node_labels(X)\n",
    "new_labels = set()\n",
    "for label in all_labels:\n",
    "    new_labels.add(label.strip())\n",
    "    new_labels.add(label)\n",
    "len(all_labels), len(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in LInked List format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "indices = [0, 1, 5, 8]\n",
    "mat = sparse.lil_matrix((10, 10))\n",
    "mat[indices]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
