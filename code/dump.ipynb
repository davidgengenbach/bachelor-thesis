{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DPI = 100\n",
    "EXPORT_FIG_SIZE = (8, 4 )\n",
    "EXPORT_FIG_WIDTH, EXPORT_FIG_HEIGHT = EXPORT_FIG_SIZE\n",
    "\n",
    "sns.set('notebook', 'whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import collections\n",
    "import networkx as nx\n",
    "import sys\n",
    "\n",
    "WALK_LENGTH = 2\n",
    "dataset_name = 'ng20'\n",
    "\n",
    "for graph_cache_file in dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset_name):\n",
    "    if 'coo' not in graph_cache_file or 'all' in graph_cache_file: continue\n",
    "    print(graph_cache_file)\n",
    "    X, Y = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "    for idx, graph in enumerate(X):\n",
    "        if idx % 100 == 0: sys.stdout.write('\\r{:3.0f}%'.format(idx / len(X) * 100))\n",
    "        if graph.number_of_edges() == 0 or graph.number_of_nodes() == 0: continue\n",
    "        shortest_paths = nx.all_pairs_shortest_path(graph, cutoff=WALK_LENGTH)\n",
    "        for source, target_dict in shortest_paths.items():\n",
    "            for target, path in target_dict.items():\n",
    "                if len(path) < 2: continue\n",
    "                graph.add_edge(source, target, attr_dict = {'weight': 1 / len(path)})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for dataset in ['ng20']:\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name=dataset)\n",
    "    gml_graph_cache = [x for x in graph_cache_files if 'gml' in x][0]\n",
    "    coo_graph_caches = [x for x in graph_cache_files if 'cooccurrence_1' in x and 'all' not in x]\n",
    "    coo_graph_cache = np.random.choice(coo_graph_caches)\n",
    "    print(gml_graph_cache, coo_graph_cache)\n",
    "    \n",
    "    X_gml, Y = dataset_helper.get_dataset_cached(gml_graph_cache)\n",
    "    X_coo, _ = dataset_helper.get_dataset_cached(coo_graph_cache)\n",
    "    print(len(X_gml), len(X_coo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figsize = (16, 4)\n",
    "NUM_BINS = 40\n",
    "alpha = 0.7\n",
    "\n",
    "graph_metrics = [\n",
    "    ('density', lambda graph: nx.density(graph) if graph.number_of_nodes() > 0 else 0.0),\n",
    "    ('number of nodes', lambda graph: graph.number_of_nodes()),\n",
    "    ('number of edges', lambda graph: graph.number_of_edges()),\n",
    "    ('#nodes/#edges', lambda graph:  graph.number_of_nodes() / graph.number_of_edges() if graph.number_of_edges() > 0 else -99),\n",
    "    ('#edges/#nodes', lambda graph:  graph.number_of_edges() / graph.number_of_nodes() if graph.number_of_nodes() > 0 else -99)\n",
    "]\n",
    "\n",
    "for metric_name, metric in graph_metrics:\n",
    "    graph_metrics = []\n",
    "    for graph_type, graphs in [('concept-map', X_gml), ('co-occurrence (ws=1)', X_coo)]:\n",
    "        graph_metrics += [(graph_type, metric(graph)) for graph in graphs]\n",
    "\n",
    "    df = pd.DataFrame(graph_metrics, columns = ['graph_type', 'graph_metric'])\n",
    "    df = df[df.graph_metric > -10]\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    metrics_ = df.graph_metric.tolist()\n",
    "    binwidth = (max(metrics_) - min(metrics_)) / NUM_BINS\n",
    "    bins = np.arange(min(metrics_), max(metrics_) + binwidth, binwidth)\n",
    "    df.groupby('graph_type').graph_metric.plot(kind = 'hist', bins = bins, alpha = alpha, ax = ax, title = 'Histogram of {}'.format(metric_name), logy = True, legend = True)\n",
    "    ax.set_xlabel(metric_name)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    #ax = sns.violinplot(x = 'graph_type', y = 'graph_densities', data=df, cut = 0, inner = 'quartile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figsize = (10, 10)\n",
    "\n",
    "NUM_GRAPHS = 2\n",
    "used_figsize = (18, NUM_GRAPHS * 5)\n",
    "used_figsize = EXPORT_FIG_SIZE\n",
    "fig, axes = plt.subplots(ncols=2, nrows=NUM_GRAPHS, figsize = used_figsize)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "for idx, row_ax in enumerate(axes):\n",
    "    graph_gml, graph_coo = np.random.choice(X_gml), np.random.choice(X_coo)\n",
    "    ax = row_ax[0]\n",
    "    nx.draw_networkx(graph_gml, ax = ax, node_size = 0, style = 'dotted')\n",
    "    \n",
    "    if idx == 0:\n",
    "        ax.set_title('Concept map')\n",
    "        \n",
    "    ax = row_ax[1]\n",
    "    nx.draw_networkx(graph_coo, ax = ax, node_size = 0, style = 'dotted')\n",
    "\n",
    "    if idx == 0:\n",
    "        ax.set_title('Co-Occurrence (window size: {})'.format(coo_graph_cache.split('cooccurrence_')[-1].split('_')[0]))\n",
    "        \n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/graph-examples.png', dpi = EXPORT_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed tfidf and co-occurrence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import pipeline\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import svm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import dataset_helper\n",
    "\n",
    "X_text, Y_text = dataset_helper.get_dataset('ling-spam')\n",
    "graph_cache_files = dataset_helper.get_all_cached_graph_phi_datasets(dataset_name='ling-spam')\n",
    "X_graph_phi, Y_graph_phi = dataset_helper.get_dataset_cached(graph_cache_files[0], check_validity=False)\n",
    "\n",
    "assert len(X_text) == X_graph_phi[0].shape[0]\n",
    "\n",
    "# See http://scikit-learn.org/stable/auto_examples/plot_feature_stacker.html#sphx-glr-auto-examples-plot-feature-stacker-py\n",
    "class TupleSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tuple_index):\n",
    "        self.tuple_index = tuple_index\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [x[self.tuple_index] for x in X]\n",
    "    \n",
    "combined_features = sklearn.pipeline.FeatureUnion([\n",
    "    ('tfidf', sklearn.pipeline.Pipeline([\n",
    "        ('selector', TupleSelector(tuple_index=0)),\n",
    "        ('tfidf', sklearn.feature_extraction.text.TfidfVectorizer(stop_words = 'english')),\n",
    "    ])),\n",
    "    ('phi', sklearn.pipeline.Pipeline([\n",
    "        ('selector', TupleSelector(tuple_index=1))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "param_grid = dict()\n",
    "X_combined = list(zip(X_text, X_graph_phi[0]))\n",
    "svm = sklearn.svm.SVC(class_weight = 'balanced')\n",
    "pipeline = sklearn.pipeline.Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "gscv = sklearn.model_selection.GridSearchCV(pipeline, param_grid=param_grid, verbose=11)\n",
    "gscv.fit(X_combined, Y_text)\n",
    "#trans = sklearn.feature_extraction.text.TfidfVectorizer(stop_words = 'english')\n",
    "#trans.fit(X_text, Y_text)\n",
    "#tfidf = trans.transform(X_text)\n",
    "#print(len(X_text))\n",
    "#combined_features.fit(X_combined, Y_text)\n",
    "#combined_features.transform(X_combined)\n",
    "#print(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import sympy\n",
    "import pandas as pd\n",
    "\n",
    "word_counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    text = []\n",
    "    for t in X:\n",
    "        text.append(t)\n",
    "    text = ' '.join(text)\n",
    "    text = text.lower().replace('\\n', ' ')\n",
    "    words = [x.strip() for x in text.split() if x.strip() != '']\n",
    "    unique_words = set(words)\n",
    "    word_counts.append((dataset_name, len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(word_counts, columns = ['dataset', 'unique_words']).set_index('dataset').sort_values(by = 'unique_words')\n",
    "df.unique_words.plot(kind = 'barh', logx = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/primes.npy', 'rb') as f:\n",
    "    primes = pickle.load(f)\n",
    "\n",
    "max_prime_range = sorted(list(primes.keys()))[-1]\n",
    "max_words = df.unique_words.max()\n",
    "num_primes = len(primes[max_prime_range])\n",
    "print(max_words, num_primes, num_primes - max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed gml classification (single document, merged document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "from kernels import fast_wl\n",
    "import networkx as nx\n",
    "import graph_helper\n",
    "import numpy as np\n",
    "\n",
    "#'reuters-21578',\n",
    "for dataset in ['ng20']:\n",
    "    X, Y = dataset_helper.get_gml_graph_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_single, Y_single = dataset_helper.get_dataset_cached([x for x in dataset_helper.get_all_cached_graph_datasets(dataset_name='ng20') if 'gml' in x and 'single' in x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_classes(x, y, classes_to_keep = ()):\n",
    "    indices = [y_ in classes_to_keep for y_ in y]\n",
    "    return np.array(x, dtype = object)[indices].tolist(), np.array(y, dtype = object)[indices].tolist()\n",
    "\n",
    "X_single_filtered, Y_single_filtered = filter_classes(X_single, Y_single, set(Y))\n",
    "\n",
    "# Compute phi\n",
    "graph_helper.convert_graphs_to_adjs_tuples(X_single_filtered)\n",
    "graph_helper.convert_graphs_to_adjs_tuples(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merged = np.concatenate([X, X_single_filtered])\n",
    "Y_merged = np.concatenate([Y, Y_single_filtered])\n",
    "phi_lists, new_label_lookups, new_label_counters = fast_wl.fast_wl_compute(X_merged.tolist(), h = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "svm = sklearn.svm.LinearSVC()\n",
    "results = []\n",
    "for idx, phi in enumerate(phi_lists):\n",
    "    svm.fit(phi.T[:len(X),:], Y)\n",
    "    Y_pred = svm.predict(phi.T[len(X):,:])\n",
    "    results.append((idx, sklearn.metrics.f1_score(y_true = Y_single_filtered, y_pred=Y_pred, average = 'micro')))\n",
    "    #print(sklearn.metrics.classification_report(y_true = Y_single_filtered, y_pred = Y_pred)) #, average='macro')\n",
    "pd.DataFrame(results, columns = ['phi', 'f1_macro']).set_index('phi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPGK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernels import spgk\n",
    "import dataset_helper\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "graph_dataset_caches = dataset_helper.get_all_cached_graph_datasets()\n",
    "for graph_cache in graph_dataset_caches:\n",
    "    if 'ling-spam' not in graph_cache: continue\n",
    "    print(graph_cache)\n",
    "    X, Y = dataset_helper.get_dataset_cached(graph_cache)\n",
    "    #X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, train_size = 0.7, stratify = Y)\n",
    "    X_train, y_train = X, Y\n",
    "    K = spgk.build_kernel_matrix(X_train, depth = 1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "NUM_ELEMENTS=int(K.shape[0] * 0.8)\n",
    "svm = sklearn.svm.SVC(kernel = 'precomputed', class_weight='balanced')\n",
    "#sklearn.model_selection.cross_val_score(svm, K, y_train)\n",
    "svm.fit(K[:NUM_ELEMENTS,:NUM_ELEMENTS], y_train[:NUM_ELEMENTS])\n",
    "#Y_pred = svm.predict(K[NUM_ELEMENTS:,:NUM_ELEMENTS])\n",
    "#print(collections.Counter(y_train[:NUM_ELEMENTS]))\n",
    "#print(sklearn.metrics.classification_report(y_true = y_train[NUM_ELEMENTS:], y_pred=Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepwalk\n",
    "from deepwalk import graph\n",
    "from deepwalk import walks as serialized_walks\n",
    "from gensim.models import Word2Vec\n",
    "from deepwalk.skipgram import Skipgram\n",
    "import dataset_helper\n",
    "import graph_helper\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import tsne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_memory_data_size = 1000000000\n",
    "number_walks = 1000\n",
    "representation_size = 64\n",
    "seed = 0\n",
    "undirected = True\n",
    "vertex_freq_degree = False\n",
    "walk_length = 60\n",
    "window_size = 10\n",
    "workers = 1\n",
    "output = 'data/DUMP'\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}.npy'.format(dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=graph_helper.convert_dataset_to_co_occurence_graph_dataset, cache_file=cache_file)\n",
    "    break\n",
    "    \n",
    "models = []\n",
    "for idx, g in enumerate(X):\n",
    "    if idx == 3: break\n",
    "    print('Graph: {:>4}'.format(idx))\n",
    "    G = graph.from_networkx(g)\n",
    "\n",
    "    print(\"Number of nodes: {}\".format(len(G.nodes())))\n",
    "    if len(G.nodes()) == 0:\n",
    "        continue\n",
    "\n",
    "    num_walks = len(G.nodes()) * number_walks\n",
    "\n",
    "    print(\"Number of walks: {}\".format(num_walks))\n",
    "\n",
    "    data_size = num_walks * walk_length\n",
    "\n",
    "    print(\"Data size (walks*length): {}\".format(data_size))\n",
    "\n",
    "    print(\"Walking...\")\n",
    "    walks = graph.build_deepwalk_corpus(G, num_paths=number_walks, path_length=walk_length, alpha=0, rand=random.Random(seed))\n",
    "    print(\"Training...\")\n",
    "    model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, workers=workers)\n",
    "\n",
    "    #model.wv.save_word2vec_format(output)\n",
    "    models.append(model)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print('Next')\n",
    "    vectors = tsne.get_tsne_embedding(model)\n",
    "    tsne.plot_embedding(model, vectors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test WL phi computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    for i, (a, b) in enumerate(zip(phi_1_2_3_test, phi_1_2_3)):\n",
    "        if not np.array_equiv(a - b.todense(), np.zeros(b.shape, dtype = np.int32)):\n",
    "            print(\"\\tPhi different! {}\".format(i))\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "\n",
    "    for i, (a, b) in enumerate(zip(K_1_2_3_test, K_1_2_3)):\n",
    "        if not np.array_equal(a, b):\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "            print(\"\\tK different! {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge node labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "for (n, treshold), lookup in results.items():\n",
    "    cliques = coreference.get_cliques_from_lookup(lookup)\n",
    "    similarity_counter = {'similar': len(lookup.keys()), 'unsimilar': num_labels - len(lookup.keys())}\n",
    "    clique_lenghts = [len(x) for x in list(cliques.values())]\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n",
    "    fig.suptitle('Treshold: {}, N={}'.format(treshold, n), fontsize = 16)\n",
    "\n",
    "    pd.DataFrame(clique_lenghts).plot(ax = axes[0], kind = 'hist', logy = True, legend = False, title = \"Histogram of clique lengths\".format(treshold))\n",
    "    pd.DataFrame(list(similarity_counter.items()), columns = ['name', 'count']).set_index('name').plot(ax = axes[1], kind = 'bar', legend = False, title = '# of labels that have been merged vs. not merged')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    fig.savefig('tmp/{:.5f}.{}.png'.format(treshold, n), dpi = 120)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set('notebook', 'white')\n",
    "def plot_by(df, by, bins = 15, title = '', figsize = (12, 5), fontsize = 16):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for n, vals in df.groupby(by):\n",
    "        labels.append(n)\n",
    "        data.append(vals.clique_length)\n",
    "    ax.hist(data, bins = bins, alpha=0.7, label=labels, log = True)\n",
    "    fig.suptitle(title, fontsize = fontsize)\n",
    "    ax.legend(loc='upper right', fontsize = fontsize)\n",
    "    ax.set_xlabel('clique sizes')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    return fig, ax\n",
    "fig, ax = plot_by(df, 'n', title = 'Clique size histogram by n (all thresholds together)')\n",
    "fig.savefig('tmp/clique_size_by_n_all_thresholds.png', dpi = 120)\n",
    "fig, ax = plot_by(df, 'threshold', title = 'Clique size histogram by threshold (all n together)')\n",
    "fig.savefig('tmp/clique_size_by_threshold_all_n.png', dpi = 120)\n",
    "fig, ax = plot_by(df[df.threshold == 0.6], 'n', title = 'Clique size histogram by n (threshold=0.6)')\n",
    "fig.savefig('tmp/clique_size_by_n_threshold_0.6.png', dpi = 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn\n",
    "graph_cache_file = 'dataset_graph_gml_ng20-single.npy'\n",
    "X, Y = dataset_helper.get_dataset_cached('data/CACHE/{}'.format(graph_cache_file))\n",
    "X, Y = np.array(X, dtype=object), np.array(Y, dtype=object)\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(n_splits = 40, random_state=42)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    X_test, Y_test = X[test_index], Y[test_index]\n",
    "    break\n",
    "with open('data/CACHE/dataset_graph_gml_small-single.npy', 'wb') as f:\n",
    "    pickle.dump((X_test.tolist(), Y_test.tolist()), f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set('notebook', 'white')\n",
    "limit_dataset = ['ng20', 'ling-spam', 'reuters-21578', 'webkb']\n",
    "#limit_dataset = ['ling-spam']\n",
    "all_stats = {}\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in limit_dataset: continue\n",
    "    print(dataset_name)\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    graphs = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    number_of_nodes = []\n",
    "    \n",
    "    coo_graph = [x for x in graphs if 'cooccurrence' in x][0]\n",
    "    gml_graph = [x for x in graphs if 'gml' in x][0]\n",
    "    \n",
    "    def get_num_nodes(graph_file):\n",
    "        X_graph, _ = dataset_helper.get_dataset_cached(graph_file)\n",
    "        return [nx.number_of_nodes(x) for x in X_graph]\n",
    "    \n",
    "    stats = {\n",
    "        'cooccurrence': get_num_nodes(coo_graph),\n",
    "        'concept-graphs': get_num_nodes(gml_graph)\n",
    "    }\n",
    "    \n",
    "    min_len = min([len(v) for k, v in stats.items()])\n",
    "    graph_stats = {k: v[:min(min_len, len(v))] for k, v in stats.items()}\n",
    "    text_stats = {'doc_lengths': [len(x) for x in X]}\n",
    "    \n",
    "    all_stats[dataset_name] = {\n",
    "        'graphs': graph_stats,\n",
    "        'text': text_stats['doc_lengths'],\n",
    "        'num_docs': len(X),\n",
    "        'num_classes': len(set(Y))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (12, 4)\n",
    "bins = 20\n",
    "\n",
    "for dataset_name, stats in all_stats.items():\n",
    "    graph_stats = stats['graphs']\n",
    "    text_stats = stats['text']\n",
    "    \n",
    "    df = pd.DataFrame(graph_stats)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols=2, figsize = figsize)\n",
    "    df.plot(kind = 'hist', bins=bins, alpha = 0.7, logy = True, ax=ax[0])\n",
    "    ax[0].set_xlabel('# nodes per graph')\n",
    "    \n",
    "    df = pd.DataFrame(text_stats)\n",
    "    df.plot(kind = 'hist', bins=bins, logy = True, legend = False, ax=ax[1])\n",
    "    ax[1].set_xlabel('# characters per document')\n",
    "    fig.savefig('tmp/other/stats-{}.png'.format(dataset_name), dpi = 150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "out = []\n",
    "for dataset, stats in all_stats.items():\n",
    "    out.append((dataset, np.mean(stats['text']), np.mean(stats['graphs']['cooccurrence']), np.mean(stats['graphs']['concept-graphs']), stats['num_docs'], stats['num_classes']))\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize = (8, 6))\n",
    "\n",
    "df = pd.DataFrame(out, columns = ['dataset', 'avg_doc_length', 'avg_coo_node_num', 'avg_cp_node_num', 'num_docs', 'num_classes'])\n",
    "df = df.set_index('dataset')\n",
    "#, ('# classes', 'num_classes')\n",
    "for idx, (name, x) in enumerate([('Average document length', 'avg_doc_length'), ('# documents', 'num_docs'), ('Average of number of concept-graph nodes', 'avg_cp_node_num'), ('Average of number of co-occurence nodes', 'avg_coo_node_num')]):\n",
    "    ax = axes[idx]\n",
    "    df[x].plot(kind = 'barh', logx = True, title = name, ax = ax)\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/other/stats-datasets.png', dpi = 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "\n",
    "check_graphs = False\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    \n",
    "    with open('data/embeddings/graph-embeddings/{}.label-lookup.npy'.format(dataset_name), 'rb') as f:\n",
    "        lookup = pickle.load(f)\n",
    "    \n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup, len(lookup.keys()))\n",
    "    plt.show()\n",
    "        \n",
    "    counter = collections.Counter()\n",
    "    all_labels = set()\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        X, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        X = trans.transform(X)\n",
    "        \n",
    "        for adj, labels in X:\n",
    "            all_labels |= set(labels)\n",
    "            for label in labels:\n",
    "                counter['found' if label in lookup and str(lookup[label]).strip() != str(label).strip() else 'not_found'] += 1\n",
    "    print(counter)\n",
    "    print(len(all_labels))\n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup, len(all_labels))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "import networkx as nx\n",
    "\n",
    "def merge_graphs(graphs):\n",
    "    return nx.compose_all(graphs)\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    \n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        if 'gml' not in graph_cache_file: continue\n",
    "        \n",
    "        X, Y = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        #X = trans.transform(X)\n",
    "        \n",
    "        num_labels = 0\n",
    "        all_labels = set()\n",
    "        all_labels_stripped = set()\n",
    "        \n",
    "        #for adj, labels in X:\n",
    "        for g in X:\n",
    "            labels = g.nodes()\n",
    "            all_labels |= set(labels)\n",
    "            all_labels_stripped |= set([str(label).strip() for label in labels])\n",
    "            num_labels += len(labels)\n",
    "    \n",
    "    num_uniq_labels = len(all_labels)\n",
    "    num_uniq_labels_stripped = len(all_labels_stripped)\n",
    "    print('#labels:\\t\\t{}'.format(num_labels))\n",
    "    print('#uniq. labels:\\t\\t{}'.format(num_uniq_labels))\n",
    "    print('#uniq. labels stripped:\\t{}'.format(num_uniq_labels_stripped))\n",
    "    print('#non-uniq. labels: \\t{}'.format(num_labels - num_uniq_labels))\n",
    "    \n",
    "    d = dataset_helper.get_dataset_dict(X, Y)\n",
    "    merged = {label: merge_graphs(graphs) for label, graphs in d.items()}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "from transformers.relabel_graphs_transformer import RelabelGraphsTransformer\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import coreference \n",
    "sns.set_style('white')\n",
    "\n",
    "def get_treshold_and_topn_from_lookupfilename(filename):\n",
    "    topn = filename.split('topn-')[1].split('.label')[0]\n",
    "    threshold = filename.split('threshold-')[1].split('.topn')[0]\n",
    "    return threshold, topn\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    #if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    if dataset_name not in ['ling-spam', 'ng20']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        print('Loading dataset: Start ({})'.format(graph_cache_file))\n",
    "        X_old, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        print('TupleTransform: Start')\n",
    "        X_old = trans.transform(X_old)\n",
    "        X_labels = [labels for _, labels in X_old]\n",
    "        \n",
    "        lookups = glob('data/embeddings/graph-embeddings/{}.*.*.label-lookup.npy'.format(dataset_name))\n",
    "        for lookup_file in lookups:\n",
    "            # Load lookup\n",
    "            threshold, topn = get_treshold_and_topn_from_lookupfilename(lookup_file)\n",
    "            with open(lookup_file, 'rb') as f:\n",
    "                lookup = pickle.load(f)\n",
    "            \n",
    "            # Relabel\n",
    "            relabel_trans = RelabelGraphsTransformer(lookup)\n",
    "            X = relabel_trans.transform(X_old)\n",
    "            duplicate_labels_count = []\n",
    "            different_counts = []\n",
    "            for (_, labels), old_labels in zip(X, X_labels):\n",
    "                labels_set = set(labels)\n",
    "                different_counts.append(collections.Counter([str(l1).lower().strip() != str(l2).lower().strip() for l1, l2 in zip(labels, old_labels)])[True])\n",
    "                duplicate_labels_count.append(len(labels) - len(labels_set))\n",
    "            label_counters = [len(labels) for _, labels in X]\n",
    "    \n",
    "            df = pd.DataFrame(list(zip(different_counts, duplicate_labels_count, label_counters)), columns = ['different_count', 'duplicate_labels_count', 'label_count'])\n",
    "            df['relabeled_ratio'] = df.different_count / df.label_count\n",
    "            df['duplicate_ratio'] = df.duplicate_labels_count / df.label_count\n",
    "            fig, ax = plt.subplots(figsize = (16, 4))\n",
    "            df[['relabeled_ratio', 'duplicate_ratio']].plot(kind = 'hist', bins = 100, log = True, title = 'Histogram (threshold={}, topn={})'.format(threshold, topn), ax = ax, alpha = 0.7)\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coreference\n",
    "for lookup_file in glob('data/embeddings/graph-embeddings/*.threshold-*.*.label-lookup.npy'):\n",
    "    threshold, topn = get_treshold_and_topn_from_lookupfilename(lookup_file)\n",
    "    with open(lookup_file, 'rb') as f:\n",
    "        lookup = pickle.load(f)\n",
    "    for key in lookup.values():\n",
    "        if not isinstance(key, (str, int)):\n",
    "            print(\"?\")\n",
    "            break\n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup=lookup, title = 'threshold={}, topn={}'.format(threshold, topn))\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import graph_helper\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        #if 'gml' not in graph_cache_file: continue\n",
    "        if 'gml' in graph_cache_file: continue\n",
    "        print('Loading dataset: {}'.format(graph_cache_file))\n",
    "        X_old, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        label_counter = collections.Counter()\n",
    "        for graph in X_old:\n",
    "            labels = [str(x).strip() for x in graph.nodes()]\n",
    "            label_counter.update(labels)\n",
    "        counts_ = list(label_counter.values())\n",
    "        counts += list(zip([dataset_name] * len(counts_), counts_))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(counts, columns = ['dataset', 'counts'])\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize = (7, 4))\n",
    "axes = df.hist(log = True, bins = 60, by = 'dataset', ax = axes)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlim((0, 10000))\n",
    "    pass\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/label-distribution-per-dataset.png', dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_elements = 10\n",
    "data = list(zip([\"a\"] * num_elements, range(num_elements))) + list(zip([\"b\"] * num_elements, range(num_elements)))\n",
    "df = pd.DataFrame(data, columns = [\"name\", \"counts\"])\n",
    "df.plot(kind = \"hist\", by = df.name)\n",
    "df.hist(by = df.name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    #if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    if dataset_name not in ['ling-spam']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        if 'gml' in graph_cache_file: continue\n",
    "        X, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(\"^[a-zA-Z0-9]+$\")\n",
    "all_labels = graph_helper.get_all_node_labels(X)\n",
    "new_labels = set()\n",
    "for label in all_labels:\n",
    "    new_labels.add(label.strip())\n",
    "    new_labels.add(label)\n",
    "len(all_labels), len(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "indices = [0, 1, 5, 8]\n",
    "mat = sparse.lil_matrix((10, 10))\n",
    "mat[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for pred_file in glob('data/results/predictions/*.npy'):\n",
    "    if 'gml' not in pred_file or 'ng20' not in pred_file: continue\n",
    "    with open(pred_file, 'rb') as f:\n",
    "        predictions = pickle.load(f)\n",
    "    Y_true, Y_pred = predictions['Y_real'], predictions['Y_pred']\n",
    "    assert len(Y_true) == len(Y_pred)\n",
    "    cmat = sklearn.metrics.confusion_matrix(y_true = Y_true, y_pred = Y_pred)\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        f1 = sklearn.metrics.f1_score(y_true = Y_true, y_pred = Y_pred, average = 'macro')\n",
    "        helper.plot_confusion_matrix(cmat, classes=set(Y_true), normalize = True, show_non_horizontal_percent=False, title='{} ({})'.format(f1, pred_file.split('/')[-1]))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    del predictions, Y_true, Y_pred\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import graph_helper\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "counts = []\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    #if dataset_name not in ['ling-spam', 'ng20', 'webkb', 'reuters-21578']: continue\n",
    "    if dataset_name not in ['ng20']: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    node_counts = []\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        if 'gml' in graph_cache_file: continue\n",
    "        print('Loading dataset: {}'.format(graph_cache_file))\n",
    "        X, Y = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        for x in X:\n",
    "            node_counts.append((graph_cache_file, len(x.nodes())))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(node_counts, columns = ['filename', 'node_counts'])\n",
    "df_all_words = df[df.filename.str.contains('_all_')]\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (14, 8))\n",
    "df_all_words.node_counts.plot(kind = 'hist', log = False, bins = 400, ax = ax, title = 'Node count histogramm (ng20)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('node count per graph')\n",
    "median = df_all_words.node_counts.median()\n",
    "ax.text(x = median, y = -400, s = 'Median: {:.0f}'.format(median), fontdict={'fontsize': 12}, horizontalalignment = 'center')\n",
    "#ax.set_xscale('log')\n",
    "ax.vlines(ymin = 0, ymax=6000, x = median, colors='red')\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/node-count-ng20.png', dpi = 100)\n",
    "display(df_all_words.node_counts.describe().to_frame().T)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "for cache_file in dataset_helper.get_all_cached_graph_phi_datasets(dataset_name = 'ling-spam'):\n",
    "    print(cache_file)\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        X, Y = pickle.load(f)\n",
    "    stacked = scipy.sparse.hstack(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_all_words[df_all_words.node_counts == 1].size, df_all_words.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPGK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "from kernels import spgk\n",
    "import networkx as nx\n",
    "from time import time\n",
    "\n",
    "depth = 1\n",
    "MAX_ELEMENTS = None\n",
    "    \n",
    "for graph_cache_file in dataset_helper.get_all_cached_graph_datasets(dataset_name='ling-spam' if True else None):\n",
    "    if 'all' in graph_cache_file: continue\n",
    "    print('Cache file: {}'.format(graph_cache_file.split('/')[-1]))\n",
    "    X, Y = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "    if not isinstance(X[0], nx.Graph):\n",
    "        print('\\tWrong X type, expected: nx.Graphs, got: {}'.format(type(X[0])))\n",
    "        continue\n",
    "\n",
    "    num_elements = MAX_ELEMENTS if MAX_ELEMENTS else len(X)\n",
    "    X = X[:num_elements]\n",
    "    \n",
    "    try:\n",
    "        start_time = time()\n",
    "        \n",
    "        for x in X:\n",
    "            for u,v,edata in x.edges(data = True):\n",
    "                if 'weight' not in edata: continue\n",
    "                if edata['weight'] < 1:\n",
    "                    print(\"Edge weight={:>2}, from='{}', to='{}'\".format(edata['weight'], u, v))\n",
    "                edata['weight'] = 1\n",
    "            if False:\n",
    "                self_loop_edges = x.selfloop_edges()\n",
    "                if len(self_loop_edges):\n",
    "                    x.remove_edges_from(self_loop_edges)\n",
    "\n",
    "        K = spgk.build_kernel_matrix(X, depth)\n",
    "        \n",
    "        time_needed = time() - start_time\n",
    "        \n",
    "        print('\\tNon-zero\\t\\t{:.0f}%'.format(100 * len(K.nonzero()[0]) / (K.shape[0] * K.shape[0])))\n",
    "        print('\\tTime needed')\n",
    "        print('\\t\\ttotal {}:\\t{:.2f}s'.format(num_elements, time_needed))\n",
    "        print('\\t\\tper element:\\t{:.2f}s'.format(time_needed / num_elements))\n",
    "        print('\\t\\tper 10000:\\t{:.2f}s'.format(time_needed / num_elements * 10000))\n",
    "    except Exception as e:\n",
    "        print('\\tError: {}'.format(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in X:\n",
    "    adj = nx.adjacency_matrix(graph)\n",
    "    adj[0, 0] = 2\n",
    "    adj[adj.nonzero()] = 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "graph = sparse.eye(2, 10)\n",
    "\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "class TupleSelector(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, selected_index = 0):\n",
    "        self.selected_index = selected_index\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return [x[self.selected_index] for x in X]\n",
    "\n",
    "estimators = [('TupleSelector', PCA()), ('kernel_pca', KernelPCA())]\n",
    "combined = FeatureUnion(estimators)\n",
    "\n",
    "trans = sklearn.feature_extraction.text.TfidfVectorizer(stop_words='english')\n",
    "trans.fit(['Yes scheint es ja zu schlaiken', 'Yes'])\n",
    "X_text = trans.transform(['yes', 'yes'])\n",
    "\n",
    "X_combined = sparse.hstack([graph, X_text])\n",
    "graph.shape, X_text.shape, X_combined.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
