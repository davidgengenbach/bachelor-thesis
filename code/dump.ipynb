{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import deepwalk\n",
    "from deepwalk import graph\n",
    "from deepwalk import walks as serialized_walks\n",
    "from gensim.models import Word2Vec\n",
    "from deepwalk.skipgram import Skipgram\n",
    "import dataset_helper\n",
    "import graph_helper\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import tsne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_memory_data_size = 1000000000\n",
    "number_walks = 1000\n",
    "representation_size = 64\n",
    "seed = 0\n",
    "undirected = True\n",
    "vertex_freq_degree = False\n",
    "walk_length = 60\n",
    "window_size = 10\n",
    "workers = 1\n",
    "output = 'data/DUMP'\n",
    "\n",
    "for dataset in dataset_helper.get_all_available_dataset_names():\n",
    "    cache_file = dataset_helper.CACHE_PATH + '/dataset_graph_cooccurrence_{}.npy'.format(dataset)\n",
    "    X, Y = dataset_helper.get_dataset(dataset, preprocessed = False, use_cached=True, transform_fn=graph_helper.convert_dataset_to_co_occurence_graph_dataset, cache_file=cache_file)\n",
    "    break\n",
    "    \n",
    "models = []\n",
    "for idx, g in enumerate(X):\n",
    "    if idx == 3: break\n",
    "    print('Graph: {:>4}'.format(idx))\n",
    "    G = graph.from_networkx(g)\n",
    "\n",
    "    print(\"Number of nodes: {}\".format(len(G.nodes())))\n",
    "    if len(G.nodes()) == 0:\n",
    "        continue\n",
    "\n",
    "    num_walks = len(G.nodes()) * number_walks\n",
    "\n",
    "    print(\"Number of walks: {}\".format(num_walks))\n",
    "\n",
    "    data_size = num_walks * walk_length\n",
    "\n",
    "    print(\"Data size (walks*length): {}\".format(data_size))\n",
    "\n",
    "    print(\"Walking...\")\n",
    "    walks = graph.build_deepwalk_corpus(G, num_paths=number_walks, path_length=walk_length, alpha=0, rand=random.Random(seed))\n",
    "    print(\"Training...\")\n",
    "    model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, workers=workers)\n",
    "\n",
    "    #model.wv.save_word2vec_format(output)\n",
    "    models.append(model)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print('Next')\n",
    "    vectors = tsne.get_tsne_embedding(model)\n",
    "    tsne.plot_embedding(model, vectors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test WL phi computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    for i, (a, b) in enumerate(zip(phi_1_2_3_test, phi_1_2_3)):\n",
    "        if not np.array_equiv(a - b.todense(), np.zeros(b.shape, dtype = np.int32)):\n",
    "            print(\"\\tPhi different! {}\".format(i))\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "\n",
    "    for i, (a, b) in enumerate(zip(K_1_2_3_test, K_1_2_3)):\n",
    "        if not np.array_equal(a, b):\n",
    "            print(np.argwhere((a - b) != 0))\n",
    "            print(\"\\tK different! {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "for (n, treshold), lookup in results.items():\n",
    "    cliques = coreference.get_cliques_from_lookup(lookup)\n",
    "    similarity_counter = {'similar': len(lookup.keys()), 'unsimilar': num_labels - len(lookup.keys())}\n",
    "    clique_lenghts = [len(x) for x in list(cliques.values())]\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n",
    "    fig.suptitle('Treshold: {}, N={}'.format(treshold, n), fontsize = 16)\n",
    "\n",
    "    pd.DataFrame(clique_lenghts).plot(ax = axes[0], kind = 'hist', logy = True, legend = False, title = \"Histogram of clique lengths\".format(treshold))\n",
    "    pd.DataFrame(list(similarity_counter.items()), columns = ['name', 'count']).set_index('name').plot(ax = axes[1], kind = 'bar', legend = False, title = '# of labels that have been merged vs. not merged')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    fig.savefig('tmp/{:.5f}.{}.png'.format(treshold, n), dpi = 120)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set('notebook', 'white')\n",
    "def plot_by(df, by, bins = 15, title = '', figsize = (12, 5), fontsize = 16):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for n, vals in df.groupby(by):\n",
    "        labels.append(n)\n",
    "        data.append(vals.clique_length)\n",
    "    ax.hist(data, bins = bins, alpha=0.7, label=labels, log = True)\n",
    "    fig.suptitle(title, fontsize = fontsize)\n",
    "    ax.legend(loc='upper right', fontsize = fontsize)\n",
    "    ax.set_xlabel('clique sizes')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    return fig, ax\n",
    "fig, ax = plot_by(df, 'n', title = 'Clique size histogram by n (all thresholds together)')\n",
    "fig.savefig('tmp/clique_size_by_n_all_thresholds.png', dpi = 120)\n",
    "fig, ax = plot_by(df, 'threshold', title = 'Clique size histogram by threshold (all n together)')\n",
    "fig.savefig('tmp/clique_size_by_threshold_all_n.png', dpi = 120)\n",
    "fig, ax = plot_by(df[df.threshold == 0.6], 'n', title = 'Clique size histogram by n (threshold=0.6)')\n",
    "fig.savefig('tmp/clique_size_by_n_threshold_0.6.png', dpi = 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn\n",
    "graph_cache_file = 'dataset_graph_gml_ng20-single.npy'\n",
    "X, Y = dataset_helper.get_dataset_cached('data/CACHE/{}'.format(graph_cache_file))\n",
    "X, Y = np.array(X, dtype=object), np.array(Y, dtype=object)\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(n_splits = 20, random_state=42)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    X_test, Y_test = X[test_index], Y[test_index]\n",
    "    break\n",
    "with open('data/CACHE/dataset_graph_gml_small-single.npy', 'wb') as f:\n",
    "    pickle.dump((X_test.tolist(), Y_test.tolist()), f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_helper\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set('notebook', 'white')\n",
    "limit_dataset = ['ng20', 'ling-spam', 'reuters-21578', 'webkb']\n",
    "#limit_dataset = ['ling-spam']\n",
    "all_stats = {}\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in limit_dataset: continue\n",
    "    print(dataset_name)\n",
    "    X, Y = dataset_helper.get_dataset(dataset_name)\n",
    "    graphs = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    number_of_nodes = []\n",
    "    \n",
    "    coo_graph = [x for x in graphs if 'cooccurrence' in x][0]\n",
    "    gml_graph = [x for x in graphs if 'gml' in x][0]\n",
    "    \n",
    "    def get_num_nodes(graph_file):\n",
    "        X_graph, _ = dataset_helper.get_dataset_cached(graph_file)\n",
    "        return [nx.number_of_nodes(x) for x in X_graph]\n",
    "    \n",
    "    stats = {\n",
    "        'cooccurrence': get_num_nodes(coo_graph),\n",
    "        'concept-graphs': get_num_nodes(gml_graph)\n",
    "    }\n",
    "    min_len = min([len(v) for k, v in stats.items()])\n",
    "    graph_stats = {k: v[:min(min_len, len(v))] for k, v in stats.items()}\n",
    "    df = pd.DataFrame(graph_stats)\n",
    "    figsize = (12, 4)\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols=2, figsize = figsize)\n",
    "    colors = reversed([\"orangered\", \"royalblue\"])\n",
    "    colors = None\n",
    "    df.plot(kind = 'hist', alpha = 0.7, bins=20, logy = True, color=colors, ax=ax[0])\n",
    "    ax[0].set_xlabel('# nodes per graph')\n",
    "    \n",
    "    text_stats = {'doc_lengths': [len(x) for x in X]}\n",
    "    df = pd.DataFrame(text_stats)\n",
    "    df.plot(kind = 'hist', bins=20, legend = False, logy = True, ax=ax[1], color = 'royalblue')\n",
    "    ax[1].set_xlabel('# characters per document')\n",
    "    plt.show()\n",
    "    fig.savefig('tmp/other/stats-{}.png'.format(dataset_name), dpi = 150)\n",
    "    all_stats[dataset_name] = {\n",
    "        'graphs': graph_stats,\n",
    "        'text': text_stats['doc_lengths'],\n",
    "        'num_docs': len(X),\n",
    "        'num_classes': len(set(Y))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "out = []\n",
    "for dataset, stats in all_stats.items():\n",
    "    out.append((dataset, np.mean(stats['text']), np.mean(stats['graphs']['cooccurrence']), np.mean(stats['graphs']['concept-graphs']), stats['num_docs'], stats['num_classes']))\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize = (8, 5))\n",
    "\n",
    "df = pd.DataFrame(out, columns = ['dataset', 'avg_doc_length', 'avg_coo_node_num', 'avg_cp_node_num', 'num_docs', 'num_classes'])\n",
    "df = df.set_index('dataset')\n",
    "#, ('# classes', 'num_classes')\n",
    "for idx, (name, x) in enumerate([('Average document length', 'avg_doc_length'), ('# documents', 'num_docs'), ('Average of number of concept-graph nodes', 'avg_cp_node_num')]):\n",
    "    ax = axes[idx]\n",
    "    df[x].plot(kind = 'barh', logx = True, title = name, ax = ax)\n",
    "fig.tight_layout()\n",
    "fig.savefig('tmp/other/stats-datasets.png', dpi = 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataset_helper\n",
    "import collections\n",
    "import coreference\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.nx_graph_to_tuple_transformer import NxGraphToTupleTransformer\n",
    "\n",
    "check_graphs = False\n",
    "\n",
    "trans = NxGraphToTupleTransformer()\n",
    "for dataset_name in dataset_helper.get_all_available_dataset_names():\n",
    "    if dataset_name not in dataset_helper.DATASETS_LIMITED: continue\n",
    "    graph_cache_files = dataset_helper.get_all_cached_graph_datasets(dataset_name)\n",
    "    if not len(graph_cache_files): continue\n",
    "    \n",
    "    with open('data/embeddings/graph-embeddings/{}.label-lookup.npy'.format(dataset_name), 'rb') as f:\n",
    "        lookup = pickle.load(f)\n",
    "    \n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup, len(lookup.keys()))\n",
    "    plt.show()\n",
    "        \n",
    "    counter = collections.Counter()\n",
    "    all_labels = set()\n",
    "    for graph_cache_file in graph_cache_files:\n",
    "        X, _ = dataset_helper.get_dataset_cached(graph_cache_file)\n",
    "        X = trans.transform(X)\n",
    "        \n",
    "        for adj, labels in X:\n",
    "            all_labels |= set(labels)\n",
    "            for label in labels:\n",
    "                counter['found' if label in lookup and str(lookup[label]).strip() != str(label).strip() else 'not_found'] += 1\n",
    "    print(counter)\n",
    "    print(len(all_labels))\n",
    "    fig, axes = coreference.plot_lookup_histogram(lookup, len(all_labels))\n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
